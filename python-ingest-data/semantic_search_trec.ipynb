{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UnbNRZTDNA1M"
   },
   "source": [
    "# Semantic Search with Cohere and Pinecone\n",
    "\n",
    "In this notebook we will demonstrate how to perform semantic search for identifying similar or duplicate questions using Cohere and Pinecone.\n",
    "\n",
    "![Steps in semantic search process](https://raw.githubusercontent.com/pinecone-io/examples/master/integrations/cohere/assets/index_query_pinecone_cohere.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0974WnM7NA1O"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUqE_6bcNA1O"
   },
   "source": [
    "We first need to setup our environment and retrieve API keys for Cohere and Pinecone. Let's start with our environment, we need HuggingFace *Datasets* for our data, and the Cohere and Pinecone clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mhmhz2D3NA1P",
    "outputId": "517ecc3b-e123-41f1-d060-fc7ec09a810a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.21.4)\n",
      "Requirement already satisfied: cohere in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (4.2.0)\n",
      "Requirement already satisfied: pinecone-client in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.2.1)\n",
      "Requirement already satisfied: aiohttp<4.0,>=3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from cohere) (3.8.4)\n",
      "Requirement already satisfied: backoff<3.0,>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from cohere) (2.2.1)\n",
      "Requirement already satisfied: requests<3.0,>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from cohere) (2.26.0)\n",
      "Requirement already satisfied: loguru>=0.5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pinecone-client) (0.7.0)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pinecone-client) (2.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pinecone-client) (4.0.1)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pinecone-client) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pinecone-client) (6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pinecone-client) (2.8.2)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pinecone-client) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<4.0,>=3.0->cohere) (2.0.9)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<4.0,>=3.0->cohere) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<4.0,>=3.0->cohere) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<4.0,>=3.0->cohere) (1.9.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<4.0,>=3.0->cohere) (21.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-dateutil>=2.5.3->pinecone-client) (1.12.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3.0,>=2.0->cohere) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3.0,>=2.0->cohere) (3.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Frameworks/Python.framework/Versions/3.10/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install numpy cohere pinecone-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-aPF7S50NA1P"
   },
   "source": [
    "And sign up for an API key over at [Cohere](https://os.cohere.ai/) and [Pinecone](https://app.pinecone.io), you can enter the keys directly in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kYBByOHYNA1Q"
   },
   "outputs": [],
   "source": [
    "COHERE_KEY = 'nuyihjgwcybIE7SVhKtMw7zRp3vLuRJY94SsMKyl'\n",
    "PINECONE_KEY = '2f14cfcd-33fa-453c-afc2-a9599563894a'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95KrmFq6NA1Q"
   },
   "source": [
    "## Create Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yQxt8ukNA1R"
   },
   "source": [
    "We can create sentence embeddings easily using Cohere. First, we import the Cohere client and initialize our connection using the API key we retrieved earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BPllmdUWNA1R"
   },
   "outputs": [],
   "source": [
    "import cohere\n",
    "\n",
    "co = cohere.Client(COHERE_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXRk3gMENA1R"
   },
   "source": [
    "We will load the **T**ext **RE**trieval **C**onference (TREC) question classification dataset which contains 5.5K labeled questions. We will take the first 1K samples for this demo, but this can be scaled to millions or even billions of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17452\n"
     ]
    }
   ],
   "source": [
    "filename = \"tensor7.txt\"\n",
    "with open(filename) as file:\n",
    "    lines = [line.strip() for line in file]\n",
    "txt = \" \".join(lines)\n",
    "\n",
    "print(len(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "221A Lecture Notes\n"
     ]
    }
   ],
   "source": [
    "print(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5655\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(txt)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_nums = [len(\" \".join(line)) for line in lines]\n",
    "def getLineNum(c_seen):\n",
    "    for i in range(len(line_nums)):\n",
    "        if c_seen > line_nums[i]:\n",
    "            c_seen -= line_nums[i]\n",
    "        else:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "{'text': '221A Lecture Notes Notes on Tensor Product 1 What is “ Tensor ” ? After discussing the tensor product in the class , I received many questions what it means . I ’ ve also talked to Daniel , and he felt this is a subject he had learned on the way here and there , never in a course or a book . I myself don ’ t remember where and when I learned about it . It may be worth solving this problem once and for all . Apparently , Tensor is a manufacturer of fancy floor lamps . See http : //growinglifestyle.shop.com/cc.class/cc ? pcd=7878065 & ccsyn=13549 . For us , the word “ tensor ” refers to objects that have multiple indices . In comparison , a “ scalar ” does not have an index , and a “ vector ” one index . It appears in many different contexts , but this point is always the same . 2 Direct Sum Before getting into the subject of tensor product , let me first discuss “ direct sum. ” This is a way of getting a new big vector space from two ( or more ) smaller vector spaces in the simplest way one can imagine : you just line them up . 2.1 Space You start with two vector spaces , Vthat isn-dimensional , andWthat is m-dimensional . The tensor product of these two vector spaces isn+m- dimensional . Here is how it works . Let', 'loc': {'from': 0, 'to': 9}}\n"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "characters_seen = 0\n",
    "chunk_size = 250\n",
    "overlap = chunk_size//10\n",
    "for i in range(0, len(tokens), chunk_size-overlap):\n",
    "    cur_chunk_tokens = tokens[i:i+chunk_size]\n",
    "    fro = getLineNum(characters_seen)\n",
    "    characters_seen += len(\"\".join(cur_chunk_tokens))\n",
    "    to = getLineNum(characters_seen)\n",
    "    chunks.append({\n",
    "        'text': \" \".join(cur_chunk_tokens),\n",
    "        'loc': {'from': fro, 'to': to}\n",
    "        }\n",
    "    )\n",
    "    \n",
    "print(len(chunks))\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lKGiLkWpNA1S",
    "outputId": "220a5ddc-a8ea-4150-b88c-c95f6a92f84e"
   },
   "outputs": [],
   "source": [
    "embeds = co.embed(\n",
    "    texts=[chunk['text'] for chunk in chunks],\n",
    "    model='small',\n",
    "    truncate='LEFT'\n",
    ").embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "shape = len(embeds)\n",
    "print(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pinecone initialized\n"
     ]
    }
   ],
   "source": [
    "import pinecone\n",
    "pinecone.init(\n",
    "    PINECONE_KEY,\n",
    "    environment=\"northamerica-northeast1-gcp\"  # find next to API key in console\n",
    ")\n",
    "print('pinecone initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pinecone index initialized\n"
     ]
    }
   ],
   "source": [
    "index = pinecone.Index('cohere-index')\n",
    "print('pinecone index initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "ids = [str(i) for i in range(shape)]\n",
    "# create list of metadata dictionaries\n",
    "meta = [{'text': chunk['text'], 'source': \"\", \"pdf_numpages\": 695, 'line-from': chunk['loc']['from'], 'line-to': chunk['loc']['to']} for chunk in chunks]\n",
    "\n",
    "# create list of (id, vector, metadata) tuples to be upserted\n",
    "to_upsert = list(zip(ids, embeds, meta))\n",
    "\n",
    "i_start = 0\n",
    "for i in range(i_start, shape, batch_size):\n",
    "    print(i)\n",
    "    i_end = min(i+batch_size, shape)\n",
    "    \n",
    "#     print(to_upsert[i:i_end])\n",
    "    \n",
    "    index.upsert(vectors=to_upsert[i:i_end],\n",
    "             namespace='tensor')\n",
    "#     time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQc5RPosNA1T"
   },
   "source": [
    "We can then pass these questions to Cohere to create embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gR-oj6C9NA1W"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "semantic-search-trec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
