{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UnbNRZTDNA1M"
   },
   "source": [
    "# Semantic Search with Cohere and Pinecone\n",
    "\n",
    "In this notebook we will demonstrate how to perform semantic search for identifying similar or duplicate questions using Cohere and Pinecone.\n",
    "\n",
    "![Steps in semantic search process](https://raw.githubusercontent.com/pinecone-io/examples/master/integrations/cohere/assets/index_query_pinecone_cohere.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0974WnM7NA1O"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUqE_6bcNA1O"
   },
   "source": [
    "We first need to setup our environment and retrieve API keys for Cohere and Pinecone. Let's start with our environment, we need HuggingFace *Datasets* for our data, and the Cohere and Pinecone clients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mhmhz2D3NA1P",
    "outputId": "517ecc3b-e123-41f1-d060-fc7ec09a810a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (1.21.4)\n",
      "Requirement already satisfied: cohere in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (4.2.0)\n",
      "Requirement already satisfied: pinecone-client in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (2.2.1)\n",
      "Requirement already satisfied: aiohttp<4.0,>=3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from cohere) (3.8.4)\n",
      "Requirement already satisfied: requests<3.0,>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from cohere) (2.26.0)\n",
      "Requirement already satisfied: backoff<3.0,>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from cohere) (2.2.1)\n",
      "Requirement already satisfied: pyyaml>=5.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pinecone-client) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pinecone-client) (4.0.1)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pinecone-client) (2.3.0)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pinecone-client) (1.26.7)\n",
      "Requirement already satisfied: loguru>=0.5.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pinecone-client) (0.7.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pinecone-client) (2.8.2)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from pinecone-client) (4.65.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<4.0,>=3.0->cohere) (2.0.9)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<4.0,>=3.0->cohere) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<4.0,>=3.0->cohere) (1.9.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<4.0,>=3.0->cohere) (21.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<4.0,>=3.0->cohere) (1.3.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from aiohttp<4.0,>=3.0->cohere) (6.0.4)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from python-dateutil>=2.5.3->pinecone-client) (1.12.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3.0,>=2.0->cohere) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages (from requests<3.0,>=2.0->cohere) (2021.10.8)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/Library/Frameworks/Python.framework/Versions/3.10/bin/python3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install numpy cohere pinecone-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-aPF7S50NA1P"
   },
   "source": [
    "And sign up for an API key over at [Cohere](https://os.cohere.ai/) and [Pinecone](https://app.pinecone.io), you can enter the keys directly in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kYBByOHYNA1Q"
   },
   "outputs": [],
   "source": [
    "COHERE_KEY = 'nuyihjgwcybIE7SVhKtMw7zRp3vLuRJY94SsMKyl'\n",
    "PINECONE_KEY = '2f14cfcd-33fa-453c-afc2-a9599563894a'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "95KrmFq6NA1Q"
   },
   "source": [
    "## Create Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yQxt8ukNA1R"
   },
   "source": [
    "We can create sentence embeddings easily using Cohere. First, we import the Cohere client and initialize our connection using the API key we retrieved earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "BPllmdUWNA1R"
   },
   "outputs": [],
   "source": [
    "import cohere\n",
    "\n",
    "co = cohere.Client(COHERE_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXRk3gMENA1R"
   },
   "source": [
    "We will load the **T**ext **RE**trieval **C**onference (TREC) question classification dataset which contains 5.5K labeled questions. We will take the first 1K samples for this demo, but this can be scaled to millions or even billions of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "495978\n"
     ]
    }
   ],
   "source": [
    "filename = \"style.txt\"\n",
    "with open(filename) as file:\n",
    "    lines = [line.strip() for line in file]\n",
    "txt = \" \".join(lines)\n",
    "\n",
    "print(len(txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ten PrinciPles for\n"
     ]
    }
   ],
   "source": [
    "print(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96212\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(txt)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_nums = [len(\" \".join(line)) for line in lines]\n",
    "def getLineNum(c_seen):\n",
    "    for i in range(len(line_nums)):\n",
    "        if c_seen > line_nums[i]:\n",
    "            c_seen -= line_nums[i]\n",
    "        else:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2139\n",
      "{'text': 'Ten PrinciPles for WriTing clearly 1 . Distinguish real grammatical rules from folklore ( pp . 10–21 ) . 2 . Use subjects to name the characters in your story ( pp . 46–52 ) . 3 . Use verbs to name their important actions ( pp . 32–39 )', 'loc': {'from': 0, 'to': 2}}\n"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "characters_seen = 0\n",
    "chunk_size = 50\n",
    "overlap = 5\n",
    "for i in range(0, len(tokens), chunk_size-overlap):\n",
    "    cur_chunk_tokens = tokens[i:i+chunk_size]\n",
    "    fro = getLineNum(characters_seen)\n",
    "    characters_seen += len(\"\".join(cur_chunk_tokens))\n",
    "    to = getLineNum(characters_seen)\n",
    "    chunks.append({\n",
    "        'text': \" \".join(cur_chunk_tokens),\n",
    "        'loc': {'from': fro, 'to': to}\n",
    "        }\n",
    "    )\n",
    "    \n",
    "print(len(chunks))\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lKGiLkWpNA1S",
    "outputId": "220a5ddc-a8ea-4150-b88c-c95f6a92f84e"
   },
   "outputs": [],
   "source": [
    "embeds = co.embed(\n",
    "    texts=[chunk['text'] for chunk in chunks],\n",
    "    model='small',\n",
    "    truncate='LEFT'\n",
    ").embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2139\n"
     ]
    }
   ],
   "source": [
    "shape = len(embeds)\n",
    "print(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pinecone initialized\n"
     ]
    }
   ],
   "source": [
    "import pinecone\n",
    "pinecone.init(\n",
    "    PINECONE_KEY,\n",
    "    environment=\"northamerica-northeast1-gcp\"  # find next to API key in console\n",
    ")\n",
    "print('pinecone initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pinecone index initialized\n"
     ]
    }
   ],
   "source": [
    "index = pinecone.Index('cohere-index')\n",
    "print('pinecone index initialized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "128\n",
      "256\n",
      "384\n",
      "512\n",
      "640\n",
      "768\n",
      "896\n",
      "1024\n",
      "1152\n",
      "1280\n",
      "1408\n",
      "1536\n",
      "1664\n",
      "1792\n",
      "1920\n",
      "2048\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "ids = [str(i) for i in range(shape)]\n",
    "# create list of metadata dictionaries\n",
    "meta = [{'text': chunk['text'], 'source': \"\", \"pdf_numpages\": 695, 'line-from': chunk['loc']['from'], 'line-to': chunk['loc']['to']} for chunk in chunks]\n",
    "\n",
    "# create list of (id, vector, metadata) tuples to be upserted\n",
    "to_upsert = list(zip(ids, embeds, meta))\n",
    "\n",
    "i_start = 0\n",
    "for i in range(i_start, shape, batch_size):\n",
    "    print(i)\n",
    "    i_end = min(i+batch_size, shape)\n",
    "    \n",
    "#     print(to_upsert[i:i_end])\n",
    "    \n",
    "    index.upsert(vectors=to_upsert[i:i_end],\n",
    "             namespace='english3')\n",
    "#     time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQc5RPosNA1T"
   },
   "source": [
    "We can then pass these questions to Cohere to create embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gR-oj6C9NA1W"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "semantic-search-trec.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
