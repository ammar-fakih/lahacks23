INTRODUCTION TO VECTORS AND TENSORS 
 
Linear and Multilinear Algebra 
 
 
Volume 1 
 
 
 
 
Ray M. Bowen 
Mechanical Engineering 
Texas A&M University 
College Station, Texas 
 
and 
 
C.-C. Wang 
  
Mathematical Sciences 
                            Rice              University              
                            Houston,              Texas              
 
 
 
 
 
 
 
 
 
 
Copyright Ray M. Bowen and C.-C. Wang 
(ISBN 0-306-37508-7 (v. 1)) 



 
 iii 
____________________________________________________________________________ 
PREFACE 
 
To Volume 1 
 
This work represents our effort to present the basic concepts of vector and tensor analysis.  Volume 
I begins with a brief discussion of algebraic structures followed by a rather detailed discussion of 
the algebra of vectors and tensors.  Volume II begins with a discussion of Euclidean Manifolds 
which leads to a development of the analytical and geometrical aspects of vector and tensor fields.  
We have not included a discussion of general differentiable manifolds.  However, we have included 
a chapter on vector and tensor fields defined on Hypersurfaces in a Euclidean Manifold. 
 
 In preparing this two volume work our intention is to present to Engineering and Science 
students a modern introduction to vectors and tensors.  Traditional courses on applied mathematics 
have emphasized problem solving techniques rather than the systematic development of concepts.   
As a result, it is possible for such courses to become terminal mathematics courses rather than 
courses which equip the student to develop his or her understanding further. 
 
As Engineering students our courses on vectors and tensors were taught in the traditional 
way.  We learned to identify vectors and tensors by formal transformation rules rather than by their 
common mathematical structure.  The subject seemed to consist of nothing but a collection of 
mathematical manipulations of long equations decorated by a multitude of subscripts and 
superscripts.  Prior to our applying vector and tensor analysis to our research area of modern 
continuum mechanics, we almost had to relearn the subject.  Therefore, one of our objectives in 
writing this book is to make available a modern introductory textbook suitable for the first in-depth 
exposure to vectors and tensors.  Because of our interest in applications, it is our hope that this 
book will aid students in their efforts to use vectors and tensors in applied areas. 
 
The presentation of the basic mathematical concepts is, we hope, as clear and brief as 
possible without being overly abstract.  Since we have written an introductory text, no attempt has 
been made to include every possible topic.  The topics we have included tend to reflect our  
personal bias.  We make no claim that there are not other introductory topics which could have 
been included. 
 
Basically the text was designed in order that each volume could be used in a one-semester 
course.  We feel Volume I is suitable for an introductory linear algebra course of one semester.   
Given this course, or an equivalent, Volume II is suitable for a one semester course on vector and 
tensor analysis.  Many exercises are included in each volume.  However, it is likely that teachers 
will wish to generate additional exercises.  Several times during the preparation of this book we 
taught  a one semester course to students with a very limited background in linear algebra and no 
background in tensor analysis.  Typically these students were majoring in Engineering or one of the 
Physical Sciences.  However, we occasionally had students from the Social Sciences.  For this one 
semester course, we covered the material in Chapters 0, 3, 4, 5, 7 and 8 from Volume I and selected 
topics from Chapters 9, 10, and 11 from Volume 2.  As to level, our classes have contained juniors, 

iv                                                                                                                                          PREFACE                                                                     
seniors and graduate students.  These students seemed to experience no unusual difficulty with the 
material. 
 
It is a pleasure to acknowledge our indebtedness to our students for their help and 
forbearance.  Also, we wish to thank the U. S. National Science Foundation for its support during 
the preparation of this work.  We especially wish to express our appreciation for the patience and 
understanding of our wives and children during the extended period this work was in preparation. 
 
Houston, Texas R.M.B. 
 C.-C.W. 
 

 
 v 
______________________________________________________________________________ 
 
CONTENTS 
 
Vol. 1  Linear and Multilinear Algebra 
 
 Contents of Volume 2................................................................ vii 
 
PART I BASIC MATHEMATICS 
 
Selected Readings for Part I.................................................................. 2 
 
 
CHAPTER 0   Elementary Matrix Theory................................................. 3 
 
CHAPTER 1   Sets, Relations, and Functions............................................. 13 
 
Section 1. Sets and Set Algebra................................................ 13 
 Section 2. Ordered Pairs" Cartesian Products" and Relations............. 16 
            Section            3.            Functions.............................................................            18            
 
CHAPTER 2   Groups, Rings and Fields................................................... 23 
 
 Section 4. The Axioms for a Group........................................... 23 
 Section 5. Properties of a Group............................................... 26 
Section 6. Group Homomorphisms............................................ 29 
 Section 7. Rings and Fields..................................................... 33 
 
PART II VECTOR AND TENSOR ALGEBRA 
 
Selected Readings for Part II.................................................................. 40 
 
CHAPTER 3   Vector Spaces................................................................. 41 
 
Section 8. The Axioms for a Vector Space................................... 41 
            Section            9.            Linear            Independence, Dimension and Basis..................... 46 
 Section 10. Intersection, Sum and Direct Sum of Subspaces................ 55 
 Section 11. Factor Spaces......................................................... 59 
 Section 12. Inner Product Spaces................................................ 62 
            Section            13.            Orthogonal            Bases            and Orthogonal Compliments............... 69 
Section 14. Reciprocal Basis and Change of Basis........................... 75 
 
CHAPTER 4.  Linear Transformations......................................................      85      
 
 Section 15. Definition of a Linear Transformation............................ 85 

CONTENTS OF VOLUME 1  vi 
 
Section 16. Sums and Products of Linear Transformations.................. 93 
 Section 17. Special Types of Linear Transformations........................ 97 
Section 18. The Adjoint of a Linear Transformation..........................    105 
 Section 19. Component Formulas................................................    118 
 
CHAPTER 5.  Determinants and Matrices...................................................   125 
 
Section 20. The Generalized Kronecker Deltas 
and the Summation Convention....................................   125 
            Section            21.            Determinants..........................................................            130            
 Section 22. The Matrix of a Linear Transformation...........................   136 
Section 23 Solution of Systems of Linear Equations..........................   142 
 
CHAPTER 6   Spectral Decompositions......................................................   145 
 
Section 24. Direct Sum of Endomorphisms....................................    145 
 Section 25. Eigenvectors and Eigenvalues......................................    148 
Section 26. The Characteristic Polynomial.....................................    151 
 Section 27. Spectral Decomposition for Hermitian Endomorphisms........   158 
Section 28. Illustrative Examples.................................................   171 
 Section 29. The Minimal Polynomial............................................    176 
 Section 30. Spectral Decomposition for Arbitrary Endomorphisms.........    182 
 
CHAPTER 7.  Tensor Algebra................................................................    203 
 
Section 31. Linear Functions, the Dual Space.................................     203 
 Section 32. The Second Dual Space, Canonical Isomorphisms.............     213 
            Section            33.            Multilinear            Functions, Tensors.....................................     218 
Section 34. Contractions............................................................................     229 
Section 35. Tensors on Inner Product Spaces..................................    235 
 
CHAPTER 8.  Exterior Algebra...............................................................    247 
 
 Section 36. Skew-Symmetric Tensors and Symmetric Tensors..............    247 
 Section 37. The Skew-Symmetric Operator....................................    250 
Section 38. The Wedge Product..................................................    256 
Section 39. Product Bases and Strict Components.............................    263 
 Section 40. Determinants and Orientations.....................................    271 
            Section            41.            Duality.................................................................            280            
 Section 42. Transformation to Contravariant Representation............. 287 
 
.

 
 vii 
__________________________________________________________________________ 
CONTENTS 
 
Vol. 2  Vector and Tensor Analysis 
 
 
 
PART III.  VECTOR AND TENSOR ANALYSIS 
 
 
 Selected Readings for Part III................................................ 296 
 
CHAPTER 9.  Euclidean Manifolds.................................................. 297 
 
Section 43. Euclidean Point Spaces...................................... 297 
Section 44. Coordinate Systems.......................................... 306 
Section 45. Transformation Rules for Vector and Tensor Fields.... 324 
Section 46. Anholonomic and Physical Components of Tensors.... 332 
Section 47. Christoffel Symbols and Covariant Differentiation...... 339 
Section 48. Covariant Derivatives along Curves....................... 353 
 
CHAPTER 10. Vector Fields and Differential Forms.............................. 359 
 
Section 49. Lie Derivatives............................................... 359 
 Section 5O. Frobenius Theorem.......................................... 368 
            Section            51.            Differential            Forms            and Exterior Derivative.............. 373 
 Section 52. The Dual Form of Frobenius Theorem: the Poincaré 
                                    Lemma........................................................                381            
 Section 53. Vector Fields in a Three-Dimensiona1 Euclidean 
                                    Manifold,            I.                        Invariants            and            Intrinsic            Equations........            389            
Section 54. Vector Fields in a Three-Dimensiona1 Euclidean 
  Manifold, II.  Representations for Special 
  Class of Vector Fields..................................... 399 
 
CHAPTER 11. Hypersurfaces in a Euclidean Manifold 
 
Section 55. Normal Vector, Tangent Plane, and Surface Metric 407 
Section 56. Surface Covariant Derivatives 416 
 Section 57. Surface Geodesics and the Exponential Map 425 
 Section 58. Surface Curvature, I. The Formulas of Weingarten 
                                    and            Gauss                                    433            
 Section 59. Surface Curvature, II. The Riemann-Christoffel 
                                    Tensor            and            the            Ricci            Identities            443            
 Section 60. Surface Curvature, III. The Equations of Gauss and Codazzi 449 
 Section 61. Surface Area, Minimal Surface 454 

CONTENTS OF VOLUME 11  viii 
 
Section 62. Surfaces in a Three-Dimensional Euclidean Manifold 457 
 
CHAPTER 12. Elements of Classical Continuous Groups 
 
Section 63. The General Linear Group and Its Subgroups 463 
Section 64. The Parallelism of Cartan 469 
 Section 65. One-Parameter Groups and the Exponential Map 476 
Section 66. Subgroups and Subalgebras 482 
Section 67. Maximal Abelian Subgroups and Subalgebras 486 
 
CHAPTER 13. Integration of Fields on Euclidean Manifolds, Hypersurfaces, and 
Continuous Groups 
 
Section 68. Arc Length, Surface Area, and Volume 491 
Section 69. Integration of Vector Fields and Tensor Fields 499 
            Section            70.            Integration            of Differential Forms 503 
 Section 71. Generalized Stokes’ Theorem 507 
Section 72. Invariant Integrals on Continuous Groups 515 
 
INDEX...................................................................................................x 

 
 
 
 
_______________________________________________________________________________ 
PART 1 
 
 
BASIC MATHEMATICS 
 

Selected Reading for Part I 
 
BIRKHOFF, G., and S. MACLANE, A Survey of Modern Algebra, 2
nd
 ed., Macmillan, New York, 
1953. 
F
RAZER, R. A., W. J. DUNCAN, and A. R. COLLAR, Elementary Matrices, Cambridge University 
Press, Cambridge, 1938. 
H
ALMOS, P. R., Naïve Set Theory, Van Nostrand, New York, 1960. 
KELLEY, J. L., Introduction to Modern Algebra, Van Nostrand, New York 1965. 
M
OSTOW, G. D., J. H. SAMPSON, and J. P. MEYER, Fundamental Structures of Algebra, McGraw-
Hill, New York, 1963. 
VAN DER WAERDEN, B. L., Modern Algebra, rev. English ed., 2 vols., Ungar, New York, 1949, 
1950. 

 
 3 
__________________________________________________________ 
Chapter 0 
 
 
ELEMENTARY MATRIX THEORY 
 
 
When we introduce the various types of structures essential to the study of vectors and 
tensors, it is convenient in many cases to illustrate these structures by examples involving matrices.  
It is for this reason we are including a very brief introduction to matrix theory here.  We shall not 
make any effort toward rigor in this chapter.  In Chapter V we shall return to the subject of matrices 
and augment, in a more careful fashion, the material presented here. 
 
An 
M by N matrix A is a rectangular array of real or complex numbers 
ij
Aarranged in 
Mrows and N columns.  A matrix is often written 
 
 
11121
21222
12
N
N
MMMN
AAA
AAA
A
AAA
⋅⋅⋅
⎡⎤
⎢⎥
⋅⋅⋅
⎢⎥
=⋅
⎢⎥
⎢⎥
⋅
⎢⎥
⎢⎥
⋅⋅⋅
⎣⎦
                                             (0.1)                                             
 
and the numbers 
ij
A
 are called the elements or components of 
A
.  The matrix 
A
 is called a real 
matrix or a complex matrix according to whether the components of A are real numbers or 
complex numbers. 
 
A matrix of Mrows and N columns is said to be of order Mby N orMN×.  It is 
customary to enclose the array with brackets, parentheses or double straight lines.  We shall adopt 
the notation in (0.1).  The location of the indices is sometimes modified to the forms 
ij
A, 
i
j
A, or 
j
i
A.  Throughout this chapter the placement of the indices is unimportant and shall always be 
written as in (0.1).  The elements 
12
,,...,
iiiN
AAAare the elements of the i
th
 row ofA, and the 
elements 
12
,,...,
kkNk
AAA are the elements of the k
th
 column. The convention is that the first index 
denotes the row and the second the column. 
 
A row matrix is a 
1N× matrix, e.g., 
 
 
[]
11121N
AAA⋅⋅⋅ 
 
while a column matrix is an 
1M× matrix, e.g., 
 

4 Chap. 0           •           ELEMENTARY MATRIX THEORY 
 
11
21
1M
A
A
A
⎡⎤
⎢⎥
⎢⎥
⋅
⎢⎥
⎢⎥
⋅
⎢⎥
⎢⎥
⋅
⎢⎥
⎣⎦
 
 
 
The matrix 
A is often written simply 
 
 
ij
AA
⎡⎤
=
⎣⎦
                                                                (0.2)                                                                
 
A square matrix is an NN× matrix.  In a square matrixA, the elements 
1122
,,...,
NN
AAA are its 
diagonal elements.  The sum of the diagonal elements of a square matrix A is called the trace and 
is written 
trA.  Two matrices 
A
 and B are said to be equal if they are identical.  That is, 
A
 and 
Bhave the same number of rows and the same number of columns and 
 
 
,1,...,    ,1,...,
ijij
ABiNjM==  =
 
 
A matrix, every element of which is zero, is called the 
zero matrix and is written simply0. 
 
If 
ij
AA
⎡⎤
=
⎣⎦
 and 
ij
BB
⎡⎤
=
⎣⎦
 are two 
MN× matrices, their sum (difference) is an MN× 
matrix 
AB+
 
()AB−
 whose elements are 
ijij
AB+
 
()
ijij
AB−
.  Thus 
 
 
ijij
AB   A B
⎡⎤
±= ±
⎣⎦
                                                         (0.3)                                                         
 
Two matrices of the same order are said to be 
conformable for addition and subtraction.  Addition 
and subtraction are not defined for matrices which are not conformable.  If 
λ is a number and 
A
 is 
a matrix, then 
Aλ is a matrix given by 
 
 
ij
AAAλλλ
⎡⎤
==
⎣⎦
                                                         (0.4)                                                         
 
Therefore, 
 
 
(1)
ij
AAA
⎡⎤
−=− =−
⎣⎦
                                                       (0.5)                                                       
 
These definitions of addition and subtraction and, multiplication by a number imply that 
 
 
ABBA+=+                                                             (0.6)                                                             
 

Chap. 0 • ELEMENTARY MATRIX THEORY 5 
 
()()ABC  ABC++=++                                                   (0.7)                                                   
 
 
0AA+=                                                                (0.8)                                                                
 
 
0AA−=                                                                (0.9)                                                                
 
 
()ABABλλλ+=+                                                     (0.10)                                                     
 
 
()AAAλμλμ+=+                                                     (0.11)                                                     
 
and 
 
 
1AA=                                                                (0.12)                                                                
 
where    ,AB and 
Care as assumed to be conformable. 
 
If A is an 
MN×
 matrix and Bis an 
NK×
 matrix, then the product of Bby A is written 
ABand is an 
MK× matrix with elements 
1
N
ijjs
j
AB
=
∑
,      1,...,iM=,       1,...,sK=.  For example, if  
 
 
1112
1112
2122
2122
3132
and
AA
BB
AAAB
BB
AA
⎡⎤
⎡⎤
⎢⎥
==
⎢⎥
⎢⎥
⎣⎦
⎢⎥
⎣⎦
 
 
then 
ABis a 32× matrix given by 
 
 
1112
1112
2122
2122
3132
11    11122111    121222
21    11222121    122222
31    11322132123222
AA
BB
ABAA
BB
AA
ABABABAB
ABABABAB
ABABABAB
⎡⎤
⎡⎤
⎢⎥
=
⎢⎥
⎢⎥
⎣⎦
⎢⎥
⎣⎦
++
⎡⎤
⎢⎥
=++
⎢⎥
++
⎢⎥
⎣⎦
 
 
The product 
AB is defined only when the number of columns of A is equal to the number 
of rows of
B.  If this is the case, A is said to be conformable to B for multiplication.  If A is 
conformable to 
B, then Bis not necessarily conformable to A.  Even if BA is defined, it is not 
necessarily equal to 
AB
.  On the assumption that 
A
, B,and C are conformable for the indicated  
sums and products, it is possible to show that 
 
                                                        ()
AB  CAB   AC+=+                                                     (0.13)                                                     
 
                                                      ()
ABC AC BC+=+                                                     (0.14)                                                     

6 Chap. 0           •           ELEMENTARY MATRIX THEORY 
 
and 
 
                                                                ()()
ABCABC=                                                         (0.15)                                                         
 
 
However, 
ABBA≠
in general, 0AB= does not imply 0A= or 0B=, and ABAC=does not 
necessarily imply 
BC=. 
 
The square matrix 
Idefined by 
 
 
100
010
001
I
⋅⋅⋅
⎡⎤
⎢⎥
⋅⋅⋅
⎢⎥
⋅
⎢⎥
=
⎢⎥
⋅
⎢⎥
⎢⎥
⋅
⎢⎥
⋅⋅⋅
⎣⎦
                                                  (0.16)                                                  
 
is the 
identity matrix.  The identity matrix is a special case of a diagonal matrix which has all of its 
elements zero except the diagonal ones.  A square matrix 
A whose elements satisfy 0
ij
A=, ij>, 
is called 
an upper triangular matrix , i.e., 
 
 
1112131
22232
33
0
00
000
N
N
NN
AAAA
AAA
A
A
A
⋅⋅⋅
⎡⎤
⎢⎥
⋅⋅⋅
⎢⎥
⋅⋅⋅
⎢⎥
⎢⎥
=⋅
⎢⎥
⎢⎥
⋅
⎢⎥
⋅
⎢⎥
⎢⎥
⋅⋅⋅
⎣⎦
 
 
A lower triangular matrix can be defined in a similar fashion.  A diagonal matrix is both an upper 
triangular matrix and a lower triangular matrix. 
 
If 
A and B are square matrices of the same order such that ABBAI==, then B is called 
the 
inverse of A and we write 
1
BA
−
=.  Also, Ais the inverse of B, i.e. 
1
AB
−
=.  If A has an  
inverse it is said to be 
nonsingular. If A and Bare square matrices of the same order with inverses 
1
A
−
and 
1
B
−
respectively, then 
 
 
111
()ABB   A
−−−
=                                                         (0.17)                                                         
 
Equation (0.17) follows because 

Chap. 0 • ELEMENTARY MATRIX THEORY 7 
 
 
111   111
()(   )AB  B   AA BBAAIAAAI
−−− −−−
==== 
 
and similarly 
 
 
11
()BAABI
−−
= 
 
The matrix of order 
NM× obtained by interchanging the rows and columns of an MN× 
matrix 
A
 is called the transpose of 
A
 and is denoted by 
T
A.  It is easily shown that 
 
                                                                   ()
TT
AA=                                                              (0.18)                                                              
 
                                                                ()
TT
AAλλ=                                                           (0.19)                                                           
 
                                                       ()
TTT
ABA  B+=+                                                      (0.20)                                                      
 
and 
 
                                                               ()
TTT
ABB  A=                                                          (0.21)                                                          
 
A square matrix 
A is symmetric if 
T
AA=and skew-symmetric if 
T
AA=−.  Therefore, for a 
symmetric matrix 
 
 
ijji
AA=
                                                               (0.22)                                                               
 
and for a skew-symmetric matrix 
 
 
ijji
AA=−                                                              (0.23)                                                              
 
Equation (0.23) implies that the diagonal elements of a skew symmetric-matrix are all zero.  Every 
square matrix 
A can be written uniquely as the sum of a symmetric matrix and a skew-symmetric 
matrix, namely 
 
 
11
()()
22
TT
AAA   AA=++−                                               (0.24)                                               
 
If A is a square matrix, its determinant is written 
detA
.  The reader is assumed at this 
stage to have some familiarity with the computational properties of determinants.  In particular, it 
should be known that 
 
                               detdetanddet(det   )(det   )
T
AAAB  AB==                     (0.25)                     
 

8 Chap. 0           •           ELEMENTARY MATRIX THEORY 
If A is an NN× square matrix, we can construct an  (1)   (1)NN
−×− square matrix by removing 
the i
th
 row and the j
th
 column.  The determinant of this matrix is denoted by 
ij
Mand is the minor of 
ij
A
.  The cofactor of 
ij
A
 is defined by 
 
 
cof(  1)
ij
ijij
AM
+
=−
                                                       (0.26)                                                       
 
For example, if 
 
 
1112
2122
AA
A
AA
⎡⎤
=
⎢⎥
⎣⎦
                                                          (0.27)                                                          
 
then 
 
 
1122122121122211
,MAMAMAMA==== 
 
11221221
cofcof,etc.AAA   A==− 
 
The adjoint of an NN×matrix A, written  adjA, is an NN
× matrix given by 
 
 
11211
12222
12
cofcofcof
cofcofcof
adj
cofcofcof
N
N
NNNN
AAA
AAA
A
AAA
⋅⋅⋅
⎡⎤
⎢⎥
⋅⋅⋅
⎢⎥
⋅
⎢⎥
=
⎢⎥
⋅
⎢⎥
⎢⎥
⋅
⎢⎥
⋅⋅⋅
⎣⎦
                                (0.28)                                
 
The reader is cautioned that the designation “adjoint” is used in a different context in Section 18.  
However, no confusion should arise since the adjoint as defined in Section 18 will be designated by 
a different symbol.  It is possible to show that 
 
 
(adj   )(det   )(adj   )AAAIAA==                                              (0.29)                                              
 
We shall prove (0.29) in general later in Section 21; so we shall be content to establish it for 
2N= 
here.  For 
2N= 
 
 
2212
2111
adj
AA
A
AA
−
⎡⎤
=
⎢⎥
−
⎣⎦
 
 
Then 
 

Chap. 0 • ELEMENTARY MATRIX THEORY 9 
 
1112222111221221
2122121111221221
0
(adj   )
0
AA A    A    AAAA
AA
AA   A AAAAA
−−
⎡⎤⎡  ⎤⎡⎤
==
⎢⎥⎢  ⎥⎢⎥
−−
⎣⎦⎣  ⎦⎣⎦
 
 
Therefore 
 
 
11221221
(adj   )()(det   )AA AAAAIAI=−= 
 
Likewise 
 
                                                                 (adj   )(det   )AAAI
= 
 
If 
det0A≠, then (0.29) shows that the inverse 
1
A
−
exists and is given by 
 
 
1
adj
det
A
A
A
−
=
                                                            (0.30)                                                            
 
Of course, if 
1
A
−
exists, then 
1
(det   )(det)det1    0AAI
−
==≠.  Thus nonsingular matrices are those 
with a nonzero determinant. 
 
Matrix notation is convenient for manipulations of systems of linear algebraic equations.   
For example, if we have the set of equations 
 
 
11   112    213   311
21   122    223   322
112 23 3
NN
NN
NN NNNNN
Ax   Ax    AxA xy
Ax   Ax   AxA xy
Ax  A x   AxA x    y
+++⋅⋅⋅+ =
+++⋅⋅⋅+ =
⋅
⋅
⋅
+++⋅⋅⋅+ =
 
 
then they can be written 
 
 
1112111
2122222
12
N
N
NNNNNN
AAA x   y
AAA x   y
AAA x   y
⋅⋅⋅
⎡⎤⎡⎤⎡⎤
⎢⎥⎢⎥⎢⎥
⎢⎥⎢⎥⎢⎥
⋅⋅⋅
⎢⎥⎢⎥⎢⎥
=
⎢⎥⎢⎥⎢⎥
⋅⋅⋅
⎢⎥⎢⎥⎢⎥
⎢⎥⎢⎥⎢⎥
⋅⋅⋅
⎢⎥⎢⎥⎢⎥
⋅⋅⋅
⎣⎦⎣⎦⎣⎦
 
 
The above matrix equation can now be written in the compact notation 
 
 
AXY=                                                               (0.31)                                                               

10 Chap. 0           •           ELEMENTARY MATRIX THEORY 
and if Ais nonsingular the solution is 
 
 
1
XAY
−
=                                                              (0.32)                                                              
 
For 
2N=, we can immediately write 
 
 
122121
221112
1
det
xAAy
xAAy
A
−
⎡⎤⎡⎤⎡⎤
=
⎢⎥⎢⎥⎢⎥
−
⎣⎦⎣⎦⎣⎦
                                             (0.33)                                             
 
 
Exercises 
 
0.1 Add the matrices 
 
 
15
26
−
⎡⎤⎡⎤
+
⎢⎥⎢⎥
⎣⎦⎣⎦
 
 
 Add the matrices 
 
 
2372  25 4
5434 33
iii
iii   ii
+−
⎡⎤⎡⎤
+
⎢⎥⎢⎥
+−+−
⎣⎦⎣⎦
 
 
0.2       Add       
 
 
15
3
26
i
i
−
⎡⎤⎡⎤
+
⎢⎥⎢⎥
⎣⎦⎣⎦
 
 
0.3       Multiply       
 
 
28
2372
16
543
32
i
ii
i
ii
i
⎡⎤
+
⎡⎤
⎢⎥
⎢⎥
⎢⎥
+
⎣⎦
⎢⎥
⎣⎦
 
 
0.4 Show that the product of two upper (lower) triangular matrices is an upper lower triangular 
matrix.  Further, if 
 
 
,
ijij
AABB
⎡⎤⎡⎤
==
⎣⎦⎣⎦
 
 
are upper (lower) triangular matrices of order 
NN×, then 
 
                                                        () ()
iiiiiiii
ABBAA B== 
 

Chap. 0 • ELEMENTARY MATRIX THEORY 11 
for all 1,...,iN=.  The off diagonal elements  ()
ij
ABand  ()
ij
BA, ij≠, generally are not 
equal, however. 
 
0.5 What is the transpose of 
 
 
2372
543
ii
ii
+
⎡⎤
⎢⎥
+
⎣⎦
 
 
0.6  What is the inverse of 
 
 
531
072
140
⎡⎤
⎢⎥
⎢⎥
⎢⎥
⎣⎦
 
 
0.7  Solve the equations 
 
 
53 2
49
xy
xy
+=
+=
 
 
by use of (0.33).

 
 
 

 13 
 
___________________________________________________________ 
Chapter 1 
 
 
SETS, RELATIONS, AND FUNCTIONS 
 
 
The purpose of this chapter is to introduce an initial vocabulary and some basic concepts.  
Most of the readers are expected to be somewhat familiar with this material; so its treatment is 
brief. 
 
 
Section 1. Sets and Set Algebra 
 
 The concept of a set is regarded here as primitive.  It is a collection or family of things 
viewed as a simple entity.  The things in the set are called elements or members of the set.  They  
are said to be contained in or to belong to the set.  Sets will generally be denoted by upper case 
script letters, 
,,,,ABCD
..., and elements by lower case letters    ,  ,  ,   ,abcd....  The sets of complex 
numbers, real numbers and integers will be denoted by C, R, and 
I
, respectively.  The notation 
a∈Ameans that the element a is contained in the set A; if ais not an element of A, the 
notation 
a∉Ais employed.  To denote the set whose elements are    ,  ,  ,abcand   ,d the notation 
{}
,,,abcd
is employed.  In mathematics a set is not generally a collection of unrelated objects like a 
tree, a doorknob and a concept, but rather it is a collection which share some common property like 
the vineyards of France which share the common property of being in France or the real numbers 
which share the common property of being real.  A set whose elements are determined by their 
possession of a certain property is denoted by 
{}
()xPx, where xdenotes a typical element and 
()Px is the property which determines xto be in the set. 
 
If 
Aand Bare sets, Bis said to be a subset of Aif every element of B is also an element 
of 
A.  It is customary to indicate that B is a subset of A by the notation ⊂BA, which may be 
read as “
B
is contained in 
A
,” or 
⊃AB
 which may be read as “
A
 contains 
B
.”  For example, 
the set of integers 
I is a subset of the set of real numbers R, ⊂IR.  Equality of two sets Aand 
B is said to exist if Ais a subset of B and Bis a subset of A; in equation form 
 
 
 and ⇔⊂⊂A=B   A  B   B  A                                                (1.1)                                                
 
A nonempty subset 
Bof A is called a proper subset of Aif Bis not equal to A.  The set of 
integers 
I is actually a proper subset of the real numbers.  The empty set or null set is the set with 
no elements and is denoted by 
∅.  The singleton is the set containing a single element a and is 
denoted by 
{}a
.  A set whose elements are sets is often called a class. 
 

14 Chap. 1 • SETS, RELATIONS, FUNCTIONS 
Some operations on sets which yield other sets will now be introduced.  The union of the 
sets 
Aand B is the set of all elements that are either in the set Aor in the setB.  The union of 
A
and 
B
 is denoted by 
AB
 
 
 
{| or }aaa     ABA    B                                                (1.2)                                                
 
It is easy to show that the operation of forming a union is commutative, 
 
 
ABBA                                                          (1.3)                                                          
 
and associative 
 
 
()( ) ABC ABC                                                 (1.4)                                                 
 
The 
intersection of the sets Aand Bis the set of elements that are in both Aand B.  The 
intersection is denoted by 
AB
 and is specified by 
 
 
{   | and }aaa  ABAB
                                               (1.5)                                               
 
Again, it can be shown that the operation of intersection is commutative 
 
 
ABBA                                                           (1.6)                                                           
 
and associative 
 
                                                       ()( )
 ABC ABC                                                 (1.7)                                                 
 
Two sets are said to be disjoint if they have no elements in common, i.e. if 
 
 
AB                                                              (1.8)                                                              
 
The operations of union and intersection are related by the following distributive laws: 
 
 
()( )( )
()( )( )
 
 
ABC AB AC
ABC AB AC
                                           (1.9)                                           
 
The 
complement of the setBwith respect to the set Ais the set of all elements contained in A but 
not contained in
B.  The complement of Bwith respect to the set Ais denoted by /AB and is 
specified by 
 
                                                             /{   |                                                             and                                                             }
aaaABAB                                              (1.10)                                              
 
It is easy to show that 
 

Sec. 1 • Sets and Set Algebra 15 
 /(/    )    for    AAB BB A                                             (1.11)                                             
 
and 
 
 
/AB A  A B                                                 (1.12)                                                 
 
 
Exercises 
 
1.1 List all possible subsets of the set 

,,,abcd. 
1.2 List two proper subsets of the set 

,ab
. 
1.3 Prove the following formulas: 
            (a)            
AAB BA
. 
            (b)            
AA. 
            (c)            
AAA. 
1.4       Show       that       
IRI. 
1.5 Verify the distributive laws (1.9). 
1.6 Verify the following formulas: 
            (a)            
AB ABA. 
            (b)            
A. 
            (c)            
AAA
. 
1.7 Give a proof of the commutative and associative properties of the intersection operation. 
1.8       Let       

1,   2 ,   3,   4  A, 

1, 0, 1, 2, 3, 7B, 

0C, and 

7,   5,   3,   1,1, 2, 3   D.  List 
the elements of ,,,,,/
ABABACBCABCAB, and  (    /   )DC  A. 

16 Chap. 1 • SETS, RELATIONS, FUNCTIONS 
 
 
Section 2.     Ordered Pairs, Cartesian Products, and Relations 
 
The idea of ordering is not involved in the definition of a set.  For example, the set 
{,}ab is 
equal to the set 
{, }ba.  In many cases of interest it is important to order the elements of a set.  To 
define an 
ordered pair (,)ab we single out a particular element of {,}ab, say a, and define (,)ab 
to be the class of sets; in equation form, 
 
 
(,)  {{},{,}}aba   ab≡                                                        (2.1)                                                        
 
This ordered pair is then different from the ordered pair 
(, )ba which is defined by 
 
 
(, )  {{},{, }}bab   ba≡                                                        (2.2)                                                        
 
It follows directly from the definition that 
 
                                                      (  ,  )(  ,   )                                                      and                                                      
abcda   cb   d=⇔== 
 
Clearly, the definition of an ordered pair can be extended to an ordered 
N-tuple 
12
(   ,    ,...,)
N
aaa. 
 
            The            
Cartesian Product of two sets AandBis a set whose elements are ordered pairs  (  ,  )ab 
where 
ais an element of Aand bis an element ofB.  We denote the Cartesian product of 
A
and
B
by
×AB
. 
 
 
{}
(,)|,ab  ab×=∈  ∈ABA  B                                                (2.3)                                                
 
It is easy to see how a Cartesian product of 
N
 sets can be formed using the notion of an ordered N-
tuple. 
 
Any subset 
Kof×AB defines a relation from the set Ato the setB.  The notation abKis 
employed if  (  ,  )
ab∈K; this notation is modified for negation as 
ab/K
 if  (  ,  )ab∉K.  As an 
example of a relation let 
Abe the set of all Volkswagons in the United States and let Bbe the set 
of all Volkswagons in Germany.  Let be 
⊂×KAB
 be defined so that abK if bis the same color 
as 
a. 
 
            If            
=AB, then Kis a relation onA.  Such a relation is said to be reflexive if aaK for all 
a∈A
, it is said to be symmetric if abK whenever baK for any aand bin 
A
, and it is said to be 
transitive if 
abK and bcKimply that acK for any    ,aband cin A.  A relation on a set Ais said 
to be an 
equivalence relation if it is reflexive, symmetric and transitive.  As an example of an 
equivalence relation 
K
on the set of all real numbers 
R
let ab  a b⇔=Kfor all    ,ab.  To verify that 
this is an equivalence relation, note that 
aa=
for each a∈K(reflexivity), 
ab ba=⇒=
 for each 
a∈K
(symmetry), and ,abbc  ac==⇒=for    ,aband cin 
K
(transitivity). 
 

Sec. 2              •              Ordered Pairs, Cartesian Products, Relations 17 
A relation 
Kon Ais antisymmetric if abKthen baK implies ba=.  A relation on a set 
A
is said to be a partial ordering if it is reflexive, antisymmetric and transitive.  The equality 
relation is a trivial example of partial ordering.  As another example of a partial ordering on 
R, let 
the inequality 
≤be the relation.  To verify that this is a partial ordering note that aa≤for every 
a∈R
(reflexivity), ab≤and ba  ab≤⇒=for all    ,ab∈R(antisymmetry), and ab≤and 
bc  ac≤⇒≤for all    ,  ,abc∈R(transitivity).  Of course inequality is not an equivalence relation 
since it is not symmetric. 
 
 
Exercises 
 
2.1 Define an ordered triple  (  ,  ,  )
abc by 
 
 
(,,)  ((,),)abcab c=
 
 
and show that 
(,,)  (,, ),abcd e fa   d b   e=⇔==
, and 
cf=
. 
 
2.2       Let       
Abe a set and Kan equivalence relation onA.  For each a∈Aconsider the set of all 
elements 
xthat stand in the relation K to a;  this set is denoted by 
{}
|xaxK, and is called 
an equivalence set. 
 (a) Show that the equivalence set of 
acontains a. 
(b) Show that any two equivalence sets either coincide or are disjoint. 
 
Note:  (a) and (b) show that the equivalence sets form a partition of 
A;  Ais the disjoint 
union of the equivalence sets. 
 
2.3 On the set 
R
of all real numbers does the strict inequality < constitute a partial ordering on 
the set? 
 
2.4 For ordered triples of real numbers define 
1112 2 2
(,,) ( , , )abca bc≤if 
1 21212
,,aabbcc≤≤≤.  
Does this relation define a partial ordering on 
××RRR? 

18 Chap. 1 • SETS, RELATIONS, FUNCTIONS 
 
 
 
Section 3.     Functions 
 
            A            relation            
ffrom 
X
 to 
Y
 is said to be a function (or a mapping) if 
1
(,  )xyf∈ and 
2
(,  )xyf∈imply 
12
yy= for all
x∈X
.  A function is thus seen to be a particular kind of relation 
which has the property that for each 
x∈Xthere exists one and only one y∈Ysuch that 
(, )xyf∈.  Thus a function f defines a single-valued relation.  Since a function is just a 
particular relation, the notation of the last section could be used, but this is rarely done.  A standard 
notation for a function from 
Xto Yis 
 
:
f→XY 
 
or 
 
 
()yfx= 
 
which indicates the element 
y∈Ythat fassociates with x∈X.  The element yis called the value 
of
f at x.  The domain of the function f is the set X.  The range off is the set of ally for 
which there exists an
xsuch that ()yfx=.  We denote the range off by()fX.  When the domain 
of
f is the set of real numbers R, f is said to be a function of a real variable.  When()fXis 
contained in
R, the function is said to be real-valued.  Functions of a complex variable and 
complex- valued functions are defined analogously. A function of a real variable need not be real-
valued, nor need a function of a complex variable be complex-valued. 
 
If 
0
Xis a subset of 
0
,⊂XX  X, and :f→XY,the image of
0
X underf is the set  
 
 
{}
00
()()|ffxx≡∈XX                                                     (3.1)                                                     
 
and it is easy to prove that 
 
 
0
()   ()ff⊂XX                                                            (3.2)                                                            
 
Similarly, if
0
Y is a subset of
Y
, then the preimage of
0
Yunderf is the set 
 
 
{}
1
00
()|()fxfx
−
≡∈YY
                                                     (3.3)                                                     
 
and it is easy to prove that 
 
 
1
0
()f
−
⊂YX
                                                              (3.4)                                                              
 

Sec. 3              •              Functions  19 
A function 
fis said be into Yif    (    )fX is a proper subset of Y, and it is said to be ontoYif 
()f=XY.  A function that is onto is also called surjective.  Stated in a slightly different way, a 
function is onto if for every element 
y∈Ythere exists at least one element x∈Xsuch that 
()yfx=.  A function f is said to be one-to-one (or injective) if for every 
12
,xx∈X 
 
 
1212
()    ( )fxfx xx=⇒=                                                    (3.5)                                                    
 
In other words, a function is one-to-one if for every element (    )yf
∈Xthere exists only one 
element 
x∈Xsuch that(  )yfx=.  A function fis said to form a one-to-one correspondence (or 
to be bijective) fromXtoYif    (    )f=XY, and
fis one-to-one.  Naturally, a function can be onto 
without being one-to-one or be one-to-one without being onto.  The following examples illustrate 
these terminologies. 
 
1.
 Let 
5
()fxx= be a particular real valued function of a real variable.  This function is 
one-to-one because 
55
xz=implies xz=when ,xz∈Rand it is onto since for every 
y∈Rthere is some x such that 
5
yx=. 
2.
 Let 
3
()fxx=be a particular complex-valued function of a complex variable.  This 
function is onto but it is not one-to-one because, for example, 
 
 
3
1313
(1)()()1
2222
ff i  f i=−+  =−−  = 
 
where 
1i=−. 
3.
 Let 
()  2fxx=
 be a particular real valued function of a real variable where 
x
denotes 
the absolute value of 
x.  This function is into rather than onto and it is not one-to-one. 
 
If 
:f→XY
 is a one-to-one function, then there exists a function 
1
:()ff
−
→XX which 
associates with each 
()yf∈X
 the unique x∈X such that 
()yfx=
.  We call 
1
f
−
 the inverse of 
f
, written 
1
()xfy
−
=.  Unlike the preimage defined by (3.3), the definition of inverse functions is 
possible only for functions that are one-to-one.  Clearly, we have 
1
(( ))ff
−
=XX.  The 
composition of two functions 
:f→XY
and 
:g→→YZ
is a function :h→XZ defined by 
()( ())hx    g f x=
 for all x∈X.  The function h is written 
hgf=D
.  The composition of any finite 
number of functions is easily defined in a fashion similar to that of a pair.  The operation of 
composition of functions is not generally commutative, 
 
 gf  fg
≠DD 
 
Indeed, if gf
D is defined, fgD may not be defined, and even if gfDand fgD are both defined, 
they may not be equal.  The operation of composition is associative 
 

20 Chap. 1 • SETS, RELATIONS, FUNCTIONS 
 
                                                           ()()hgf   hg f
=DD  DD 
 
if each of the indicated compositions is defined.  The identity function 
:id→XX is defined as 
the function 
()id  xx= for all x∈X.  Clearly if fis a one-to-one correspondence from Xto Y, 
then 
 
 
11
,ffidff   id
−−
==
XY
DD 
 
where id
X
and id
Y
 denote the identity functions of XandY, respectively. 
 
To close this discussion of functions we consider the special types of functions called 
sequences.  A finite sequence of Nterms is a function 
fwhose domain is the set of the first 
Npositive integers 
{}
1, 2, 3,...,N.  The range of 
f
is the set of Nelements 
{}
(1),   (2),...,   (    )fffN, 
which is usually denoted by 
{}
12
,,...,
N
fff
.  The elements 
12
,,...,
N
fffof the range are called terms 
of the sequence.  Similarly, an infinite sequence is a function defined on the positive integers 
+
I
.   
The range of an infinite sequence is usually denoted by 
{}
n
f
, which stands for the infinite set 
{}
123
,,    ,...ff f.  A function gwhose domain is 
+
I
and whose range is contained in 
+
I
is said to be 
order preserving if 
mn<
 implies that    (   )(  )gm    gn< for all ,mn
+
∈I.  If fis an infinite sequence 
and gis order preserving, then the composition 
fgD is a subsequence of f.  For example, let 
 
 
1(1),4
n
nn
fng=+= 
 
Then 
 
 
() 1(4   1)
n
fgn=+D 
 
is a subsequence of 
f. 
 
 
Exercises 
 
3.1 Verify the results (3.2) and (3.4). 
3.2 How may the domain and range of the sine function be chosen so that it is a one-to-one 
correspondence? 
3.3 Which of the following conditions defines a function (  )
yfx=? 
 
 
22
1,1,,.xyxyxy+=    =∈R 
 
3.4 Give an example of a real valued function of a complex variable. 
3.5 Can the implication of (3.5) be reversed? Why? 
3.6 Under what conditions is the operation of composition of functions commutative? 

Sec. 3              •              Functions  21 
3.7 Show that the arc sine function 
1
sin
−
really is not a function from Rto R.  How can it be 
made into a function?

 
 
 

 23 
_______________________________________________________________________________ 
Chapter 2 
 
 
GROUPS, RINGS, AND FIELDS 
 
 
The mathematical concept of a group has been crystallized and abstracted from numerous 
situations both within the body of mathematics and without. Permutations of objects in a set is a 
group operation.  The operations of multiplication and addition in the real number system are group 
operations.  The study of groups is developed from the study of particular situations in which 
groups appear naturally, and it is instructive that the group itself be presented as an object for 
study.  In the first three sections of this chapter certain of the basic properties of groups are 
introduced.  In the last section the group concept is used in the definition of rings and fields. 
 
Section 4.  The Axioms for a Group 
 
Central to the definition of a group is the idea of a binary operation.  If G is a non-empty 
set, a binary operation on 
G is a function from ×CC to G.  If    ,         ,ab∈G the binary operation 
will be denoted by ∗ and its value byab∗.  The important point to be understood about a binary 
operation on G is that G is closed with respect to∗in the sense that if    , ab∈G then 
ab∗∈G
 also.  
A binary operation ∗on G is associative if 
 
 
()()
for all   ,  ,ab c a  bcabc∗∗=∗∗∈G (4.1) 
 
Thus, parentheses are unnecessary in the combination of more than two elements by an associative 
operation.  A semigroup is a pair 
()
,∗Gconsisting of a nonempty set G with an associative binary 
operation 
∗
.  A binary operation 
∗
on G is commutative if 
 
 for all   ,
ab baab∗=∗∈G (4.2) 
 
The multiplication of two real numbers and the addition of two real numbers are both examples of 
commutative binary operations.  The multiplication of two 
N by N matrices (N > 1) is a 
noncommutative binary operation.  A binary operation is often denoted by 
,,abab⋅Dor ab rather 
than byab∗; if the operation is commutative, the notation ab+ is sometimes used in place of 
ab∗. 
 
An element 
e∈G
that satisfies the condition 

24 Chap. 2           •           GROUPS, RINGS, FIELDS 
 
 
for all ea ae aa∗=∗=∈G (4.3) 
 
is called an identity element for the binary operation ∗ on the set 
G
.  In the set of real numbers 
with the binary operation of multiplication, it is easy to see that the number 1 plays the role of 
identity element.  In the set of real numbers with the binary operation of addition, 0 has the role of 
the identity element.  Clearly G contains at most one identity element in
e.  For if 
1
eis another 
identity element in G, then 
11
eaae a∗=∗ = for all 
a∈G
 also.  In particular, if we choose 
ae=
, 
then 
1
eee∗=
.  But from (4.3), we have also 
11
eee∗=
.  Thus 
1
ee=
.  In general, 
G
 need not 
have any identity element.  But if there is an identity element, and if the binary operation is 
regarded as multiplicative, then the identity element is often called the unity element; on the other 
hand, if the binary operation is additive, then the identity element is called the zero element. 
 
In a semigroup 
G containing an identity element e with respect to the binary operation 
∗
, 
an element 
1
a
−
 is said to be an inverse of the element a if 
 
 
11
aa   a   a e
−−
∗=∗= (4.4) 
 
In general, 
a need not have an inverse.  But if an inverse 
1
a
−
 of  a exists, then it is unique, the 
proof being essentially the same as that of the uniqueness of the identity element.  The identity 
element is its own inverse.  In the set 
{}
/0R
 with the binary operation of multiplication, the 
inverse of a number is the reciprocal of the number.  In the set of real numbers with the binary 
operation of addition the inverse of a number is the negative of the number. 
 
A group is a pair 
()
,∗G consisting of an associative binary operation ∗ and a set 
G
 which 
contains the identity element and the inverses of all elements of G with respect to the binary 
operation∗.  This definition can be explicitly stated in equation form as follows. 
 
Definition.  A group is a pair 
()
,∗Gwhere 
G
 is a set and ∗ is a binary operation satisfying the 
following: 
(a) 
()
()ab c a  bc∗∗=∗∗for all    ,  ,abc∈G. 
(b) There exists an element 
e∈Gsuch that ae ea a∗=∗= for all a∈G. 
(c)        For        every        a∈ Gthere exists an element 
1
a
−
∈ G such that  
11
.aaa   a e
−−
∗=∗= 
 
If the binary operation of the group is commutative, the group is said to be a commutative (or 
Abelian) group. 

Sec. 4              •              The Axioms for a Group 25 
 
The set
 
{}
/0R with the binary operation of multiplication forms a group, and the set R 
with the binary operation of addition forms another group.  The set of positive integers with the 
binary operation of multiplication forms a semigroup with an identity element but does not form a 
group because the condition © above is not satisfied. 
 
A notational convention customarily employed is to denote a group simply by 
G rather than 
by the pair 
()
,∗G.  This convention assumes that the particular ∗ to be employed is understood.  
We shall follow this convention here. 
 
 
Exercises 
 
4.1 Verify that the set 
{}
1,  , -1, -ii∈G, where 
2
1i=−
, is a group with respect to the binary 
 operation of multiplication. 
4.2 Verify that the set 
G consisting of the four 
22×
matrices 
 
100 10  11 0
,,,
01101 00   1
−−
⎡⎤   ⎡ ⎤ ⎡ ⎤ ⎡  ⎤
⎢⎥   ⎢ ⎥ ⎢ ⎥ ⎢  ⎥
−−
⎣⎦   ⎣ ⎦ ⎣ ⎦ ⎣  ⎦
 
 
constitutes a group with respect to the binary operation of matrix multiplication. 
4.3 Verify that the set 
G consisting of the four 22× matrices 
 
10101 01 0
,,,
010 10  10   1
−−
⎡⎤   ⎡ ⎤ ⎡ ⎤ ⎡  ⎤
⎢⎥   ⎢ ⎥ ⎢ ⎥ ⎢  ⎥
−−
⎣⎦   ⎣ ⎦ ⎣ ⎦ ⎣  ⎦
 
  
constitutes a group with respect to the binary operation of matrix multiplication. 
4.4 Determine a subset of the set of all 
33×matrices with the binary operation of matrix 
multiplication that will form a group. 
4.5       If       
A is a set, show that the set G, 
{}
:,    is a one-to-one correspondencefff=→GAA 
is a one-to-one correspondence}constitutes a group with respect to the binary operation of 
composition. 

26 Chap. 2 • GROUPS, RINGS, FIELDS 
 
Section 5. Properties of a Group 
 
In this section certain basic properties of groups are established.  There is a great economy 
of effort in establishing these properties for a group in general because the properties will then be 
possessed by any system that can be shown to constitute a group, and there are many such systems 
of interest in this text.  For example, any property established in this section will automatically hold 
for the group consisting of 
{}
/0R with the binary operation of multiplication, for the group 
consisting of the real numbers with the binary operation of addition, and for groups involving 
vectors and tensors that will be introduced in later chapters. 
 
The basic properties of a group G in general are: 
1.
 The identity element 
e∈G
 is unique. 
2.
 The inverse element 
1
a
−
 of any element a∈G is unique.  The proof of Property 1 
was given in the preceding section.  The proof of Property 2 follows by essentially the same 
argument. 
If 
n is a positive integer, the powers of a∈Gare defined as follows:  (i) For 
1
1,naa==.  
(ii) For 
1
1,.
nn
naa a
−
>=∗  (iii) 
0
.ae=  (iv) 
()
1
.
n
n
aa
−−
=
  
3. If    ,   ,mnk are any integers, positive or negative, then for a∈G 
 
()
,(  ),
k
mnmnm nmnmnmknk
aa aaaaa
+++
∗=== 
 
In particular, when 
1mn==−, we have: 
4. 
()
1
1
aa
−
−
= for all a∈G 
5. 
()
1
11
abb   a
−
−−
∗=∗for all    ,ab∈G 
 
For square matrices the proof of this property is given by (0.17); the proof in general is exactly the 
same. 
The following property is a useful algebraic rule for all groups; it gives the solutions 
x and 
y to the equations xab∗= and ,ay b∗= respectively. 
6.
 For any elements    ,abin G, the two equations xab∗= and ay b∗= have 
the unique solutions 
1
xba
−
=∗ and 
1
.yab
−
=∗ 
 

Sec. 5              •              Properties of a Group  27 
Proof.  Since the proof is the same for both equations, we will consider only the equation 
.xab∗=  Clearly 
1
xba
−
=∗satisfies the equation ,xab∗= 
 
()()
11
xaba  aba abeb
−−
∗= ∗  ∗=∗  ∗ =∗=
 
 
Conversely,               ,
xab∗=implies 
 
()
()
111
xxe x  aaxa  aba
−−−
=∗=∗ ∗  = ∗ ∗ =∗
 
 
which is unique.  As a result we have the next two properties. 
7. For any three elements    ,  ,
abc in G, either ac bc∗=∗ or ca cb∗=∗ implies that 
ab=. 
8. For any two elements    ,
ab in the group G, either ab b∗= or ba b∗= implies that 
a is the identity element. 
A non-empty subset 
'G of G is a subgroup of G if 'G is a group with respect to the binary 
operation of 
G, i.e., 'G is a subgroup of G if and only if (i) 'e∈G (ii) 
1
''aa
−
∈⇒∈GG (iii) 
,''
aba  b∈⇒∗∈GG. 
9.         Let         
'G be a nonempty subset of 
G
.  Then 'G is a subgroup if 
1
,''aba  b
−
∈⇒∗∈GG. 
 
Proof.
  The proof consists in showing that the conditions (i), (ii), (iii) of a subgroup are satisfied.  
(i) Since 'G is non-empty, it contains an element a, hence 
1
'aae
−
∗=∈G.  (ii) If 'b∈G then 
11
'ebb
−−
∗=∈G.  (iii) If    ,'ab∈G, then 
()
1
1
'abab
−
−
∗=∗∈G. 
If 
G is a group, then G itself is a subgroup of G, and the group consisting only of the element e is 
also a subgroup of 
G.  A subgroup of G other than G itself and the group e is called a proper 
subgroup
 of 
G
.. 
10.
 The intersection of any two subgroups of a group remains a subgroup. 
 
 
Exercises 
 
5.1       Show       that       e is its own inverse. 
5.2 Prove Property 3. 

28 Chap. 2 • GROUPS, RINGS, FIELDS 
5.3 Prove Properties 7 and 8. 
5.4       Show       that       
xy= in Property 6 if G is Abelian. 
5.5       If       
G is a group and a∈G show that aa a∗= implies ae=. 
5.6 Prove Property 10. 
5.7 Show that if we look at the non-zero real numbers under multiplication, then (a) the rational 
numbers form a subgroup, (b) the positive real numbers form a subgroup, (c) the irrational 
numbers do not form a subgroup. 

Sec. 6              •              Group Homorphisms   29 
 
Section 6.     Group Homomorphisms 
 
A “group homomorphism” is a fairly overpowering phrase for those who have not heard it 
or seen it before.  The word homomorphism means simply the same type of formation or structure.  
A group homomorphism has then to do with groups having the same type of formation or structure. 
 
Specifically, if 
G and H are two groups with the binary operations 
∗
 and 
,D
 respectively, 
a function :
f→GHis a homomorphism if 
 
 
()()()
for all    ,fab   fa  fbab∗=∈GD (6.1) 
 
If a homomorphism exists between two groups, the groups are said to 
be homomorphic.  A 
homomorphism     :
f→GH is an isomorphism if f is both one-to-one and onto.  If an 
isomorphism exists between two groups, the groups are said to be 
isomorphic.  Finally, a 
homomorphism     :
f→GG is called an endomorphism and an isomorphism :f→GG is called an 
automorphism.  To illustrate these definitions, consider the following examples: 
 
1.         Let         
G be the group of all nonsingular, real, NN× matrices with the binary operation of 
matrix multiplication.  Let 
H
 be the group 
{}
/0Rwith the binary operation of scalar 
multiplication.  The function that is the determinant of a matrix is then a homomorphism 
from 
G to H. 
2.         Let         
G be any group and let H be the group whose only element is.e  Let f be the 
function that assigns to every element of 
G the value   ;e then fis a (trivial) 
homomorphism. 
3.         The         identity         function         
id :→GG is a (trivial) automorphism of any group. 
4.         Let         
G be the group of positive real numbers with the binary operation of multiplication and 
let 
H  be the group of real numbers with the binary operation of addition.  The log function 
is an isomorphism between 
G
 and 
H
. 
5.         Let         
G be the group 
{}
/0R with the binary operation of multiplication.  The function that 
takes the absolute value of a number is then an endomorphism of 
G
 into 
G
.  The restriction 
of this function to the subgroup 
H of H consisting of all positive real numbers is a 
(trivial) automorphism of 
H, however. 
 
From these examples of homomorphisms and automorphisms one might note that a 
homomorphism maps identities into identities and inverses into inverses.  The proof of this 

30 Chap. 2 • GROUPS, RINGS, FIELDS 
observation is the content of Theorem 6.1 below.  Theorem 6.2 shows that a homomorphism takes 
a subgroup into a subgroup and Theorem 6.3 proves a converse result. 
Theorem 6.1.  If 
:f→GH
 is a homomorphism, then 
()
fe coincides with the identity element 
0
eof 
H
 and  
 
 
()
()
1
1
fafa
−
−
= 
 
Proof.  Using the definition (6.1) of a homomorphism on the identity ,aae=∗one finds that  
 
 
()()()()
fafaefafe=∗=D 
 
From Property 8 of a group it follows that 
()
fe
 is the identity element 
0
e of H.  Now, using this 
result and the definition of a homomorphism (6.1) applied to the equation 
1
,eaa
−
=∗ we find  
 
 
()
()
()
()
11
0
efefaa   fafa
−−
==∗=D 
 
It follows from Property 2 that 
()
1
fa
−
is the inverse of 
()
.fa 
Theorem 6.2.  If :f→GH is a homomorphism and if 'G is a subgroup of G, then (')fG is a 
subgroup of 
H. 
Proof.  The proof will consist in showing that the set 
(')fG
satisfies the conditions 
()( )()
,,iiiiiiof 
a subgroup.  
()
i
Since 'e∈G it follows from Theorem 6.1 that the identity element 
0
e of H is 
contained in 
(')fG
.  
()
iiFor any 'a∈G, 
()
1
(')faf
−
∈G
 by Theorem 6.1.  
()
iiiFor any    ,'ab∈G, 
()  ()
(')fa  fb   f∈GD
since 
()  ()()
(')fa  fb   fab   f=∗∈GD
. 
 
As a corollary to Theorem 6.2 we see that    (   )fGis itself a subgroup of 
H. 
 
Theorem 6.3.  If     :f→GH is a homomorphism and if 'His a subgroup of H , then the 
preimage 
1
(')f
−
His a subgroup of G. 
 
The kernel of a homomorphism :f→GHis the subgroup 
()
1
0
fe
−
of 
G
.  In other words, 
the kernel of 
f is the set of elements of G  that are mapped by f to the identity element 
0
e of H.  

Sec. 6              •              Group Homorphisms   31 
The notation     (   )Kfwill be used to denote the kernel of 
f.  In Example 1 above, the kernel of f 
consists of NN×matrices with determinant equal to the real number 1, while in Example 2 it is 
the entire group 
G
.  In Example 3 the kernel of the identity map is the identity element, of course. 
 
Theorem 6.4.  A homomorphism :f→GHis one-to-one if and only if 
(){}
.Kfe=
 
 
Proof.  This proof consists in showing that 
()()
fafb= implies ab=if and only if 
(){}
.Kfe= 
If 
()  ()
,fafb=  then 
()  ()
1
0
,fafb   e
−
=D hence 
()
1
0,
fabe
−
∗=and it follows that 
()
1
.ab    Kf
−
∗∈
  Thus, if 
(){}
,Kfe=
then 
1
ab    e
−
∗= or .ab=  Conversely, now, we assume f 
is  
one-to-one and since 
()
Kfis a subgroup of 
G
 by Theorem 6.3, it must contain e. If 
()
Kfcontains any other element a such that 
()
0,
fae=we would have a contradiction since 
f
 is 
one-to-one; therefore 
(){}
.Kfe=
 
 
Since an isomorphism is one-to-one and onto, it has an inverse which is a function from 
H
 
onto 
G.  The next theorem shows that the inverse is also an isomorphism. 
 
Theorem 6.5.  If :f→GHis an isomorphism, then 
1
:f
−
→HGis an isomorphism. 
 
Proof   Since   
f is one-to-one and onto, it follows that 
1
f
−
is also one-to-one and onto (cf. Section 
3).  Let 
0
abe the element of H such that 
()
1
0
af a
−
= for any a∈G;  then 
()   ()
11
00
.ab  f   a    f   b
−−
∗=∗
  But ab∗is the inverse image of the element 
00
ab∈HDbecause 
()()()
00
fab   fa  fb   a b∗==∗D since f is a homomorphism.  Therefore 
()  ()   (   )
11 1
00   00
,fafb fab
−− −
∗=D which shows that 
1
f
−
satisfies the definition (6.1) of a 
homomorphism. 
 
Theorem 6.6.  A homomorphism :f→GHis an isomorphism if it is onto and if its kernel 
contains only the identity element of 
G. 
 
The proof of this theorem is a trivial consequence of Theorem 6.4 and the definition of an 
isomorphism. 
 
 

32 Chap. 2 • GROUPS, RINGS, FIELDS 
Exercises 
 
6.1
 If :f→GHand     :g→HM are homomorphisms, then show that the composition of the 
            mappings            
f and g is a homomorphism from G to M. 
6.2
 Prove Theorems 6.3 and 6.6. 
6.3
 Show that the logarithm function is an isomorphism from the positive real numbers under 
multiplication to all the real numbers under addition. What is the inverse of this 
isomorphism? 
6.4
 Show that the function :f
()
()
:,,f
+
+→⋅RRdefined by 
()
2
fxx= is not a 
homomorphism. 

Sec. 7              •              Rings and Fields  33 
 
Section 7.     Rings and Fields 
 
Groups and semigroups are important building blocks in developing algebraic structures.  In 
this section we shall consider sets that form a group with respect to one binary operation and a 
semigroup with respect to another.  We shall also consider a set that is a group with respect to two 
different binary operations. 
 
Definition.  A ring is a triple (,,)+⋅D consisting of a set D and two binary operations + and ⋅such 
that 
 
(a) 
D with the operation 
+
 is an Abelian group. 
(b)       The       operation       
⋅
 is associative. 
(c) D contains an identity element, denoted by 1, with respect to the operation 
⋅, i.e., 
 
 11aa   a⋅=⋅= 
 
for all a∈D. 
(d)       The       operations       
+ and ⋅ satisfy the distributive axioms 
 
 
()
()
abc  abac
bca baca
⋅+=⋅+⋅
+⋅=⋅+⋅
 (7.1) 
 
The operation + is called 
addition and the operation ⋅ is called multiplication.  As was done with 
the notation for a group, the ring  (   ,   , )
+⋅D will be written simply as D.  Axiom (a) requires that D 
contains the identity element for the 
+ operation.  We shall follow the usual procedure and denote 
this element by 
0.  Thus, 00.aaa+=+= Axiom (a) also requires that each element 
a∈D
 have 
an additive inverse, which we will denote by 
a−, 
()
0aa+−=.  The quantity ab+is called the 
sum of a and b, and the difference between a and b is 
()
,ab+−which is usually written as 
.ab−
  Axiom (b) requires the set D and the multiplication operation to form a semigroup.  If the 
multiplication operation is also commutative, the ring is called a 
commutative ring.  Axiom (c) 
requires that 
D
 contain an identity element for multiplication;  this element is called the unity 
element of the ring and is denoted by 1.  The symbol 1 should not be confused with the real number 
one.  The existence of the unity element is sometimes omitted in the ring axioms and the ring as we 
have defined it above is called the 
ring with unity.  Axiom (d), the distributive axiom, is the only 
idea in the definition of a ring that has not appeared before.  It provides a rule for the interchanging 
of the two binary operations. 
 

34 Chap. 2 • GROUPS, RINGS, FIELDS 
The following familiar systems are all examples of rings. 
1.         The         set         
I of integers with the ordinary addition and multiplication operations form a ring. 
2.         The         set         
a
R of rational numbers with the usual addition and multiplication operations form a 
            ring.            
3.         The         set         
R
 of real numbers with the usual addition and multiplication operations form a ring. 
4.         The         set         
C of complex numbers with the usual addition and multiplication operations form a 
            ring.            
5. The set of all 
N by N matrices form a ring with respect to the operations of matrix addition 
 and matrix multiplication.  The unity element is the unity matrix and the zero element is the 
            zero            matrix.            
6. The set of all polynomials in one (or several) variable with real (or complex) coefficients 
 form a ring with respect to the usual operations of addition and multiplication. 
 
Many properties of rings can be deduced from similar results that hold for groups.  For 
example, the zero element 
0, the unity element 
1
, the negative element a− of an element a are 
unique.  Properties that are associated with the interconnection between the two binary operations 
are contained in the following theorems: 
 
Theorem 7.1.  For any element 
a∈D
, 000.aa⋅=⋅= 
 
Proof.  For  any  b∈D we have by Axiom (a) that 0.bb+=  Thus for any a∈Dit follows that 
(0),
baba+⋅=⋅ and the Axiom (d) permits this equation to be recast in the form 
0.baa  ba⋅+⋅=⋅
  
From Property 8 for the additive group this equation implies that 
00.a⋅=  It can be shown that 
00a⋅=by a similar argument. 
 
Theorem 7.2.  
For all elements    ,ab∈D 
 
 
()()()
ab a  bab−⋅=⋅− =− ⋅ 
 
This theorem shows that there is no ambiguity in writing 
ab−⋅ for 
()
.ab−⋅ 
 
Many of the notions developed for groups can be extended to rings;  for example, subrings 
and ring homomorphisms correspond to the notions of subgroups and group homomorphisms.  The 
interested reader can consult the Selected Reading for a discussion of these ideas. 
 
The set of integers is an example of an algebraic structure called an 
integral domain. 

Sec. 7              •              Rings and Fields  35 
 
Definition.  
A ring Dis an integral domain if it satisfies the following additional axioms: 
 
(e)        The        operation        
⋅is commutative. 
(f)        If        ,  ,
abc are any elements of 
D
 with 0,c≠ then 
 
 ac  bc    a  b⋅=⋅⇒= (7.2) 
 
The cancellation law of multiplication introduced by Axiom (f) is logically equivalent to the 
assertion that a product of nonzero factors is nonzero.  This is proved in the following theorem. 
 
Theorem 7.3.  
A commutative ring 
D
 is an integral domain if and only if for all elements    ,ab∈D, 
0ab⋅≠ unless 0a= or 0.b= 
 
Proof.   Assume first that 
D
 is an integral domain so the cancellation law holds.  Suppose 0ab⋅= 
and 
0.b≠  Then we can write 0abb⋅=⋅and the cancellation law implies 0.a=  Conversely, 
suppose that a product in a commutative ring D cannot be zero unless one of its factors is zero.  
Consider the expression ,
ac  bc⋅=⋅ which can be rewritten as 0ac bc⋅−⋅=or as 
()
0.abc−⋅=  
If 
0c≠ then by assumption 0ab−=or .ab=  This proves that the cancellation law holds. 
 
The sets of integers 
I
, rational numbers 
a
R, real numbers R, and complex numbers C are 
examples of integral domains as well as being examples of rings.  Sets of square matrices, while 
forming rings with respect to the binary operations of matrix addition and multiplication, do not, 
however, form integral domains.  We can show this in several ways; first by showing that matrix 
multiplication is not commutative and, second, we can find two nonzero matrices whose product is 
zero, for example, 
 
 
5000    00
0001    00
⎡⎤⎡   ⎤  ⎡   ⎤
=
⎢⎥⎢   ⎥  ⎢   ⎥
⎣⎦⎣   ⎦  ⎣   ⎦
 
 
The rational numbers, the real numbers, and the complex numbers are examples of an 
algebraic structure called a 
field. 
 
Definition.  A field 
F
 is an integral domain, containing more than one element, and such that any 
element 
{}
/0a∈F
has an inverse with respect to multiplication. 
 

36 Chap. 2 • GROUPS, RINGS, FIELDS 
It is clear from this definition that the set F and the addition operation as well as the set 
{}
/0Fand the multiplication operation form Abelian groups.  Hence the unity element 1 , the zero 
element 
0, the negative 
(),a−
 as well as the reciprocal 
()
1/,0,aa≠ are unique.  The formulas of 
arithmetic can be developed as theorems following from the axioms of a field.  It is not our purpose 
to do this here;  however, it is important to convince oneself that it can be done. 
 
The definitions of ring, integral domain, and field are each a step more restrictive than the 
other.  It is trivial to notice that any set that is a field is automatically an integral domain and a ring.  
Similarly, any set that is an integral domain is automatically a ring. 
 
The dependence of the algebraic structures introduced in this chapter is illustrated 
schematically in Figure 7.1.  The dependence of the vector space, which is to be introduced in the 
next chapter, upon these algebraic structures is also indicated. 
 
 
 
Semigroup 
Group 
Abelian group 
Ring 
Commutative ring 
Integral domain 
Field 
Vector space
Figure 1.  A schematic of algebraic structures. 

Sec. 7              •              Rings and Fields  37 
Exercises 
 
7.1
 Verify that Examples 1 and 5 are rings. 
7.2
 Prove Theorem 7.2. 
7.3
 Prove that if    ,  ,  ,abcd are elements of a ring D, then 
(a) 
()()
.abab−⋅−=⋅ 
(b) 
()    ()
11.aaa−⋅=⋅−=− 
(c) 
()()
.a  b   c  d    ac  ad  bc  bd+ ⋅ + =⋅+⋅+⋅+⋅
 
7.4 Among the Examples 1-6 of rings given in the text, which ones are actually integral 
 domains, and which ones are fields? 
7.5 Is the set of rational numbers a field? 
7.6 Why does the set of integers not constitute a field? 
7.7       If       
F
 is a field, why is 
{}
/0Fan Abelian group with respect to multiplication? 
7.8 Show that for all rational numbers    ,
xy, the set of elements of form 2xy+constitutes a 
            field.            
7.9 For all rational numbers 
,,xyz, does the set of elements of the form 23xyz++ form a 
 field?  If not, show how to enlarge it to a field.

 
 

_______________________________________________________________________________ 
PART 2 
 
 
VECTOR AND TENSOR ALGEBRA 

 
Selected Readings for Part II 
 
FRAZER, R. A., W. J. DUNCAN, AND A. R. COLLAR, Elementary Matrices, Cambridge University 
Press, Cambridge, 1938. 
G
REUB, W. H., Linear Algebra, 3
rd
 ed., Springer-Verlag, New York, 1967. 
G
REUB, W. H., Multilinear Algebra, Springer-Verlag, New York, 1967. 
H
ALMOS, P. R., Finite Dimensional Vector Spaces, Van Nostrand, Princeton, New Jersey, 1958. 
JACOBSON, N., Lectures in Abstract Algebra, Vol. II. Linear Algebra, Van Nostrand, New York, 
            1953.            
M
OSTOW, G. D., J. H. SAMPSON, AND J. P. MEYER, Fundamental Structures of Algebra, McGraw-
 Hill, New York, 1963. 
N
OMIZU, K., Fundamentals of Linear Algebra, McGraw-Hill, New York, 1966. 
SHEPHARD, G. C., Vector Spaces of Finite Dimensions, Interscience, New York, 1966. 
V
EBLEN, O., Invariants of Quadratic Differential Forms, Cambridge University Press, Cambridge, 
1962. 

 
 41 
_____________________________________________________________________________ 
Chapter 3 
 
 
VECTOR SPACES 
 
 
Section 8.     The Axioms for a Vector Space 
 
Generally, one’s first encounter with the concept of a vector is a geometrical one in the 
form of a directed line segment, that is to say, a straight line with an arrowhead.  This type of 
vector, if it is properly defined, is a special example of the more general notion of a vector 
presented in this chapter.  The concept of a vector put forward here is purely algebraic.  The 
definition given for a vector is that it be a member of a set that satisfies certain algebraic rules. 
A vector space is a triple 
()
,,fVF consisting of (a) an additive Abelian group V, (b) a 
field F, and (c) a function :f×→FV   Vcalled scalar multiplication such that 
 
 
()
()
()
()()()
()()()
()
,,,
,,,
,,,
1,
fff
fff
fff
f
λμ    λμ
λμλμ
λλλ
=
+=  +
+=    +
=
vv
uuu
uvuv
vv
                                            (8.1)                                            
 
for all    ,
λμ∈F and all    ,∈uvV.  A vector is an element of a vector space.  The notation 
()
,,fVFfor a vector space will be shortened to simply V.  The first of (8.1) is usually called the 
associative law for scalar multiplication, while the second and third equations are distributive laws, 
the second for scalar addition and the third for vector addition. 
 
It is also customary to use a simplified notation for the scalar multiplication function 
f.  
We shall write 
()
,fλλ=vvand also regard λv and λv to be identical.  In this simplified notation 
we shall now list in detail the axioms of a vector space.  In this definition the vector 
+uv in V is 
called the sum of 
u and v and the difference of u and v is written −uvand is defined by 
 
 
()
−=+−uv u    v                                                           (8.2)                                                           
 
Definition.  Let V be a set and F a field.  V is a vector space if it satisfies the following rules: 
(a) There exists a binary operation in 
V
 called addition and denoted by + such that 

42  Chap. 3            •      VECTOR SPACES 
(1) 
()()
++=++uv  w u  vw for all    ,   ,∈uvwV. 
(2) 
+=+uv vu for all    ,∈uvV. 
(3) There exists an element 
∈0V such that +=u0 u for all .∈uV 
(4)       For       every       
∈uVthere exists an element .−∈uVsuch that 
()
+− =uu0
. 
(b) There exists an operation called scalar multiplication in which every scalar 
λ∈F 
can be combined with every element 
∈uV to give an element λ∈uVsuch that 
(1) 
()()
λμλμ=uu 
(2) 
()
λμλμ+=+uuu 
(3) 
()
λλλ+= +uvu  v 
(4) 
1=uu 
for all 
,λμ∈F
and all    ,∈uvV. 
 
If the field 
Femployed in a vector space is actually the field of real numbers ,R the space is 
called a real vector space.  A complex vector space is similarly defined. 
 
Except for a few minor examples, the material in this chapter and the next three employs 
complex vector spaces.  Naturally the real vector space is a trivial special case.  The reason for 
allowing the scalar field to be complex in these chapters is that the material on spectral 
decompositions in Chapter 6 has more usefulness in the complex case.  After Chapter 6 we shall 
specialize the discussion to real vector spaces.  Therefore, unless we provide some qualifying 
statement to the contrary, a vector space should be understood to be a complex vector space. 
 
There are many and varied sets of objects that qualify as vector spaces.  The following is a 
list of examples of vector spaces: 
1.
 The vector space 
N
C is the set of all N-tuples of  the form  
()
12
,,...,
N
λλλ=u, where N is a 
positive integer and 
12
,,...,
N
λλλ∈C.  Since an N-tuple is an ordered set, if 
()
12
,,...,
N
μμμ=v is a second N-tuple, then    anduvare equal if and only if 
 
 for all  1, 2,...,
kk
kNμλ== 
 
The zero 
N-tuple is 
()
0, 0,..., 0=0 and the negative of the N-tuple u is 
()
12
,,...,
N
λλλ−=− −   −u.  Addition and scalar multiplication of N-tuples are defined by the 
formulas 
 
()
112 2
,,...,
NN
μλμ λ  μ  λ+= +   ++uv 

Sec. 8              •              The Axioms for a Vector Space 43 
 
and 
 
()
12
,,....,
N
μμλ μλ    μλ=u 
 
 
respectively.  The notation 
N
C
 is used for this vector space because it can be considered to 
be an 
NthCartesian product ofC. 
2.         The         set
 V of all NM× complex matrices is a vector space with respect to the usual 
operation of matrix addition and multiplication by a scalar. 
3.         Let         
H be a vector space whose vectors are actually functions defined on a set Awith 
values in 
C
.  Thus, if 
∈hH
, x∈Athen 
()
x∈hC and    :→hAC.  If kis another vector of 
Hthen  equality of vectors is defined by  
 
()()
for allxxx=⇔   =∈hk  hkA 
 
The zero vector is the zero function whose value is zero for all 
x.  Addition and scalar 
multiplication are defined by 
 
()()()()
xxx+=+hkhk 
 
and 
()()()
()
xxλλ=hh 
respectively. 
4.         Let         
P
denote the set of all polynomials u of degree N of the form 
 
2
01   2
N
N
uxx  xλλ λλ=+ +  +⋅⋅⋅+
 
 
where
012
,,,...,
N
λλλ  λ∈C
.  The set Pforms a vector space over the complex numbers
 Cif 
addition and scalar multiplication of polynomials are defined in the usual way. 
5. The set of complex numbers 
,C
with the usual definitions of addition and multiplication by 
a real number, forms a real vector space. 
6. The zero element 
0 of any vector space forms a vector space by itself. 
 
The operation of scalar multiplication is not a binary operation as the other operations we 
have considered have been.  In order to develop some familiarity with the algebra of this operation 
consider the following three theorems. 
 

44  Chap. 3            •      VECTOR SPACES 
Theorem 8.1.  0λλ=⇔=u0 or =u0. 
 
Proof.  The proof of this theorem actually requires the proof of the following three assertions: 
(a)  0,     (b) ,      (c) 0 or λλλ===⇒==u000u0u0 
To prove (a), take 
0μ= in Axiom (b2) for a vector space; then 0.λλ=+uuu  Therefore 
 
0λλλλ−=−+uu uuu 
 
and by Axioms (a4) and (a3) 
 
00=+=00  u  u 
 
which proves (a). 
 
To prove (b), set 
=v0 in Axiom (b3) for a vector space; then .λλλ=+uu0  Therefore 
 
λλλλλ−=−+uu uu0
 
 
and by Axiom (a4) 
 
λλ=+=00  0   0 
 
 
To prove (c), we assume 
.λ=u0 If 0,λ= we know from (a) that the equation λ=u0 is 
satisfied.  If 0,
λ≠ then we show that umust be zero as follows: 
 
()()
11    1
1
λλ
λλ  λ
⎛⎞
===   =  =
⎜⎟
⎝⎠
uuuu00 
 
Theorem 8.2.  
()
λλ−=−uu
 for all ,λ∈∈uCV. 
 
Proof.  Let  0μ=and replace λ by λ− in Axiom (b2) for a vector space and this result follows 
directly. 
 
Theorem 8.3.  
()
λλ−=−uu for all ,λ∈∈uCV. 
 

Sec. 8              •              The Axioms for a Vector Space 45 
Finally, we note that the concepts of 
length and angle have not been introduced.  They will 
be introduced in a later section.  The reason for the delay in their introduction is to emphasize that 
certain results can be established without reference to these concepts. 
 
 
Exercises 
 
8.1 Show that at least one of the axioms for a vector space is redundant.  In particular, one 
might show that Axiom (a2) can be deduced from the remaining axioms.  [Hint: expand 
()( )
11++uv
by the two different rules.] 
8.2 Verify that the sets listed in Examples 1- 6 are actually vector spaces.  In particular, list the 
zero vectors and the negative of a typical vector in each example. 
8.3 Show that the axioms for a vector space still make sense if the field 
Fis replaced by a ring.  
The resulting structure is called a 
module over a ring. 
8.4  Show that the Axiom (b4) for a vector space can be replaced by the axiom 
 
()
40orbλλ
′
=⇔=   =u0u0
 
 
8.5       Let       
Vand Ube vector spaces.  Show that the set ×VU is a vector space with the 
definitions 
 
()()()
,,,+=+ +uxvyuvxy 
and 
 
()()
,,λλλ=uxu  x 
 
where    ,
∈uvV;    ,∈xyU; and λ∈F 
8.6  Prove Theorem 8.3. 
8.7  Let 
V
 be a vector space and consider the set 
.×VV
  Define addition in 
×VV
by 
 
()()()
,,,+=+ +uvxyuxvy 
 
and multiplication by complex numbers by 
 
()()()
,,iλμλμμλ+=−+uvuv   uv 
 
where    ,.
λμ∈R  Show that 
×VV
 is a vector space over the field of complex numbers. 

46  Chap. 3            •      VECTOR SPACES 
 
Section 9.     Linear Independence, Dimension, and Basis 
 
The concept of linear independence is introduced by first defining what is meant by linear 
dependence in a set of vectors and then defining a set of vectors that is not linearly dependent to be 
linearly independent.  The general definition of linear dependence of a set of N vectors is an 
algebraic generalization and abstraction of the concepts of collinearity from elementary geometry. 
 
Definition.  A finite set of 
()
1NN≥vectors 
{}
12
,,...,
N
vvvin a vector space 
V
 is said to be 
linearly dependent if there exists a set of scalars 
{}
12
,,...,
N
λλλ, not all zero, such that 
 
 
1
N
j
j
j
λ
=
=
∑
v0                                                              (9.1)                                                              
 
The essential content of this definition is that at least one of the vectors 
{}
12
,,...,
N
vvv can 
be expressed as a linear combination of the other vectors.  This means that if 
1
0,λ≠
 then  
1
2
N
j
j
i
μ
=
=
∑
vv,where 
1
/
jj
μλλ=− for 
2, 3,...,    .jN=
  As a numerical example, consider the two 
vectors 
 
()()
12
1, 2 , 3  ,3, 6, 9==vv 
 
from 
3
.R  These vectors are linearly dependent since  
 
21
3=vv 
 
The proof of the following two theorems on linear dependence is quite simple. 
 
Theorem 9.1.  If the set of vectors 
{}
12
,,,
N
vvv" is linearly dependent, then every other finite set 
of vectors containing 
{}
12
,,,
N
vvv"is linearly dependent. 
 
Theorem 9.2.  Every set of vectors containing the zero vector is linearly dependent. 
 

Sec. 9              •              Linear Independence, Dimension, Basis 47 
A set of 
()
1NN≥vectors that is not linearly dependent is said to be linearly independent.  
Equivalently, a set of  
()
1NN≥
vectors 
{}
12
,,...,
N
vvv
 is linearly independent if (9.1) implies 
12
...0
N
λλ   λ=== =
.  As a numerical example, consider the two vectors 
()
1
1, 2=v and 
()
2
2,1=v from 
2
.R  These two vectors are linearly independent because 
 
()()
12
1, 22 , 120, 20λλλ  μλμ λμ+=  +  =⇔+= +=vv0
 
 
and 
 
20,200,   0λμλμλμ+=  +=⇔=  = 
 
Theorem 9.3.  Every non empty subset of a linearly independent set is linearly independent. 
 
A linearly independent set in a vector space is said to be maximal if it is not a proper subset 
of any other linearly independent set.  A vector space that contains a (finite) maximal, linearly 
independent set is then said to be finite dimensional.  Of course, if 
V is not finite dimensional, 
then it is called infinite dimensional.  In this text we shall be concerned only with finite-
dimensional vector spaces. 
 
Theorem 9.4.  Any two maximal, linearly independent sets of a finite-dimensional vector space 
must contain exactly the same number of vectors. 
 
Proof.  Let  
{}
1
,...,
N
vv and 
{}
1
,....,
M
uu be two maximal, linearly independent sets of .V  Then we 
must show that 
.NM=  Suppose that 
,NM≠
 say .NM<  By the fact that 
{}
1
,...,
N
vv
 is 
maximal, the sets 
{}
{}
111
,...,,,...,,...,,
NNM
vvu   vvu are all linearly dependent.  Hence there exist 
relations 
 
 
11    111    1
11
NN
MMNNMM
λλμ
λλμ
+++=
+++=
vvu0
vvu0
"
#
"
                                              (9.2)                                              
 
where the coefficients of each equation are not all equal to zero.  In fact, the coefficients 
1
,...,
M
μμ 
of the vectors 
1
,...,
M
uu are all nonzero, for if 0
i
μ= for any i, then 
 
11iiNN
λλ++=vv0" 

48  Chap. 3            •      VECTOR SPACES 
 
for some nonzero 
1
,...,
iiN
λλ
 contradicting the assumption that 
{}
1
,...,
N
vv is a linearly independent 
set.  Hence we can solve (9.2) for the vectors 
{}
1
,....,
M
uu
 in terms of the vectors
{}
1
,...,
N
vv
, 
obtaining 
 
 
11111
11
NN
MMMNN
μμ
μμ
=++
=++
uvv
uvv
"
#
"
                                                   (9.3)                                                   
 
where 
/
ijiji
μλμ=−
 for 
1,...,;1,...,iMjN==
. 
 
Now we claim that the first N equations in the above system can be inverted in such a way 
that the vectors 
{}
1
,...,
N
vv
 are given by linear combinations of the vectors 
{}
1
,....,
N
uu
.  Indeed, 
inversion is possible if the coefficient matrix 
ij
μ
⎡⎤
⎣⎦
for 
,1,...,ijN= is nonsingular.  But this is 
clearly the case, for if that matrix were singular, there would be nontrivial solutions 
{}
1
,....,
N
αα
 to 
the linear system 
 
 
1
0,1,...,
N
iij
i
jNαμ
=
==
∑
                                                  (9.4)                                                  
 
Then from (9.3) and (9.4) we would have 
 
111
NNN
iii ijj
iji
ααμ
===
⎛⎞
==
⎜⎟
⎝⎠
∑∑∑
uv0 
 
contradicting the assumption that set 
{}
1
,...,
N
uu
,being a subset of the linearly independent set 
{}
1
,...,
M
uu, is linearly independent. 
 
Now if the inversion of the first 
N equations of the system (9.3) gives 
 
 
11111
11
...
NN
NNNNN
ξξ
ξξ
=++
=++
vuu
vuu
"
#
                                                     (9.5)                                                     
 

Sec. 9              •              Linear Independence, Dimension, Basis 49 
where 
ij
ξ
⎡⎤
⎣⎦
 is the inverse of 
ij
μ
⎡⎤
⎣⎦
 for ,1,...,ijN=,we can substitute (9.5) into the remaining 
MN−
 equations in the system (9.3), obtaining 
 
11
11
11
NN
NNjjii
ji
NN
MMjjii
ji
μξ
μξ
++
==
==
⎛⎞
=
⎜⎟
⎝⎠
⎛⎞
=
⎜⎟
⎝⎠
∑∑
∑∑
uu
uu
#
 
 
But these equations contradict the assumption that the set 
{}
1
,....,
M
uu is linearly independent.  
Hence 
MN> is impossible and the proof is complete. 
 
An important corollary of the preceding theorem is the following. 
 
Theorem 9.5.  Let 
{}
1
,....,
N
uube a maximal linearly independent set in ,V and suppose that 
{}
1
,...,
N
vv is given by (9.5).  Then 
{}
1
,...,
N
vv is also a maximal, linearly independent set if and 
only if the coefficient matrix 
ij
ξ
⎡⎤
⎣⎦
 in (9.5) is nonsingular.  In particular, if 
ii
=vufor 
1,...,1,1,...,ikkN=−+
 but 
kk
≠vu, then 
{}
11  1
,...,,,,...,
kkkN−+
uuvu   u is a maximal, linearly 
independent set if and only if the coefficient 
kk
ξ in the expansion of 
k
v in terms of 
{}
1
,...,
N
uu
 in 
(9.5) is nonzero. 
 
From the preceding theorems we see that the number 
N
of vectors in a maximal, linearly 
independent set in a finite dimensional vector space 
V is an intrinsic property of .V  We shall call 
this number 
N the dimension of ,Vwritten dim ,V namely dimN=,Vand we shall call any 
maximal, linearly independent set of 
V a basis of that space.  Theorem 9.5 characterizes all bases 
of 
V as soon as one basis is specified.  A list of examples of bases for vector spaces follows: 
 
1. The set of 
N vectors 
()
()
()
()
1, 0, 0,..., 0
0,1, 0,..., 0
0, 0,1,..., 0
0, 0, 0,...,1
N
#
	

 
 

50  Chap. 3            •      VECTOR SPACES 
is linearly independent and constitutes a basis for 
C,
N
 called  the standard basis. 
2.         If         
22×
M denotes the vector space of all 22× matrices with elements from the complex 
numbers 
,C then the four matrices 
10010000
,, ,
00001001
⎡⎤⎡⎤⎡⎤⎡⎤
⎢⎥⎢⎥⎢⎥⎢⎥
⎣⎦⎣⎦⎣⎦⎣⎦
 
 
form a basis for 
22×
M called the standard basis. 
3. The elements 1 and 
1i=− form a basis for the vector space 
C
 of complex numbers over 
 the field of real numbers. 
 
The following two theorems concerning bases are fundamental. 
 
Theorem 9.6.  If 
{}
12
,    ,...,
N
eee
 is a basis for 
,V
 then every vector in V has the representation 
 
 
1
N
j
j
j
ξ
=
=
∑
ve                                                               (9.6)                                                               
 
where 
{}
12
,,...,
N
ξξξ are elements of 
C
 which depend upon  the vector 
∈vV
. 
 
The proof of this theorem is contained in the proof of Theorem 9.4. 
 
Theorem 9.7.  The N scalars 
{}
12
,,...,
N
ξξξ in (9.6) are unique. 
 
Proof
.  As is customary in the proof of a uniqueness theorem, we assume a lack of uniqueness.  
Thus we say that 
vhas two representations, 
 
11
,
NN
jj
jj
jj
ξμ
==
==
∑∑
vev e 
 
Subtracting the two representations, we have 
 
()
1
N
jj
j
j
ξμ
=
−=
∑
e0
 
 

Sec. 9              •              Linear Independence, Dimension, Basis 51 
and the linear independence of the basis requires that 
 
,1, 2,...,
jj
jNξμ== 
 
The coefficients 
{}
12
,,...,
N
ξξξ in the representation (9.6) are the components of vwith respect to 
the basis 
{}
12
,    ,...,
N
eee.  The representation of vectors in terms of the elements of their basis is 
illustrated in the following examples. 
1.         The         vector         space         
3
C has a standard basis consisting of 
123
,,and;eee  
 
()()()
123
1, 0, 0  ,0,1, 0  ,0, 0,1===ee e 
 
A vector 
()
2,7,83ii=+  +v can be written as 
 
()()
123
2783ii=+ + ++vee   e
 
 
2. The vector space of complex numbers 
C
over the space of real numbers
R
has the basis 
 
{}
1,    .i
  Any complex number z can then be represented by 
 
ziμλ=+ 
 
where 
,μλ∈R. 
3.         The         vector         space         
22×
Mof all 22x matrices with elements from the complex numbers C
 has the standard basis 
 
11122122
10010000
,,
00001001
⎡⎤    ⎡⎤   ⎡⎤    ⎡⎤
====
⎢⎥    ⎢⎥   ⎢⎥    ⎢⎥
⎣⎦    ⎣⎦   ⎣⎦    ⎣⎦
eeee 
 
Any 
22x matrix of the form 
v
μλ
ξ
⎡⎤
=
⎢⎥
⎣⎦
v 
 
where    ,   ,  ,,
vμλξ∈C can then be represented by 
 
11122122
vμλξ=+++ve  e e  e 
 

52  Chap. 3            •      VECTOR SPACES 
The basis for a vector space is not unique and the general rule of change of basis is given by 
Theorem 9.5.  An important special case of that rule is made explicit in the following 
exchange 
theorem. 
 
Theorem 9.8
.  If 
{}
12
,    ,...,
N
eeeis a basis for Vand if 
{}
12
,,...,
K
=bbbB is a linearly independent 
set of 
()
KN  K≥
 in 
,V
then it is possible to exchange a certain Kof the original base vectors with 
12
,,...,
K
bbb so that the new set is a basis for     .V 
 
Proof. We select 
1
b from the set 
B
and order the basis vectors such that the component 
1
ξ of 
1
b is 
not zero in the representation 
 
1
1
N
j
j
j
ξ
=
=
∑
be
 
 
By Theorem 9.5, the vectors 
{}
12
,    ,...,
N
bee form a basis for 
.V
  A second vector 
2
b is selected 
from 
Vand we again order the basis vectors so that this time the component 
2
λ is not zero in the 
formula 
 
1
21
2
N
j
j
j
λλ
=
=+
∑
bbe 
 
Again, by Theorem 9.5 the vectors 
{}
123
,,    ,...,
N
bb eeform a basis forV.  The proof is completed by 
simply repeating the above construction 2
K− times. 
 
We now know that when a basis for 
V is given, every vector in V has a representation in 
the form (9.6).  Inverting this condition somewhat, we now want to consider a set of vectors 
{}
12
,,...,
M
=bbbB of Vwith the property that every vector ∈vV can be written as 
 
1
M
j
j
j
λ
=
=
∑
vb
 
 
Such a. set is called a 
generating set of V (or is said to span V).  In some sense a generating set is 
a counterpart of a linearly independent set.  The following theorem is the counter part of Theorem 
9.1. 
 

Sec. 9              •              Linear Independence, Dimension, Basis 53 
Theorem 9.9.  If 
{}
1
,...,
M
=bbB is a generating set of V, then every other finite set of vectors 
containing 
Bis also a generating set of V. 
 
In view of this theorem, we see that the counterpart of a maximal, linearly independent set 
is a minimal generating set, which is defined by the condition that a generating set 
{}
1
,...,
M
bb is 
minimal if it contains no proper subset that is still a generating set.  The following theorem shows 
the relation between a maximal, linearly independent set and a minimal generating set. 
 
Theorem 9.10.  Let 
{}
1
,...,
M
=bbB
 be a finite subset of a finite dimensional vector spaceV.  
Then the following conditions are equivalent: 
(
i) 
B
is a maximal linearly independent set. 
(
ii) Bis a linearly independent generating set. 
(
iii)Bis a minimal generating set. 
 
Proof. We shall show that 
()
()( )().iiiiiii⇒⇒ ⇒ 
(
i)⇒(ii).  This implication is a direct consequence of the representation (9.6). 
(
ii)⇒(iii).  This implication is obvious.  For if 
B
is a linearly independent generating set but 
not a minimal generating set, then we can remove at least one vector, say ,
M
b and the remaining 
set is still a generating set.  But this is impossible because ,
M
b can then be expressed as a linear 
combination of 
{}
11
,...,,
M−
bb contradicting the linear independence of    .B 
(
iii)⇒(i).  If 
B
 is a minimal generating set, then 
B
must be linearly independent because 
otherwise one of the vectors of 
B, say ,
M
b can be written as a linear combination of the vectors 
{}
11
,...,.
M−
bb  It follows then that 
{}
11
,...,
M−
bb is still a generating set, contradicting the 
assumption that 
{}
1
,...,
M
bb is minimal.  Now a linearly independent generating set must be 
maximal, for otherwise there exists a vector 
∈bVsuch that 
{}
1
,...,,
M
bbb is linearly independent.  
Then bcannot be expressed as a linear combination of 
{}
1
,...,,
M
bb thus contradicting the 
assumption that 
B
is a generating set. 
 
In view of this theorem, a basis 
Bcan be defined by any one of the three equivalent 
conditions (
i), (ii), or (iii). 
 
 
Exercises 
 

54  Chap. 3            •      VECTOR SPACES 
9.1 In elementary plane geometry it is shown that two straight lines determine a plane if the 
straight lines satisfy a certain condition.  What is the condition?  Express the condition in 
vector notation. 
9.2  Prove Theorems 9.1-9.3, and 9.9. 
9.3       Let       
33×
M denote the vector space of al1 
33×
 matrices with elements from the complex 
numbers 
.C
 Determine a basis for 
33×
M
 
9.4       Let       
22×
M denote the vector space of all 22× matrices with elements from the real numbers 
.
R Is either of the following sets a basis for
22×
M? 
 
10060101
,,
00023068
1 0010000
,, ,
00000100
⎧⎫
⎡⎤⎡⎤⎡⎤⎡⎤
⎨⎬
⎢⎥⎢⎥⎢⎥⎢⎥
⎣⎦⎣⎦⎣⎦⎣⎦
⎩⎭
⎧⎫
⎡ ⎤⎡⎤⎡⎤⎡ ⎤
⎨⎬
⎢ ⎥⎢⎥⎢⎥⎢ ⎥
⎣ ⎦⎣⎦⎣⎦⎣ ⎦
⎩⎭
 
 
9.5
 Are the complex numbers 24i+ and 62i+ linearly independent with respect to the field of 
real numbers
R?  Are they linearly independent with respect to the field of complex 
numbers? 

Sec. 10            •            Intersection, Sum, Direct Sum 55 
 
Section 10.  Intersection, Sum and Direct Sum of Subspaces 
 
In this section operations such as “summing” and “intersecting” vector spaces are 
discussed.  We introduce first the important concept of a subspace of a vector space in analogy with 
a subgroup of a group.  A non empty subset 
U of a vector space V is a subspace if: 
(a)    ,for all    ,.∈⇒+∈∈uwu  wuw
UU  U 
(b) 
for all  .λλ∈⇒ ∈∈uuUU   C 
 
Conditions (a) and (b) in this definition can be replaced by the equivalent condition: 
(a ' )    ,for all  .
λμλ∈⇒ + ∈∈uwuwUU  C 
 
Examples of subspaces of vector spaces are given in the following list: 
1.   The subset of the vector space 
N
C of all N-tuples of the form 
()
23
0,,    ,...,
N
λλλ is a subspace of 
N
C. 
2.   Any vector space 
Vis a subspace of itself. 
3.   The set consisting of the zero vector 
{}
0is a subspace of 
V
. 
4.   The set of real numbers 
R can be viewed as a subspace of the real space of complex   numbers 
.
C 
 
The vector spaces 
{}
0 and 
V
itself are considered to be trivial subspaces of the vector space 
V
.  If  
U is not a trivial subspace, it is said to be a proper subspace of V. 
 
Several properties of subspaces that one would naturally expect to be true are developed in 
the following theorems. 
 
Theorem 10.1.  If 
U
 is a subspace of ,V then ∈0U. 
 
Proof.  The proof of this theorem follows easily from (b) in the definition of a subspace above by 
setting 
0.λ= 
 
Theorem 10.2.  If 
U
 is a subspace of 
V
, then dim
U
dim≤
.V
 
 
Proof. By Theorem 9.8 we know that any basis of Ucan be enlarged to a basis of
;V
 it follows 
that 
dim
U
dim≤
.V
 
 

56  Chap. 3            •      VECTOR SPACES 
Theorem 10.3.  If 
U is a subspace of ,V then dimUdimV if and only if .UV 
 
Proof.  If ,UV then clearly dimdim    .UV  Conversely, if dimdim    ,UV then a basis for U 
is also a basis for 
.V  Thus, any vector vVis also in 
,U
 and this implies .UV 
 
Operations that combine vector spaces to form other vector spaces are simple extensions of 
elementary operations defined on sets.  If 
U
 and 
W
 are subspaces of a vector space,V the sum of 
Uand ,W written ,UW is the set 
 

,  uwuwUWUW 
 
Similarly, if 
U and  W are subspaces of a vector space ,V the intersection of U and ,Wdenoted 
by            ,
UW is the set 
 

and     uuuUWUW 
 
The 
union of two subspaces 
,andofUWV
 denoted by ,UW is the set 
 

or   uuuUWUW 
 
Some properties of these three operations are stated in the following theorems. 
 
Theorem 10.4.  If 
U and Ware subspaces of ,V  then UW is a subspace of     .V 
 
Theorem 10.5.  If 
U and W are subspaces of ,V then U and W are also subspaces of .UW 
 
Theorem 10.6.  If 
U
and 
W
are subspaces of ,V then the intersection 
UW
 is a subspace of ,V 
 
Proof.  Let ,uwUW.  Then ,uwWand  ,.uwU  Since Uand W are subspaces, 
uw
Wand uwU which means that uwUW also.  Similarly, if uUW,then for 
all 
R. uUW. 
 

Sec. 10            •            Intersection, Sum, Direct Sum 57 
Theorem l0.7.  If 
Uand Ware subspaces of ,V then the union ∪UW is not generally a subspace 
of 
.V 
 
Proof.  Let  ,,∈∉uuUWand let ∈wWand ;∉wU then +∉uwUand ,+∉uwW which means 
that                         .+∉∪uw
UW 
 
Theorem 10.8.  Let 
v be a vector in ,+UW where 
U
 and 
W
are subspaces of 
.V
  The 
decomposition of ∈v
U+W into the form 
,=+vuw
where ∈uUand ,∈wW is unique if and 
only if 
{}
∩=0UW. 
 
Proof. Suppose there are two ways of decomposingv; for example, let there be a uand 
′
u in 
U
 
and a 
wand 'w in W such that 
 
and''
=+=+vuwvuw 
 
The decomposition of 
vis unique if it can be shown that the vector ,b 
 
''=−=−buu ww 
 
vanishes.  The vector 
b is contained in ∩UW since u and 'uare known to be in 
,U
 and w and  
'w
 are known to be in W.  Therefore  
{}
∩=0UW implies uniqueness.  Conversely, if we have 
uniqueness, then 
{}
∩=0UW, for otherwise any nonzero vector ∈∩yUWhas at least two 
decompositions 
.=+=+yy00y 
 
The sum of two subspaces 
Uand Wis called the direct sum of U and Wand denoted by 
{}
if.⊕∩=0UWUW  This definition is motivated by the result proved in Theorem 10.8.  If 
,⊕=
UWV then U is called the direct complement of W in .V  The operation of direct 
summing can be extended to a finite number of subspaces 
12Q
,    ,...,.ofVVVV
  The direct sum  
12Q
⊕⊕⊕VVV" is required to satisfy the conditions 
 
{}
1
11
1, 2 ,,
Q
KR
RKRK
KKR
for  RQ
=−
==+
∩∩= =
∑∑
0VV+VV... 
 

58  Chap. 3            •      VECTOR SPACES 
The concept of a direct sum of subspaces is an important tool for the study of certain concepts in 
geometry, as we shall see in later chapters.  The following theorem shows that the dimension of a 
direct sum of subspaces is equal to the sum of the dimensions of the subspaces. 
 
Theorem 10.9.  If Uand Ware subspaces of 
,V then 
 
dimdimdim⊕=+UWUW 
 
Proof.  Let 
{}
12
,,...,
R
uuu
 be a basis for Uand 
{}
12
,,...,
Q
www be a basis for .W.  Then the set of 
vectors 
{}
121  2
,,, , , ,,
RQ
uuu www......
 is linearly independent since 
{}
∩=0UW.  This set of 
vectors generates 
⊕UW
 since for any ∈⊕vUW we can write 
 
11
Q
R
jj
jj
jj
λμ
==
=+ =+
∑∑
vuwuw 
 
where 
∈uU and .∈wW  Therefore by Theorem 9.10, dimRQ⊕=+UW. 
 
The result of this theorem can easily be generalized to 
 
()
12
1
dimdim
Q
Qi
j=
⊕⊕⋅⋅⋅⊕ =
∑
VVVV 
 
The designation “direct sum” is sometimes used in a slightly different context.  If Vand U 
are vector spaces, not necessarily subspaces of a common vector space, the set 
×VU
can be given 
the vector space structure by defining addition and scalar multiplication as in Exercise 8.5.  The set 
×VU with this vector space structure is also called the direct sum and is written 
.⊕VU
  This 
concept of direct sum is slightly more general since Vand U need not initially be subspaces of a 
third vector space.  However, after we have defined ,
⊕UV then 
U
 and 
V
 can be viewed as 
subspaces of ⊕UV; further, in that sense, 
⊕UVis the direct sum of 
U
 and 
V
 in accordance 
with the original definition of the direct sum of subspaces. 
 
 
Exercises 
 
10.1     Under what conditions can a single vector constitute a subspace? 
10.2     Prove Theorems 10.4 and 10.5. 
10.3     If     Uand W are subspaces of V, show that any subspace which contains ∪UW also 
contains            .+UW 

Sec. 10            •            Intersection, Sum, Direct Sum 59 
10.4     If     Vand Uare vector spaces and if 
⊕VU is their direct sum in the sense of the second 
definition given in this section, reprove Theorem 10.9. 
10-5     Let     
3
,V=R
 and suppose that U is the subspace spanned by 
()
{}
0,1,1    and W is the 
subspace spanned by 
()()
{}
1, 0, 1  ,   1, 1, 1   .   S h o w   t h a t   .=⊕VU W  Find another subspace 
Usuch that .=⊕VU  W 

60  Chap. 3            •      VECTOR SPACES 
 
Section 11.   Factor Space 
 
In Section 2 the concept of an equivalence relation on a set was introduced and in Exercise 
2.2 an equivalence relation was used to partition a set into equivalence sets.  In this section an 
equivalence relation is introduced and the vector space is partitioned into equivalence sets.  The 
class of all equivalence sets is itself a vector space called the factor space. 
 
If 
U is a subspace of a vector space 
,V
then two vectors w and v in V are said to be 
equivalent with respect to
U
, written vw∼, if −wv is a vector contained in
U
.  It is easy to see 
that this relation is an equivalence relation and it induces a partition of 
V into equivalence sets of 
vectors.  If 
,∈vV then the equivalence set of    ,vdenoted by v,
1
 is the set of all vectors of the form 
,+vu where 
u is any vector of 
,U
 
 
{}
=+ ∈vvuuU 
 
To illustrate the equivalence relation and its decomposition of a vector space into 
equivalence sets, we will consider the real vector space 
2
,R
 which we can represent by the 
Euclidean plane.  Let 
u be a fixed vector in 
2
,Rand define the subspace U of 
2
R by 
{}
uλλ∈R.  
This subspace consists of all vectors of the form 
λu, λ∈R, which are all parallel to the same 
straight line.  This subspace is illustrated in Figure 2.  From the definition of equivalence, the 
vector 
v is seen to be equivalent to the vector w if the vector −vw is parallel to the line 
representing 
.U
 Therefore all vectors that differ from the vector v by a vector that is parallel to the 
line representing U are equivalent.  The set of all vectors equivalent to the vector
v is therefore the 
set of all vectors that terminate on the dashed line parallel to the line representing Uin Figure 2.  
The equivalence set of 
,,vvis the set of all such vectors.  The following theorem is a special case 
of Exercise 2.2 and shows that an equivalence relation decomposes V into disjoint sets, that is to 
say, each vector is contained in one and only one equivalence set. 
 
                                                
 
1
 Only in this section do we use a bar over a letter to denote an equivalence set.  In other sections, unless otherwise 
specified, a bar denotes the complex conjugate. 

Sec. 11            •            Factor Space                                                                     61 
 
Theorem 11.l.  If 
,≠vw then ∩=∅vw. 
 
Proof. Assume that ≠vwbut that there exists a vector x in ∩vw.  Then ∈xv, xv∼, and 
∈xw, xw∼.  By the transitive  property of the equivalence relation we have vw∼, which 
implies 
=vw
 and which is a contradiction.  Therefore 
∩vw
 contains no vector unless ,=vw in 
which case 
∩=vwv. 
We shall now develop the structure of the factor space.  The 
factor class of 
U
, denoted by 
/VU is the class of all equivalence sets in V formed by using a subspace U of V.  The factor 
class is sometimes called a 
quotient class.  Addition and scalar multiplication of equivalence sets 
are denoted by 
 
+=+vw vw 
 
and 
 
λλ=vv
 
 
respectively.  It is easy to verify that the addition and multiplication operations defined above 
depend only on the equivalence sets and not on any particular vectors used in representing the sets.  
The following theorem is easy to prove. 
 
v
w
Figure 2 
U 

62  Chap. 3            •      VECTOR SPACES 
Theorem 11.2.  The factor set 
/VU forms a vector space, called a factor space, with respect to 
the operations of addition and scalar multiplication of equivalence classes defined above. 
 
The factor space is also called a 
quotient space.  The subspace 
U
 in /VU plays the role of 
the zero vector of the factor space.  In the trivial case when 
=UV, there is only one equivalence 
set and it plays the role of the zero vector.  On the other extreme when 
{}
=0U,then each 
equivalence set is a single vector and 
{}
/=0VV
. 
 
 
Exercises 
 
11.1     Show that the relation between two vectors 
v and ∈wV that makes them equivalent is, in 
 fact, an equivalence relation in the sense of Section 2. 
11.2 Give a geometrical interpretation of the process of addition and scalar multiplication of 
      equivalence      sets      in      
2
R
in Figure 2. 
11.3     Show     that     
V is equal to the union of all the equivalence sets in V.  Thus /VU is a class 
of nonempty, disjoint sets whose union is the entire spaceV. 
11.4     Prove Theorem 11.2. 
11.5     Show     that     
()
dimdim- dim=VUVU. 

Sec. 12            •            Inner Product Spaces                                                                     63 
 
Section 12.   Inner Product Spaces 
 
There is no concept of length or magnitude in the definition of a vector space we have been 
employing.  The reason for the delay in the introduction of this concept is that it is not needed in 
many of the results of interest.  To emphasize this lack of dependence on the concept of magnitude, 
we have delayed its introduction to this point.  Our intended applications of the theory, however, do 
employ it extensively. 
 
We define the concept of length through the concept of an 
inner product.  An inner product 
on a complex vector space V is a function 
:f×→VV    C  with the following properties: 
(1)  
()()
,,ff=uvvu; 
(2)  
() ( )
,,ffλλ=uvuv; 
(3)  
()()()
,, ,fff+=  +uwvuvwv
; 
(4)  
()()
,0  and ,0ff≥=⇔=uuuuu   0; 
 
for all 
,,∈uvwV and .λ∈C  In Property 1 the bar denotes the complex conjugate.  Properties 2 
and 3 require that 
f be linear in its first argument; i.e., 
()()()
,,,fffλμ   λμ+=  +uvwuwvw
 
for all 
,,∈uvwV
 and all    ,λμ∈C.  Property 1 and the linearity implied by Properties 2 and 3 
insure that 
f is conjugate linear in its second argument;  i.e.,  
()()()
,,,fffλμ λμ+=    +uv   wuvuw for all ,,∈uvwV and all    ,λμ∈C.  Since Property 1 
ensures that 
()
,fuu is real, Property 4 is meaningful, and it requires that 
f
 be positive definite.  
There are many notations for the inner product.  We shall employ the notation of the “dot product” 
and write 
 
()
,f=⋅uvu v
 
 
An 
inner product space is simply a vector space with an inner product.  To emphasize the 
importance of this idea and to focus simultaneously all its details, we restate the definition as 
follows. 
 
Definition.  A 
complex inner product space, or simply an inner product space, is a set 
V
 and a 
field 
C such that: 
(a) There exists a binary operation in Vcalled addition and denoted by 
+ such that: 
(1) 
()()
++=++uv  w u  vwfor all ,,∈uvwV. 
(2) +=+uv vu for all 
,∈uvV. 

64  Chap. 3            •      VECTOR SPACES 
(3) There exists an element 
0V such that u0 u for all uV. 
(4)       For       every       
uV
 there exists an element 
uV
 such that 

 uu0. 
(b) There exists an operation called 
scalar multiplication in which every scalar C 
can be combined with every element 
uV
 to give an element uV such that: 
(l) 

uu
; 
(2) 

uuu; 
(3) 

 uvu  v; 
(4) 
1uu; 
 for all    ,   ,
C and all    ,uvV; 
(c) There exists an operation called 
inner product by which any ordered pair of vectors 
u and v in V determines an element of C denoted by uv such that 
(1) 
uv  vu; 
(2) 

   uvu  v; 
(3) 

uwv uvwv; 
(4) 
uu  0 and 0uu  0u; 
for all 
,,uvwV and C. 
 
A 
real inner product space is defined similarly.  The vector space 
N
C becomes an inner product 
space if, for any two vectors 
,.
N
uvCwhere 

12
,,...,
N
u and 

12
,,...,,
N
  v we define 
the inner product of 
u and v by 
1
N
jj
j




uv 
 
The 
length of a vector is an operation, denoted by ,that assigns to each nonzero vector 
vV
 a 
positive real number by the following rule: 
 
 
vvv                                                             (12.1)                                                             
 
Of course the length of the zero vector is zero.  The definition represented by (12.1) is for an inner 
product space of 
N dimensions in general and therefore generalizes the concept of “length” or 
“magnitude” from elementary Euclidean plane geometry to 
N-dimensional spaces.  Before 
continuing this process of algebraically generalizing geometric notions, it is necessary to pause and 
prove two inequalities that will aid in the generalization  process. 
 
Theorem 12.1.  The 
Schwarz inequality, 
 

Sec. 12            •            Inner Product Spaces                                                                     65 
 
⋅≤uv    u v                                                           (12.2)                                                           
 
is valid for any two vectors    ,uv in an inner product space. 
 
Proof.    The Schwarz inequality is easily seen to be trivially true when either u or v is the 
0vector, so we shall assume that neither u nor v is zero.  Construct the vector 
()()
⋅−⋅uuv   vuuand employ Property (c4), which requires that every vector have a nonnegative 
length, hence 
 
()()
()
222
0−⋅⋅  ≥uv    uvuvu
 
 
Since 
u must not be zero, it follows that 
 
()
()
222
≥⋅  ⋅=⋅u  vuv uvuv 
 
and the positive square root of this equation is Schwarz’s inequality. 
 
Theorem 12.2.  The triangle inequality 
 
 
+≤ +uv   u  v
                                                        (12.3)                                                        
 
is valid for any two vectors    ,uv in an inner product space. 
 
Proof:  The squared length of +uv can be written in the form 
 
()()
()
222
22
2Re
+=+⋅+= + +⋅+⋅
=++  ⋅
uvuv uv   uv   uvvu
uvuv
 
 
where Re  signifies the real part.  By use of the Schwarz inequality this can be rewritten as 
 
()
2
222
2+≤ + +   = +uvuvuvu  v 
 
Taking the positive square root of this equation, we obtain the triangular inequality. 
For a 
real inner product space the concept of angle is defined as follows.  The angle between two 
vectors 
u and    ,v denoted by    ,θ is defined by 

66  Chap. 3            •      VECTOR SPACES 
 
 
cosθ
⋅
=
uv
uv
                                                           (12.4)                                                           
 
This definition of a real-valued angle is meaningful because the Schwarz inequality in this case 
shows that the quantity on the right-hand side of (12.4) must have a value lying between 1  and    1,
− 
i.e., 
11
⋅
−≤≤+
uv
uv
 
 
Returning to complex inner product spaces in general, we say that two vectors 
u and v are 
orthogonal if ⋅uv or ⋅vu is zero.  Clearly, this definition is consistent with the real case, since 
orthogonality then means 
/2θπ=
. 
 
The inner product space is a very substantial algebraic structure and parts of the structure 
can be given slightly different interpretations.  In particular it can be shown that the length 
vof a 
vector ∈vV is a particular example of a mathematical concept known as a norm, and thus an 
inner product space is a 
normal space.  A norm on 
V
 is a real-valued function defined on 
V
 
whose value is denoted by 
v
 and which satisfies the following axioms: 
(1) 
0≥vand 0=⇔=vv0; 
(2) 
λλ=vv; 
(3) 
+≥+uv uv
; 
 
for all 
,∈uvV and all λ∈C.  In defining the norm of    ,v we have employed the same notation as 
that for the length of 
v because we will show that the length defined by an inner product is a norm, 
but the converse need not be true. 
 
Theorem 12.3.  The operation of determining the length 
v of 
∈vV
 is a norm on 
.V
 
 
Proof.  The proof follows easily from the definition of length.  Properties (c1), (c2), and (c4) of an 
inner product imply Axioms l and 2 of a norm, and the triangle inequality is proved by Theorem 
12.2. 
 
We will now show that the inner product space is also a 
metric space.  A metric space is a 
nonempty set M equipped with a positive real-valued function 
×→MM   R,called the distance 
function that satisfies the following axioms: 

Sec. 12            •            Inner Product Spaces                                                                     67 
(1) 
()
,0d≥uvand 
()
,0d=⇔=uvu   v; 
(2) 
()()
,,dd=uvvu
; 
(3) 
()()()
,,,ddd≤+uwuvvw; 
 
for all 
,,.∈uvwM  In other words, a metric space is a set M of objects and a positive-definite, 
symmetric distance function 
dsatisfying the triangle inequality.  The boldface notation for the 
elements of M is simply to save the introduction of yet another notation. 
 
Theorem 12.4.  An inner product space 
V
 is a metric space with the distance function given by 
 
 
()
,d=−uvu  v
                                                         (12.5)                                                         
 
Proof. Let =−wuv;  then from the requirement that 0≥wand 0=⇔=ww0 it follows that 
 
0−≥uv and 0−=⇔=uvu v 
 
Similarly, from the requirement that 
,=−ww
 it follows that 
 
−=−uv   vu 
 
Finally, let 
u be replaced by −uv and vby −vw in the triangle inequality (12.3); then 
 
−≤−+−uw   uv  vw 
 
which is the third and last requirement for a distance function in a metric space. 
 
The inner product as well as the expressions (12.1) for the length of a vector can be 
expressed in terms of any basis and the components of the vectors relative to that basis.  Let 
{}
12
,    ,...,
N
eeebe a basis of Vand denote the inner product of any two base vectors by 
jk
e
, 
 
 
jkjkkj
ee≡⋅=ee                                                         (12.6)                                                         
 
Thus, if the vectors 
uand v have the representations 
 

68  Chap. 3            •      VECTOR SPACES 
 
11
,
NN
jk
jk
jk
λμ
==
==
∑∑
ueve                                                (12.7)                                                
 
relative to the basis 
{}
12
,    ,...,
N
eee
 then the inner product of u and v is given by 
 
 
11    11
NN    NN
jkjk
jkjk
jkjk
eλμ   λμ
==  ==
⋅=⋅=
∑∑ ∑∑
uvee                                      (12.8)                                      
 
Equation (12.8) is the component expression for the inner product. 
 
From the definition (12.1) for the length of a vector    ,v and from (12.6) and (12.7)
2
, we can 
write 
 
 
1/ 2
11
NN
jk
jk
jk
eμμ
==
⎛⎞
=
⎜⎟
⎝⎠
∑∑
v
                                                   (12.9)                                                   
 
This equation gives the component expression for the length of a vector.  For a real inner product 
space, it easily follows that 
 
 
11
1/ 2
1/ 2
111 1
cos
NN
jk
jk
jk
NNNN
prsl
prsl
prl s
e
ee
μλ
θ
μμλλ
==
=== =
=
⎛⎞
⎛⎞
⎜⎟
⎜⎟
⎝⎠
⎝⎠
∑∑
∑∑∑∑
                                (12.10)                                
 
In formulas (12.8)-(12.10) notice that we have changed the particular indices that indicate 
summation so that no more than two indices occur in any summand.  The reason for changing these 
dummy indices of summation can be made apparent by failing to change them.  For example, in 
order to write (12.9) in its present form, we can write the component expressions for 
v in two 
equivalent ways, 
 
11
,
NN
jk
jk
jk
μμ
==
==
∑∑
veve 
 
and then take the dot product.  If we had used the same indices to represent summation in both 
cases, then that index would occur four times in (12.9) and all the cross terms in the inner product 
would be left out. 

Sec. 12            •            Inner Product Spaces                                                                     69 
 
 
Exercises 
 
12.1     Derive the formula 
 
()()
22  2  2
211iiii⋅= + + +  − +   − +uv   u  vu   vuv 
 
which expresses the inner product of two vectors in terms of the norm.  This formula is 
known as the 
polar identity. 
12.2
 Show that the norm  induced by an inner product according to the definition (12.1) must 
satisfy the following 
parallelogram law; 
 
2222
22++−=  +uvuvvu 
 
for all vectors 
uand .v  Prove by counterexample that a  norm in general need not be an 
induced norm of any inner product. 
12.3     Use the definition of angle given in this section and the properties of the inner product to 
derive the law of cosines. 
12.4     If     Vand Uare inner produce spaces, show that the equation 
 
()()
()
,,,f=⋅+ ⋅vw   ubv u  w b 
 
where 
,∈vuV and ,,∈wbUdefines an inner product on .⊕VU 
12.5     Show that the only vector in Vthat is orthogonal to every other vector in Vis the zero 
vector. 
12.6 Show that the Schwarz and triangle inequalities become equalities if and only if the vectors 
concerned are linearly dependent. 
12.7     Show     that     
1212
−≥ −uu   u  u for all 
12
,uu in an inner product space     .V 
12.8     Prove by direct calculation that 
11
NN
jk
jk
jk
eμμ
==
∑∑
 in (12.9) is real. 
12.9     If     U is a subspace of an inner product space 
,V prove that U is also an inner product 
space. 
12.10   Prove that the
NN× matrix 
jk
e
⎡⎤
⎣⎦
 defined by (12.6) is nonsingular. 

70  Chap. 3            •      VECTOR SPACES 
 
Section 13.   Orthonormal Bases and Orthogonal Complements 
 
Experience with analytic geometry tells us that it is generally much easier to use bases 
consisting of vectors which are orthogonal and of unit length rather than arbitrarily selected 
vectors.  Vectors with a magnitude of 1 are called 
unit vectors or normalized vectors.  A set of 
vectors in an inner product space 
V is said to be an orthogonal set if all the vectors in the set are 
mutually orthogonal, and it is said to be an 
orthonormal set if the set is orthogonal and if all the 
vectors are unit vectors.  In equation form, an orthonormal set 
{}
1
,...,
M
iisatisfies the conditions 
 
 
1if
0if
jkjk
jk
jk
δ
=
⎧
⋅= ≡
⎨
≠
⎩
ii                                              (13.1)                                              
 
The symbol 
jk
δ introduced in the equation above is called the Kronecker delta. 
 
Theorem 13.1.  An orthonormal set is linearly independent. 
 
Proof.  Assume that the orthonormal set 
{}
12
,   ,...,
M
iii is linearly dependent, that is to say there 
exists a set of scalars 
{}
12
,,,   ,
M
λλ   λ...
 not all zero, such that 
 
1
M
j
j
j
λ
=
=
∑
i0 
 
The inner product of this sum with the unit vector 
k
i gives the expression 
 
1
1
1
0
M
jMk
jkkMk
j
λδ   λδλ δ   λ
=
=++   = =
∑
"
 
 
for       1, 2,...,.
kM=  A contradiction has therefore been achieved and the theorem is proved. 
 
As a corollary to this theorem, it is easy to see that an orthonormal set in an inner product 
space Vcan have no more than 
dimN=V elements.  An orthonormal set is said to be complete in 
an inner product space V if it is not a proper subset of another orthonormal set in the same space.  
Therefore every orthonormal set with 
dimN=V elements is complete and hence maximal in the 
sense of a linearly independent set.  It is possible to prove the converse:  Every complete 
orthonormal set in an inner product space Vhas 
dimN=V elements.  The same result is put in a 
slightly different fashion in the following theorem. 

Sec. 13            •            Orthonormal Bases, Orthogonal Complements 71 
 
Theorem 13.2.  A complete orthonormal set is a basis for
V; such a basis is called an orthonormal 
basis. 
 
Any set of linearly independent vectors can be used to construct an orthonormal set, and 
likewise, any basis can be used to construct an orthonormal basis.  The process by which this is 
done is called the Gram-Schmidt orthogonalization process and it is developed in the proof of the 
following theorem. 
 
Theorem 13.3.  Given a basis 
{}
12
,, ,
N
eee...of an inner product space
V
, then there exists an 
orthonormal basis 
{}
1
,,
N
ii... such that 
{}
1
,
k
ee... and 
{}
1
,,
k
ii... generate the same subspace 
k
Uof 
V, for each 1,,    .kN=... 
 
Proof.  The construction proceeds in two steps; first a set of orthogonal vectors is constructed, then 
this set is normalized.  Let 
{}
12
,,...,
N
ddddenote a set of orthogonal, but not unit, vectors.  This set 
is constructed from 
{}
12
,    ,...,
N
eee as follows:  Let 
11
=de and put 
 
22   1
ξ=+de  d 
 
The scalar ξwill be selected so that 
2
d is orthogonal to 
1
;d orthogonality of 
1
d and 
2
drequires 
that their inner product be zero; hence 
 
212111
0ξ⋅== ⋅ +  ⋅dded   dd 
 
implies 
 
21
11
ξ
⋅
=−
⋅
ed
dd
 
 
where 
11
0⋅≠ddsince 
1
0.≠d  The vector 
2
d is not zero, because 
1
e and 
2
eare linearly 
independent.  The vector 
3
d is defined by 
 
21
3321
ξξ=+  +de   d   d 
 
The scalars 
2
ξ and 
1
ξ are determined by the requirement that 
3
d be orthogonal to both 
1
dand 
2
d; 
thus 

72  Chap. 3            •      VECTOR SPACES 
 
1
31   3111
0ξ⋅=⋅+  ⋅=dd  eddd 
 
2
32   3222
0ξ⋅=⋅+  ⋅=dd  dddd
 
 
and, as a result, 
 
1
11
11
,ξ
⋅
=−
⋅
ed
dd
 
2
32
22
ξ
⋅
=−
⋅
ed
dd
 
 
The linear independence of 
12
,,eeand 
3
e requires that 
3
d be nonzero.  It is easy to see that this 
scheme can be repeated until a set of 
N orthogonal vectors 
{}
12
,,,
N
ddd... has been obtained.  The 
orthonormal set is then obtained by defining 
 
/, 1,2,,
kkk
kN==idd...
 
 
It is easy to see that 
{}{}
11
,, ,, ,
kk
ee d d......and, hence, 
{}
1
,,
k
ii... generate the same subspace for 
each 
k. 
 
The concept of mutually orthogonal vectors can be generalized to mutually orthogonal 
subspaces.  In particular, if U is a subspace of an inner product space, then the 
orthogonal 
complement
 of U is a subset of V, denoted by ,
⊥
U such that 
 
 
{}
0 for all  
⊥
=⋅=∈vv uuUU                                            (13.2)                                            
 
The properties of orthogonal complements are developed in the following theorem. 
 
Theorem 13.4.  If 
U
 is a subspace of 
V
, then (a) 
⊥
U
 is a subspace of 
V
 and (b) .
⊥
=⊕VU U 
 
Proof. 
(a) If 
1
u and 
2
u are in ,
⊥
U then 
 
12
0⋅=⋅=uv u v 
 
for all 
∈vU
.  Therefore for any 
12
,,λλ∈C 

Sec. 13            •            Orthonormal Bases, Orthogonal Complements 73 
 
()
112 2112 2
0λλ   λ  λ+⋅=⋅+⋅=u uvuvuv 
 
Thus 
112 2
.λλ
⊥
+∈uuU 
(b)  Consider the vector space .
⊥
+UU  Let .
⊥
∈∩vUU  Then by (13.2), 0,⋅=vv which 
implies 
.=v0  Thus .
⊥⊥
+=⊕UUU   U  To establish that ,
⊥
=⊕VU U let 
{}
1
,,
R
ii...be an 
orthonormal basis for 
.U Then consider the decomposition 
 
()   ()
11
RR
qqqq
qq==
⎧⎫
=−  ⋅  +  ⋅
⎨⎬
⎩⎭
∑∑
vv    viivii
 
 
The term in the brackets is orthogonal to each 
q
iand it thus belongs to .
⊥
U  Also, the second term 
is in 
U  Therefore ,
⊥⊥
=+ =⊕VUU  U U and, the proof is complete. 
 
As a result of the last theorem, any vector 
∈vV
 can be written uniquely in the form 
 
 
=+vuw                                                              (13.3)                                                              
 
where ∈uU and .
⊥
∈wU  If vis a vector with the decomposition indicated in (13.3), then 
 
222222
=+=+ +⋅+⋅=+vuwuwuwwuuw
 
 
It follows from this equation that 
 
222 2
,≥≥vu    vw 
 
The equation is the well-known 
Pythagorean theorem, and the inequalities are special cases of an 
inequality known as 
Bessel’s inequality. 
 
 
Exercises 
 
13.1
 Show that with respect to an orthonormal basis the  formulas (12.8)-(12.10) can be written 
as 
 
 
1
N
jj
j
λμ
=
⋅=
∑
uv
                                                          (13.4)                                                          

74  Chap. 3            •      VECTOR SPACES 
 
 
2
1
N
jj
j
μμ
=
=
∑
v                                                          (13.5)                                                          
 
and, for a real vector space, 
 
 
(
)
(
)
1
1/ 21/ 2
11
cos
N
jj
j
NN
ppll
pl
λμ
θ
μμμμ
=
==
=
∑
∑∑
                                        (13.6)                                        
 
13.2     Prove Theorem 13.2. 
13.3     If     U is a subspace of the inner product space V, show that 
 
()
⊥
⊥
=UU 
 
13.4     If     
V
 is an inner product space, show that 
 
{}
⊥
=0V 
 
Compare this with Exercise 12.5. 
13.5     Given the basis 
 
()()()
12 3
1,1,1  ,0,1,1  ,0, 0,1===ee e 
 
for 
,
3
R construct an orthonormal basis according to the Gram-Schmidt orthogonalization 
process. 
13.6     If     U is a subspace of an inner product space V, show that 
 
dimdim- dim
⊥
=UVU 
 
13.7     Given two subspaces 
1
V and 
2
V of V, show that 
 
()
1122
⊥
⊥⊥
+=∩VVV   V
 
 
and 
 
()
1122
⊥
⊥⊥
∩=+VVV V 

Sec. 13            •            Orthonormal Bases, Orthogonal Complements 75 
 
13.8     In the proof of Theorem 13.4 we used the fact that if a vector is orthogonal to each vector 
q
i
 of a basis 
{}
1
,,
R
ii...of U, then that vector belongs to .
⊥
U  Give a proof of this fact. 

76                                                              Chap.                                                              3                                                              • VECTOR SPACES 
 
Section 14.   Reciprocal Basis and Change of Basis 
 
In this section we define a special basis, called the 
reciprocal basis, associated with each 
basis of the inner product space 
V.  Components of vectors relative to both the basis and the 
reciprocal basis are defined and formulas for the change of basis are developed. 
 
The set of 
N vectors 
{}
12
,    ,...,
N
eee is said to be the reciprocal basis relative to the basis 
{}
12
,    ,...,
N
eee of an inner product space 
V
 if 
 
                                                             ,,1,2,,
kk
ss
ksNδ⋅==ee...                                             (14.1)                                             
 
where the symbol 
k
s
δ
 is the Kronecker delta defined by 
 
 
1,
0,
k
s
ks
ks
δ
=
⎧
=
⎨
≠
⎩
                                                         (14.2)                                                         
 
Thus each vector of the reciprocal basis is orthogonal to 
1N− vectors of the basis and when its 
inner product is taken with the 
Nth vector of the basis, the inner product has the value one.  The 
following two theorems show that the reciprocal basis just defined exists uniquely and is actually a 
basis for 
V
. 
 
Theorem 14.1.  The reciprocal basis relative to a given basis exists and is unique. 
 
Proof.  Existence.  We prove only existence of the vector 
1
;e
 existence of the remaining vectors 
can be proved similarly.  Let 
U be the subspace generated by the vectors 
2
,...,,
N
ee and suppose 
that 
⊥
U
 is the orthogonal complement of 
U
.  Then dim1N=−U, and from Theorem 10.9, 
dim1.
⊥
=U  Hence we can choose a nonzero vector .
⊥
∈wU  Since 
1
∉eU, 
1
e and ware not 
orthogonal, 
 
1
0⋅≠ew 
 
we can simply define 
 
1
1
1
≡
⋅
ew
ew
 

Sec. 14. • Reciprocal Basis and Change of Basis 77 
 
Then 
1
e obeys (14.1) for 1k=. 
 
Uniqueness.  Assume that there are two reciprocal bases, 
{}
12
,    ,...,
N
eeeand 
{}
12
,,...,
N
ddd relative 
to the basis  
{}
12
,    ,...,
N
eee
of V.  Then from (14.1) 
 
andfor   ,1, 2,,
kkk  k
ssss
ksNδδ⋅=⋅==eed e...
 
 
Subtracting these two equations, we obtain 
 
 
()
0,,1, 2,,
kk
s
ksN−⋅==ed  e...                                         (14.3)                                         
 
Thus the vector 
kk
−edmust be orthogonal to 
{}
12
,    ,...,
N
eee Since the basis generates V, (14.3) is 
equivalent to 
 
 
()
0forall
kk
−⋅=∈ed vvV                                            (14.4)                                            
 
In particular, we can choose 
v in (14.4) to be equal to 
kk
−ed, and it follows then from the 
definition of an inner product space that 
 
                                                         ,1,2,,
kk
kN==ed...                                                 (14.5)                                                 
 
Therefore the reciprocal basis relative to 
{}
12
,    ,...,
N
eeeis unique. 
 
The logic in passing from (14.4) to (14.5) has appeared before; cf. Exercise 13.8. 
 
Theorem 14.2.  The reciprocal basis 
{}
12
,    ,...,
N
eee with respect to the basis 
{}
12
,    ,...,
N
eeeof the 
inner product space V is itself a basis forV. 
 
Proof. Consider the linear relation 
 
1
N
q
q
q
λ
=
=
∑
e0
 

78                                                              Chap.                                                              3                                                              • VECTOR SPACES 
 
If we compute the inner product of this equation with
k
e, 1, 2,...,kN=, then 
 
11
0
NN
qq
qkqkk
qq
λλδλ
==
⋅===
∑∑
ee 
 
Thus the reciprocal basis is linearly independent.  But since the reciprocal basis contains the same 
number of vectors as that of a basis, it is itself a basis for 
V. 
 
Since 
k
e,       1, 2,...,kN=, is in V, we can always write 
 
 
1
N
kkq
q
q
e
=
=
∑
ee                                                            (14.6)                                                            
 
where, by (14.1) and (12.6), 
 
 
1
N
kkkq
ssqs
q
eeδ
=
⋅= =
∑
ee
                                                     (14.7)                                                     
 
and 
 
 
1
N
kjkqj  kj   jk
q
q
eeeδ
=
⋅== =
∑
ee                                                (14.8)                                                
 
From a matrix viewpoint, (14.7) shows that the 
2
N quantities 
kq
e 
()
,1,2,,kqN=... are the 
elements of the inverse of the matrix whose elements are 
qs
e.  In particular, the matrices 
kq
e
⎡⎤
⎣⎦
 and 
qs
e
⎡⎤
⎣⎦
 are nonsingular.  This remark is a proof of Theorem 14.2 also by using Theorem 9.5.  It is 
possible to establish from (14.6) and (14.7) that 
 
 
1
N
k
ssk
k
e
=
=
∑
ee                                                            (14.9)                                                            
 
To illustrate the construction of a reciprocal basis by an algebraic method, consider the real 
vector space 
2
,R
 which we shall represent by the Euclidean plane.  Let a basis be given for 
2
R 
which consists of two vectors 45° degrees apart, the first one, 
1
,e two units long and the second 
one, 
2
,eone unit long.  These two vectors are illustrated in Figure 3. 
 

Sec. 14. • Reciprocal Basis and Change of Basis 79 
To construct the reciprocal basis, we first note that from the given information and equation 
(12.6) we can write 
 
 
11122122
4,2 ,1eee    e===  =                                         (14.10)                                         
 
Writing equations (14.7)
2
 out explicitly for the case 2N=, we have 
 
 
11122122
11211121
11122122
12221222
1,0
0,1
ee    eeee    e e
ee    eeee    e e
+=+=
+=+=
                                    (14.11)                                    
 
Substituting (14.10) into (14.11), we find that 
 
 
11122122
1
,12,2
2
eeee===−  =                                  (14.12)                                  
 
 
 
 
When the results (14.2) are put into the special case of (14.6) for 2,N
= we obtain the explicit 
expressions for the reciprocal basis 
{}
12
,,ee 
 
 
12
1212
111
,2
2
22
=−=− +ee   eeee
                                      (14.13)                                      
 
The reciprocal basis is illustrated in Figure 3 also. 
 
1
e 
2
e
Figure 3.  A basis and reciprocal basis for 
2
R
 
2
e 
1
e

80                                                              Chap.                                                              3                                                              • VECTOR SPACES 
Henceforth we shall use the same kernel letter to denote the components of a vector relative 
to a basis as we use for the vector itself.  Thus, a vector 
v has components 
k
v,       1, 2,...,kN= 
relative to the basis 
{}
12
,    ,...,
N
eee
and components 
k
v,       1, 2,...,kN=, relative to its reciprocal basis, 
 
 
11
,
NN
kk
kk
kk
vv
==
==
∑∑
veve                                              (14.14)                                              
 
The components 
12
,    ,...,
N
vvv
 with respect to the basis 
{}
12
,    ,...,
N
eee
 are often called the 
contravariant components of    ,
v while the components 
12
,    ,...,
N
vvv with respect to the reciprocal 
basis 
{}
12
,    ,...,
N
eeeare called covariant components.  The names covariant and contravariant are 
somewhat arbitrary since the basis and reciprocal basis are both bases and we have no particular 
procedure to choose one over the other.  The following theorem illustrates further the same remark. 
 
Theorem 14. 3.  If 
{}
1
,...,
N
eeis the reciprocal basis of 
{}
1
,...,
N
ee then 
{}
1
,...,
N
ee is also the 
reciprocal basis of 
{}
1
,...,
N
ee. 
 
For this reason we simply say that the bases 
{}
1
,...,
N
ee and 
{}
1
,...,
N
eeare (mutually) 
reciprocal.  The contravariant and covariant components of 
vare related to one another by the 
formulas 
 
 
1
1
N
kk qk
q
q
N
q
kk qk
q
vev
vev
=
=
=⋅ =
=⋅ =
∑
∑
ve
ve
                                                    (14.15)                                                    
 
where equations (12.6) and (14.8) have been employed.  More generally, if 
u has contravariant 
components 
i
u and covariant components 
i
urelative to 
{}
1
,...,
N
eeand 
{}
1
,...,
N
ee, respectively, 
then the inner product of 
uand vcan be computed by the formulas 
 
 
111111
NNNNNN
iiijij
ii    ijij
iiijij
uvuve uve uv
======
⋅==    ==
∑   ∑   ∑∑∑∑
uv                           (14.16)                           
 
which generalize the formulas (13.4) and (12.8). 
 

Sec. 14. • Reciprocal Basis and Change of Basis 81 
As an example of the covariant and contravariant components of a vector, consider a vector 
v in 
2
R which has the representation 
 
12
3
2
2
=+vee
 
 
relative to the basis 
{}
12
,ee
 illustrated in Figure 3.  The contravariant components of v are then 
()
3/2,2;  to compute the covariant components, the formula (14.16)
2
 is written out for the case 
2,
N= 
 
1212
1112121222
,vevev  v evev=+=+ 
 
Then from (14.10) and the values of the contravariant components the covariant components are 
given by 
 
()
12
622,3 2  2vv=+=+ 
hence 
 
()()
12
6223 2 2=+    +   +ve  e 
 
The contravariant components of the vector
v
 are illustrated in Figure 4.  To develop a geometric 
feeling for covariant and contravariant components, it is helpful for one to perform a vector 
decomposition of this type for oneself. 
 
 
 
A substantial part of the usefulness of orthonormal bases is that each orthonormal basis is 
self-reciprocal; hence contravariant and covariant components of vectors coincide.  The self-
1
e 
2
e
Figure 4.  The covariant and contravariant components of a vector in 
2
R. 
2
e
 
1
e

82                                                              Chap.                                                              3                                                              • VECTOR SPACES 
reciprocity of orthonormal bases follows by comparing the condition (13.1) for an orthonormal 
basis with the condition (14.1) for a reciprocal basis.  In orthonormal systems indices are written 
only as subscripts because there is no need to distinguish between covariant and contravariant 
components. 
 
Formulas for transferring from one basis to another basis in an inner product space can be 
developed for both base vectors and components.  In these formulas one basis is the set of linearly 
independent vectors 
{}
1
,...,
N
ee, while the second basis is the set 
{}
1
ˆˆ
,...,
N
ee.  From the fact that 
both bases generate 
V, we can write 
 
 
1
ˆ
,1, 2,...,
N
s
kks
s
TkN
=
==
∑
ee                                             (14.17)                                             
 
and 
 
 
1
ˆ
ˆ
,1, 2,...,
N
k
qqk
k
Tq  N
=
==
∑
ee                                            (14.18)                                            
 
where 
ˆ
k
q
T and 
s
k
Tare both sets of 
2
N scalars that are related to one another.  From Theorem 9.5 the 
NN×
 matrices 
s
k
T
⎡⎤
⎣⎦
 and 
ˆ
k
q
T
⎡⎤
⎣⎦
 must be nonsingular. 
 
Substituting (14.17) into (14.18) and replacing 
q
e by 
1
N
s
qs
s
δ
=
∑
e, we obtain 
 
111
ˆ
NNN
kss
qqksqs
kss
TTδ
===
==
∑∑∑
eee 
 
which can be rewritten as 
 
 
11
ˆ
NN
kss
qkq  s
sk
TTδ
==
⎛⎞
−=
⎜⎟
⎝⎠
∑∑
e0                                                 (14.19)                                                 
 
The linear independence of the basis 
{}
s
e
requires that 
 
 
1
ˆ
N
kss
qkq
k
TTδ
=
=
∑
                                                         (14.20)                                                         

Sec. 14. • Reciprocal Basis and Change of Basis 83 
 
A similar argument yields 
 
 
1
ˆ
N
qkq
kss
k
TTδ
=
=
∑
                                                         (14.21)                                                         
 
In matrix language we see that (14.20) or (14.21) requires that the matrix of elements 
k
q
Tbe the 
inverse of the matrix of elements 
ˆ
q
k
T.  It is easy to verify that the reciprocal bases are related by 
 
 
11
ˆ
ˆˆ
,
NN
kkqqqk
qk
qk
TT
==
==
∑∑
eeee
                                            (14.22)                                            
 
The covariant and contravariant components of 
v∈
V
relative to the two pairs of reciprocal bases 
are given by 
 
 
1111
ˆˆ
ˆˆ
NN N N
kkqq
kkqq
kk qq
vvvv
====
====
∑∑∑∑
ve  e  e  e
                                    (14.23)                                    
 
To obtain a relationship between, say, covariant components of    ,v one can substitute (14.22)
1
 into 
(14.23), thus 
 
111
ˆˆ
ˆ
NNN
kqq
kqq
kqq
vTv
===
=
∑∑∑
ee 
 
This equation can be rewritten in the form 
 
11
ˆ
ˆ
NN
kq
qkq
qk
Tv   v
==
⎛⎞
−=
⎜⎟
⎝⎠
∑∑
e0 
 
and it follows from the linear independence of the basis 
{}
ˆ
q
e
 that 
 
 
1
ˆ
N
k
qqk
k
vTv
=
=
∑
                                                          (14.24)                                                          
 
In a similar manner the following formulas can be obtained: 

84                                                              Chap.                                                              3                                                              • VECTOR SPACES 
 
1
ˆ
ˆ
N
qqk
k
k
vTv
=
=
∑
                                                          (14.25)                                                          
 
 
1
ˆ
N
kkq
q
q
vTv
=
=
∑
                                                          (14.26)                                                          
 
and 
 
 
1
ˆ
ˆ
N
q
kkq
q
vTv
=
=
∑
                                                          (14.27)                                                          
 
 
Exercises 
 
14.1
 Show that the quantities 
s
q
T and 
ˆ
k
q
T introduced in formulas (14.17) and (14.18) are given 
by the expressions 
 
 
ˆ
ˆˆ
,
sskk
qkq q
TT=⋅=⋅eeee                                              (14.28)                                              
 
14.2     Derive equation (14.21). 
14.3     Derive equations (14.25)-(14.27). 
14.4     Given the change of basis in a three-dimensional inner  product space 
V, 
 
11232 133123
2,,46=−−   =−+  =−+feeef eefeee 
 
find the quantities 
s
k
T and 
ˆ
.
k
q
T 
14.5     Given the vector 
123
=++ve e e in the vector space of the problem above, find the 
covariant and contravariant components of 
v relative to the basis 
{}
123
,,  .ff f
 
14.6     Show that under the basis transformation (14.17) 
 
,1
ˆˆ
ˆ
N
sq
kjkjkjsq
sq
eTTe
=
=⋅ =
∑
ee 
and 
,1
ˆˆ
ˆˆ
ˆ
N
kjkjkj    sq
sq
sq
eTTe
=
=⋅ =
∑
ee 
14.7     Prove Theorem 14.3. 

 85 
___________________________________________________________________________ 
Chapter 4 
 
 
LINEAR TRANSFORMATIONS 
 
 
Section 15    Definition of Linear Transformation 
 
In this section and in the other sections of this chapter we shall introduce and study a special 
class of functions defined on a vector space.  We shall assume that this space has an inner 
product, although this structure is not essential for the arguments in Sections 15-17.  If 
Vand U 
are vector spaces, a linear transformation is a function :→AVU such that 
 
(a) 
()()()
Au+v =Au+Av 
(b) 
()  ()
λλ=Au    Au
 
 
for all ∈u, vV and λ∈C.  Condition (a) asserts that A is a homomorphism on 
V
 with respect 
to the operation of addition and thus the theorems of Section 6 can be applied here.  Condition 
(b) shows that
A, in addition to being a homomorphism, is also homogeneous with respect to the 
operation of scalar multiplication.  Observe that the 
+
 symbol on the left side of (a) denotes 
addition inV, while on the right side it denotes addition in U.  It would be extremely 
cumbersome to adopt different symbols for these quantities.  Further, it is customary to omit the 
parentheses and write simply 
Au for 
()
Au when A is a linear transformation. 
 
Theorem 15.1. If  :→AVU is a function from a vector space V to a vector space,U then 
Ais a linear transformation if and only if 
 
()()()
+=+λμ λ   μAu  v    Au   Av
 
 
for all 
∈u, vV
and ,λμ∈C. 
 
The proof of this theorem is an elementary application of the definition.  It is also 
possible to show that  
 
 
()
1212
1212
RR
RR
λλ    λ  λ λλ++⋅⋅⋅+=++⋅⋅⋅+AvvvAvAvAv                    (15.1)                    

86  Chap. 4      •      LINEAR TRANSFORMATIONS 
 
for all 
1
,...,
R
∈vvV and 
1
,...,
R
λλ∈C. 
 
By application of Theorem 6.1, we see that for a linear transformation 
:→AVU 
 
 
()
and=−−A0 = 0AvAv
                                      (15.2)                                      
 
Note that in (15.2)
1
 we have used the same symbol for the zero vector in 
V
 as in 
.U
 
 
Theorem 15.2.  If 
{}
12
,,,
R
vvv...
is a linearly dependent set in Vand if :→AVUis a linear 
transformation, then 
{}
12
,,,
R
Av   AvAv...is a linearly dependent set in
U
. 
 
Proof  Since the vectors 
1
,...,
R
vv are linearly dependent, we can write  
 
1
R
j
j
j
λ
=
=
∑
v0 
 
where at least one coefficient is not zero. Therefore  
 
11
RR
jj
jj
jj
λλ
==
⎛⎞
==
⎜⎟
⎝⎠
∑∑
AvAv0
 
 
where (15.1) and (15.2) have been used.  The last equation proves the theorem. 
 
If the vectors 
1
,,
R
vv...are linearly independent, then their image set 
{}
12
,,,
R
Av   AvAv... 
may or may not be linearly independent.  For example, 
A
might map all vectors into .0  The 
kernel of a linear transformation :→AVU is the set 
 
()
{}
K=AvAv=0
 
 

Sec. 15            •            Definition                                        87 
In other words, 
()
KAis the preimage of the set 
{}
0 in   .U  Since 
{}
0 is a subgroup of the 
additive group
,U
 it follows from Theorem 6.3 that 
()
KA
 is a subgroup of the additive group.V 
However, a stronger statement can be made. 
 
Theorem 15.3.  The set 
()
KAis a subspace of 
V
. 
 
Proof.  Since 
()
KA is a subgroup, we only need to prove that if 
()
K∈vA, then 
()
Kλ∈vA 
for all 
.λ∈C  This is clear since 
 
()
λλ==Av    Av0 
 
for all 
v in 
()
KA
. 
 
The kernel of a linear transformation is sometimes called the 
null space.  The nullity of a 
linear transformation is the dimension of the kernel, i.e., 
()
dimKA.  Since 
()
KA is a subspace 
of 
,Vwe have, by Theorem 10.2, 
 
 
()
dim               dimK≤AV
                                                    (15.3)                                                    
 
Theorem 15.4.  A linear transformation :→AVU is one-to-one if and only if 
(){}
K=A0. 
 
Proof. This theorem is just a special case of Theorem 6.4.  For ease of reference, we shall repeat 
the proof.  If ,
Au = Av then by the linearity of 
A
, 
()
-=Au v    0.  Thus if
(){}
,K=A0 then 
Au = Av implies ,u=v so that A is one-to-one.  Now assume A is one-to-one.  Since 
()
KA
 
is a subspace, it must contain the zero in 
V
 and therefore A0 = 0.  If 
()
KA contained any other 
element    ,
v we would have ,Av = 0 which contradicts the fact that 
A
is one-to-one. 
 
Linear transformations that are one-to-one are called 
regular linear transformations.  The 
following theorem gives another condition for such linear transformations. 
 
Theorem 15.5.  A linear transformation :→AVUis regular if and only if it maps linearly 
independent sets in 
V to linearly independent sets in   .U 

88  Chap. 4      •      LINEAR TRANSFORMATIONS 
 
Proof.  Necessity.  Let 
{}
12
,,,
R
vvv... be a linearly independent set in V and :→AVU be a 
regular linear transformation. Consider the sum 
 
1
R
j
j
j
λ
=
=
∑
Av0
 
This equation is equivalent to  
1
R
j
j
j
λ
=
⎛⎞
=
⎜⎟
⎝⎠
∑
Av0
 
 
Since A is regular, we must have 
 
1
R
j
j
j
λ
=
=
∑
v0 
 
Since the vectors 
12
,,,
R
vvv... are linearly independent, this equation shows 
12
0
R
λλ    λ==⋅⋅⋅==, which implies that the set 
{}
1
,,
R
AvAv... is linearly independent. 
 
Sufficiency.  The assumption that A preserves linear independence implies, in particular, that 
≠Av0 for every nonzero vector ∈vVsince such a vector forms a linearly independent set.  
Therefore 
()
KA consists of the zero vector only, and thus A is regular. 
 
For a linear transformation 
:→AVU we denote the range of A by 
 
()
{}
R=∈AAvvV 
 
It follows form Theorem 6.2 that 
()
RA is a subgroup of    .U  We leave it to the reader to prove 
the stronger results stated below. 
 
Theorem 15.6.  The range 
()
RA is a subspace of    .U 
 

Sec. 15            •            Definition                                        89 
We have from Theorems 15.6 and 10.2 that 
 
 
()
dimdimR≤AU                                                     (15.4)                                                     
 
The 
rank of a linear transformation is defined as the dimension of 
()
RA
, i.e., 
()
dim.RA
  A 
stronger statement than (15.4) can be made regarding the rank of linear transformation 
A
. 
 
Theorem 15.7.  
()()
dimmin  dim, dim.R≤AVU 
 
Proof. Clearly, it suffices to prove that 
()
dimdimR≤AV, since this inequality and (15.4) 
imply the assertion of the theorem.  Let 
{}
12
,, ,
N
eee...
 be a basis forV, where dimN=V.  
Then 
∈vV
 can be written 
 
1
N
j
j
j=
=
∑
vve
 
 
Therefore any vector 
()
R∈AvAcan be written 
 
1
N
j
j
j
v
=
=
∑
AvAe 
 
Hence the vectors 
{}
12
,,,
N
Ae   AeAe...
generate 
()
RA
.  By Theorem 9.10, we can conclude 
 
 
()
dimdimR≤AV                                                     (15.5)                                                     
 
A result that improves on (15.5) is the following important theorem. 
 
Theorem 15.8.  If :→AVU is a linear transformation, then 
 
 
()()
dimdimdimRK=+AAV                                          (15.6)                                          

90  Chap. 4      •      LINEAR TRANSFORMATIONS 
 
Proof. Let 
()
dim,PK=A 
()
dim,RR=A and 
dim    .N=V
  We must prove that .NPR=+  
Select 
N vectors in V such that 
{}
12
,, ,
P
eee...
 is a basis for 
()
KA
 and 
{}
121
,, , ,   , ,
PPN+
eee ee...... a basis for 
V
.  As in the proof of Theorem 15.7, the vectors 
{}
121
,,, ,   ,,
PPN+
Ae   AeAe    AeAe...... generate
()
RA.  But, by the properties of the kernel, 
12p
==⋅⋅⋅==AeAeAe0.  Thus the vectors 
{}
12
,,,
PPN++
AeAeAe...
 generate 
()
RA
.  If we can 
establish that these vectors are linearly independent, we can conclude from Theorem 9.10 that 
the vectors 
{}
12
,,,
PPN++
AeAeAe...form a basis for 
()
RAand that 
()
dim.RRNP==−A 
Consider the sum 
 
1
R
j
Pj
j
λ
+
=
=
∑
Ae0 
 
Therefore 
1
R
j
pj
j
λ
+
=
⎛⎞
=
⎜⎟
⎝⎠
∑
Ae0
 
 
which implies that the vector 
()
1
R
j
Pj
j
Kλ
+
=
∈
∑
eA.  This fact requires that 
12
0
R
λλ    λ====", or otherwise the vector 
1
R
j
Pj
j
λ
+
=
∑
e could be expanded in the basis 
{}
12
,, ,
P
eee..., contradicting the linear independence of 
{}
12
,, ,
N
eee....  Thus the set 
{}
12
,,,
PPN++
AeAeAe... is linearly independent and the proof of the theorem is complete. 
 
As usual, a linear transformation 
:→AVU is said to be onto if 
()
,R=AU
 i.e., for 
every vector 
∈uUthere exists ∈vV such that 
Av = u
. 
 
Theorem 15.9.
  
()
dimdimR=AU
 if and only if A is onto. 
 
In the special case when  dimdim   ,
=VU it is possible to state the following important 
theorem.  
 
Theorem 15.10.
  If :→AVU is a linear transformation and if  dimdim   ,=VU then 
A
 is a 
linear transformation 
onto U if and only if A is regular. 

Sec. 15            •            Definition                                        91 
 
Proof. Assume that :→AVU is onto ,U then (15.6) and Theorem 15.9 show that 
 
()
dimdimdimdimK==+AVUU
 
 
Therefore 
()
dim0K=A and thus 
(){}
K=A0and A is one-to-one.  Next assume that A is 
one-to-one.  By Theorem 15.4, 
(){}
K=A0and thus
()
dim0K=A.  Then (15.6) shows that 
 
()
dimdimdimR==AVU
 
 
By using Theorem 10.3, we can conclude that  
 
()
R=AU 
 
and thus 
A is onto. 
 
 
Exercises 
 
15.1     Prove Theorems 15.1, 15.6, and 15.9 
15.2     Let     
:→AVUbe a linear transformation, and let 
1
Vbe a subspace of     .V  The 
restriction of 
A to
1
V
 is a function 
1
1
:→A
V
VU defined by 
 
1
Av=Av
V
 
 
for all 
1
.∈vV Show that 
1
A
V
 is a linear transformation and that 
()
()
1
1
KK=∩AA
V
V 
15.3
 Let :→AVU be a linear transformation, and define a function 
()
:/K→AAVU by 
 
=AvAv 
 

92  Chap. 4      •      LINEAR TRANSFORMATIONS 
for all .
∈vV  Here vdenotes the equivalence set of v in 
()
/.KAV  Show that A is a 
linear transformation.  Show also that 
Ais regular and that 
()
()
.RR=AA
 
15.4     Let     
1
Vbe a subspace of V, and define a function 
1
:/→PVVVby 
 
=Pvv 
for all 
v in 
V
.  Prove that P is a linear transformation, onto, and that 
()
1
.K=PV  The 
mapping 
P is called the canonical projection from Vto 
1
/VV. 
 
15.5
 Prove the formula 
 
 
()
11
dimdim/dim=+VVVV                                           (15.7)                                           
 
of Exercise 11.5 by applying Theorem 15.8 to the canonical projection 
P defined in the 
preceding exercise.  Conversely, prove the formula (15.6) of Theorem 15.8 by using the 
formula (15.7) and the result of Exercise 15.3. 
15.6
 Let U and Vbe vector spaces, and let ⊕UV be their direct sum.  Define mapping 
1
:⊕→PUVU and 
2
:⊕→PUVV by  
 
()
1
,Pu,v=u 
()
2
Pu,v=v 
 
for all 
∈uU
 and 
.∈vV
  Show that 
1
Pand 
2
P are onto linear transformations.  Show 
also the formula 
 
()
dimdimdim⊕+UV=UV
 
 
of Theorem 10.9 by using these linear transformations and the formula (15.6).  The 
mappings 
1
Pand 
2
P are also called the canonical projections from ⊕UVto Uand V, 
respectively. 

Sec. 16            •            Sums and Products  93 
 
Section 16.  Sums and Products of Liner Transformations 
 
In this section we shall assign meaning to the operations of 
addition and scalar 
multiplication
 for linear transformations.  If A and B are linear transformations→VU, then 
their 
sum +AB is a linear transformation defined by 
 
 
()
+=+ABv AvBv                                                   (16.1)                                                   
 
for all 
.∈vV  In a similar fashion, if ,λ∈C then λA is a linear transformation →VUdefined 
by  
 
 
()()
λλ=AvAv                                                      (16.2)                                                      
 
for all .
∈vV  If we write 
()
;LVU for the set of linear transformations from V to U, then 
(16.1) and (16.2) make 
()
;LVU
 a vector space.  The zero element in 
()
;LVU
is the linear 
transformation 
0defined by 
 
 
0v = 0                                                              (16.3)                                                              
 
for all .
∈vV  The negative of  
()
;∈ALVU is a linear transformation 
()
;−∈ALVUdefined 
by 
 
 1
−=−AA                                                           (16.4)                                                           
 
It follows from (16.4) that  -
A is the additive inverse of 
()
;∈ALVU
.  This assertion follows 
from 
 
 
()()()()
11  1  11 0−=+− =+− =− = =A+    AAAAAAA   0                      (16.5)                      
 

94  Chap. 4      •      LINEAR TRANSFORMATIONS 
where (16.1) and (16.2) have been used.  Consistent with our previous notation, we shall write 
−AB
 for the sum 
()
+−AB formed from the linear transformations 
A
 andB.  The formal 
proof that 
()
;LVU
 is a vector space is left as an exercise to the reader. 
 
Theorem 16.1.  
()
dim;dimdim   .=LVUVU 
 
Proof. Let 
{}
1
,...,
N
eebe a basis for V and 
{}
1
,...
M
b,b be a basis forU.  Define NM linear 
transformations 
:
k
α
→AVU by 
 
 
,1,,;1,,
,
k
k
k
p
kNM
kp
αα
α
α==...=...
=≠
Aeb
Ae0
                                    (16.6)                                    
 
If A is an arbitrary member of 
()
;LVU, then 
k
∈AeU, and thus 
 
1
,1,...,
M
kk
Ak N
α
α
α
=
==
∑
Aeb 
 
Based upon the properties of 
k
α
A
, we can write the above equation as 
 
111
MMN
ks
kkksk
s
AA
αα
αα
αα
===
==
∑∑∑
AeAeAe
 
 
Therefore, since the vectors 
1
,...,
N
ee generate V, we find 
 
11
MN
s
s
s
A
α
α
α
==
⎛⎞
−
⎜⎟
⎝⎠
∑∑
AA    v=0 
 
for all vectors .
∈vV  Thus, from (16.3), 
 
 
11
MN
s
s
s
A
α
α
α==
=
∑∑
AA                                                       (16.7)                                                       

Sec. 16            •            Sums and Products  95 
 
This equation means that the 
MN linear transformations 
s
α
A 
()
1,...,    ;1,...,sNMα== generate 
()
;LVU
.  If we can prove that these linear transformations are linearly independent, then the 
proof of the theorem is complete.  To this end, set 
 
11
MN
s
s
s
A
α
α
α
==
=
∑∑
A0 
 
Then, from (16.6), 
 
()
111
MNM
s
spp
s
AA
αα
αα
αα
===
==
∑∑∑
Aeb  0
 
 
Thus 
0
p
A
α
=, 
()
1,...,    ;1,...,pNMα== because the vectors 
1
,...,
M
bb are linearly independent 
in 
U.  Hence 
{}
s
α
A
 is a basis of
()
;LVU.  As a result, we have 
 
 
()
dim;dim    dimMN==LVUUV                                       (16.8)                                       
 
If 
:→AVU and :→BUW are linear transformations, their product is a linear 
transformation 
→VW, written 
BA
, defined by 
 
 
()
BAv = B  Av                                                        (16.9)                                                        
 
for all .
∈vV  The properties of the product operation are summarized in the following theorem. 
 
Theorem 16.2.   
 
 
()()
()
()
λμ  λ μ
λμ λ μ
+=+
+= +
CBA = CBA
ABCACBC
CA  B    CA  CB
                                           (16.10)                                           
 

96  Chap. 4      •      LINEAR TRANSFORMATIONS 
for all    ,
λμ∈C and where it is understood that     ,   , andABC are defined on the proper vector 
spaces so as to make the indicated products defined. 
 
The proof of Theorem 16.2 is left as an exercise to the reader. 
 
 
Exercises 
16.1 Prove that 
()
;LVU
 is a vector space 
16.2
 Prove Theorem 16.2. 
16.3
 Let     ,   , andVUWbe vector spaces. Given any linear mappings :→AVU and 
:→BUW
, show that  
 
()()()
()
dimmin  dim, dimRRR≤BAAB
 
 
16.4
 Let :→AVU be a linear transformation and define 
()
:/K→AAVUas in Exercises 
15.3.  If 
()
:/K→PAVV
 is the canonical projection defined in Exercise 15.4, show 
that 
 
A=AP 
 
This result along with the results of Exercises 15.3 and 15.4 show that every linear 
transformation can be written as the composition of an onto linear transformation and a 
regular linear transformation. 

Sec. 17            •            Special Types  97 
 
Section 17.  Special Types of Linear Transformations 
 
In this section we shall examine the properties of several special types of linear 
transformations.  The first of these is one called an 
isomorphism.  In section 6 we discussed 
group isomorphisms.  A vector space 
isomorphism is a regular onto linear transformation 
:→AVU.  It immediately follows from Theorem 15.8 that if :→AVU is an isomorphism, 
then  
 
 
dimdim=VU                                                        (17.1)                                                        
 
An isomorphism 
:→AVU establishes a one-to-one correspondence between the elements of 
Vand U.  Thus there exists a unique inverse function :→BUV with the property that if 
 
 
u=Av                                                              (17.2)                                                              
 
then 
 
 
()
v=B u                                                            (17.3)                                                            
 
for all 
∈uUand ∈vV.  We shall now show that B is a linear transformation.  Consider the 
vectors 
1
u and 
2
∈uU and the corresponding vectors [as a result of (17.2) and (17.3)] 
1
v and 
2
∈vV.  Then by (17.2), (17.3), and the properties of the linear transformationA,  
 
()()
()
()
()  ()
1212
12
12
12
λμ   λ μ
λμ
λμ
λμ
+=  +
=+
=+
=+
BuuBAvAv
BA   vv
vv
BuBu
 
 
Thus 
B is a linear transformation.  This linear transformation shall be written 
1−
A.  Clearly the 
linear transformation 
1−
A is also an isomorphism whose inverse is A; i.e., 
 
 
()
1
1
−
−
=AA                                                          (17.4)                                                          

98  Chap. 4      •      LINEAR TRANSFORMATIONS 
 
Theorem 17.1.  If :→AVU and     :→BUWare isomorphisms, then :→BAVW is an 
isomorphism whose inverse is computed by 
 
 
()
1
11
−
−−
=BAA   B
                                                      (17.5)                                                      
 
Proof. The fact that BA is an isomorphism follows directly from the corresponding properties 
of 
A and B.  The fact that the inverse of BA is computed by (17.5) follows directly because if  
 
 
andu=Avw=Bu 
 
then 
 
                                                                             and
-1-1
v=A uu=B w 
 
Thus  
 
()
1
11
−
−−
==vBAwABw
 
 
Therefore 
()
()
1
11
−
−−
−BAA   Bw = 0 for all ∈wW, which implies (17.5). 
 
The 
identity linear transformation 
:→IVV
 is defined by 
 
 
Iv = v                                                               (17.6)                                                               
 
for all 
v
 in     .V  Often it is desirable to distinguish the identity linear transformations on different 
vector spaces.  In these cases we shall denote the identity linear transformation by 
I
V
.  It follows 
from (17.1) and (17.3) that if 
A
 is an isomorphism, then 
 
 
11
and
−−
==AAIA   AI
UV
                                         (17.7)                                         
 
Conversely, if 
A is a linear transformation from V toU, and if there exists a linear 
transformation     :
→BUVsuch that =ABI
U
 and=BAI
V
, then 
A
 is an isomorphism and 

Sec. 17            •            Special Types  99 
1
.
−
B=A  The proof of this assertion is left as an exercise to the reader.  Isomorphisms are often 
referred to as 
invertible or nonsingular linear transformations. 
 
A vector space 
V
and a vector space 
U
are said to be isomorphic if there exists at least 
one isomorphism from 
VtoU. 
 
Theorem 17.2.  Two finite-dimensional vector spaces Vand U are isomorphic if and only if 
they have the same dimension. 
 
Proof. Clearly, if 
V
and 
U
 are isomorphic, by virtue of the properties of isomorphisms, 
dimdimV=U.  If U and V have the same dimension, we can construct a regular onto linear 
transformation 
:→AVUas follows.  If 
{}
1
,,
N
ee... is a basis for Vand 
{}
1
,,
N
bb... is a basis 
for 
U, define A by 
 
                                                                        ,1,...,
kk
kN==Aeb                                               (17.8)                                               
 
Or, equivalently, if  
1
N
kk
k
v
=
=
∑
ve 
then define 
A
 by 
 
 
1
N
k
k
k
v
=
=
∑
Avb                                                         (17.9)                                                         
 
A is regular because if ,Av = 0 then .v=0 Theorem 15.10 tells us A is onto and thus is an 
isomorphism. 
 
As a corollary to Theorem 17.2, we see that 
V
 and the vector space ,
N
C where 
dim    ,
N=V are isomorphic. 
 
In Section 16 we introduced the notation 
()
;LVU
for the vector space of linear 
transformations from 
V
 to 
U
.  The set 
()
;LVV corresponds to the vector space of linear 
transformations 
→VV.  An element of 
()
;LVVis called an endomorphism ofV. This 
nomenclature parallels the previous usage of the word endomorphism introduced in Section 6.  If 

100  Chap. 4      •      LINEAR TRANSFORMATIONS 
an endomorphism is regular (and thus onto), it is called an 
automorphism.  The identity linear 
transformation defined by (17.6) is an example of an automorphism.  If 
()
;,∈ALVVthen it is 
easily seen that  
 
 
AI = IA = A                                                       (17.10)                                                       
 
Also, if 
A and are in
()
;LVV
, it is meaningful to compute the products ABand BA; 
however, 
 
 
≠ABBA
                                                          (17.11)                                                          
 
in general.  For example, let 
V be a two-dimensional vector space with basis 
{}
12
,ee and define 
A and B by the rules 
 
22
11
and
jj
kkjkkj
jj
AB
==
==
∑∑
AeeBee 
 
where 
j
k
Aand 
j
k
B. ,1,2kj=, are prescribed.  Then 
 
22
,1,1
and
jljl
kkjlkkjl
jljl
ABB A
==
==
∑∑
BAeeABee 
 
An examination of these formulas shows that it is only for special values of  
j
k
Aand 
j
k
Bthat 
.AB = BA
 
 
The set 
()
;LVV has defined on it three operations.  They are (a) addition of elements 
of 
()
;LVV, (b) multiplication of an element of 
()
;LVV by a scalar, and (c) the product of a 
pair of elements of 
()
;LVV.  The operations (a) and (b) make 
()
;LVVinto a vector space, 
while it is easily shown that the operations (a) and (c) make 
()
;LVV into a ring.  The structure 
of 
()
;LVV is an example of an associative algebra. 
 

Sec. 17            •            Special Types  101 
The subset of 
()
;LVVthat consists of all automorphisms of V is denoted by 
()
.GLV  
It is immediately apparent that 
()
GLV
 is not a subspace of 
()
;LVV
, because the sum of two 
of its elements need not be an automorphism.  However, this set is easily shown to be a 
group 
with respect to the product operation.  This group is called the 
general linear group.  Its identity 
element is 
I and if 
()
,∈AGLV its inverse is 
()
1
.
−
∈AGLV 
 
A 
projection is an endomorphism 
()
;∈PLVV which satisfies the condition 
 
 
2
=PP                                                            (17.12)                                                            
 
The following theorem gives an important property of a projection. 
 
Theorem 17.3.  If :→PVV is a projection, then 
 
 
()()
RK=⊕PPV                                                   (17.13)                                                   
 
Proof. Let v be an arbitrary vector in V.  Let 
 
 -
w=v  Pv                                                         (17.14)                                                         
 
Then, by (17.12), 
()
.−=−Pw = Pv    P  PvPv    Pv = 0  Thus, 
()
.K∈wP  Since 
()
,R∈PvP 
(17.14) implies that  
 
()()
RK=+PPV 
 
To show that 
()  () {}
,RK∩=PP0
 let 
()()
RK∈∩uP   P
.  Then, since 
()
R∈uP
 for some 
∈vV, =uPv.  But, since 
u
 is also in
()
KP, 
 
()
0 = Pu = P  Pv   = Pv = u 
 
which completes the proof. 

102  Chap. 4      •      LINEAR TRANSFORMATIONS 
 
The name 
projection arises from the geometric interpretation of (17.13).  Given any 
∈vV, then there are unique vectors 
()
R∈uP and 
()
K∈wP such that  
 
 
v=u+w                                                          (17.15)                                                          
 
where  
 
 
andPu = uPw = 0                                          (17.16)                                          
 
Geometrically, 
P takes v and projects in onto the subspace 
()
RP along the subspace
()
KP.  
Figure 5 illustrates this point for 
2
=VR. 
 
 
Given a projection
P, the linear transformation −IP is also a projection.  It is easily 
shown that  
 
()()
RK=−⊕ −IPIPV 
 
v
 
=uPv
()
=−wIPw 
Figure 5 

Sec. 17            •            Special Types  103 
and  
 
()()()()
,RKKR−=−=IPPIPP 
 
It follows from (17.16) that the restriction of 
P to 
()
RP
 is the identity linear transformation on 
the subspace 
()
RP.  Likewise, the restriction of −IP to 
()
KP is the identity linear 
transformation on 
()
KP.  Theorem 17.3 is a special case of the following theorem. 
 
Theorem 17.4.
  If 
k
P,       1,...,kR=,  are projection operators with the properties that 
 
 
2
,1,...,
,   
kk
kq
kR
kq
==
=≠
PP
PP0
                                            (17.17)                                            
 
and 
 
 
1
R
k
k
=
=
∑
IP                                                           (17.18)                                                           
 
then 
 
 
()()()
12R
RRR=⊕⊕⋅⋅⋅⊕PPPV
                                       (17.19)                                       
 
The proof of this theorem is left as an exercise for the reader.  As a converse of Theorem 
17.4, if 
V has the decomposition 
 
 
1R
=⊕⋅⋅⋅⊕VVV                                                    (17.20)                                                    
 
then the endomorphisms :
k
→PVVdefined by 
 
                                                                       ,1,...,
kk
kR==Pv   v                                              (17.21)                                              
 

104  Chap. 4      •      LINEAR TRANSFORMATIONS 
where 
 
 
12R
=+  +⋅⋅⋅+vv vv                                                  (17.22)                                                  
 
are projections and satisfy (17.18).  Moreover, 
()
kk
R=PV,       1,,kR=.... 
 
 
Exercises 
 
17.1 Let :→AVU and 
:→BUV
 be linear transformations. If ,AB = I then B is the right 
inverse
 of 
A
. If ,BA = I then  B is the left inverse of
A
.  Show that 
A
 is an 
isomorphism if and only if it has a right inverse and a left inverse. 
17.2
 Show that if an endomorphism Aof V commutes with every endomorphism B of V, 
then 
A
 is a scalar multiple of   .I 
17.3
 Prove Theorem 17.4 
17.4
 An involution is an endomorphism Lsuch that 
2
.=LI
  Show that L is an involution if 
and only if 
()
1
2
=+PLI is a projection. 
17.5
 Consider the linear transformation 
1
1
:→A
V
VU defined in Exercise 15.2.  Show that if 
()
1
,K=⊕AVV then 
1
A
V
is am isomorphism from 
1
Vto 
()
.RA 
17.6
 If A is a linear transformation from V to U where  dimdim   ,=VU assume that there 
exists a linear transformation 
:→BUV
such that 
()
or.==ABIBAI  Show that Ais 
an isomorphism and that 
1
.
−
=BA 

Sec. 18            •            Adjoint                                                105 
 
Section 18.  The Adjoint of a Linear Transformation 
 
The results in the earlier sections of this chapter did not make use of the inner product 
structure on
V.  In this section, however, a particular inner product is needed to study the adjoint 
of a linear transformation as well as other ideas associated with the adjoint. 
 
Given a linear transformation
:→AVU
, a function :
∗
→AUV is called the adjoint of 
A
 if 
 
 
()
()
∗
⋅=⋅uAv    Auv                                                    (18.1)                                                    
 
for all 
∈vV and ∈uU.  Observe that in (18.1) the inner product on the left side is the one in 
U, while the one for the right side is the one in V.  Next we will want to examine the properties 
of the adjoint.  It is probably worthy of note here that for linear transformations defined on real 
inner product spaces what we have called the 
adjoint is often called the transpose.  Since our 
later applications are for real vector spaces, the name transpose is actually more important. 
 
Theorem 18.1.  For every linear transformation
:→AVU, there exists a unique adjoint 
:
∗
→AUV satisfying the condition (18.1). 
 
Proof.  Existence.  Choose a basis 
{}
1
,,
N
ee... for V and a basis 
{}
1
,,
M
bb...for U.  Then 
A
 
can be characterized by the 
MN×matrix 
k
A
α
⎡⎤
⎣⎦
 in such a way that 
 
 
1
,1,...,
M
kk
AkN
α
α
α
=
==
∑
Aeb                                        (18.2)                                        
 
This system suffices to define A since for any 
∈vV
 with the representation 
 
1
N
k
k
k
v
=
=
∑
ve
 
 
the corresponding representation of 
Av is determined by 
k
A
α
⎡⎤
⎣⎦
and 
k
v
⎡⎤
⎣⎦
by 

106  Chap. 4      •      LINEAR TRANSFORMATIONS 
 
11
MN
k
k
k
Av
α
α
α==
⎛⎞
=
⎜⎟
⎝⎠
∑∑
Avb 
 
Now let 
{}
1
,,
N
ee...and 
{}
1
,...,
M
bbbe the reciprocal bases of 
{}
1
,,
N
ee...
 and 
{}
1
,,
M
bb...
, 
respectively.  We shall define a linear transformation 
∗
A by a system similar to (18.2) except 
that we shall use the reciprocal bases.  Thus we put 
 
 
1
N
k
k
k
A
αα∗∗
=
=
∑
Abe                                                      (18.3)                                                      
 
where the matrix 
k
A
α∗
⎡⎤
⎣⎦
 is defined by 
 
 
 kk
AA
αα∗
≡                                                           (18.4)                                                           
 
for all 1,...,
Mα= and 1,...,kN=.  For any vector  ∈uU the representation of 
∗
Au relative to 
{}
1
,...,
N
ee is then given by 
11
NM
k
k
k
Au
α
α
α
∗∗
==
⎛⎞
=
⎜⎟
⎝⎠
∑∑
Aue 
 
where the representation of 
u itself relative to 
{}
1
,...,
M
bb is  
 
1
M
u
α
α
α
=
=
∑
ub
 
 
Having defined the linear transformation 
∗
A, we now verify that A and 
∗
A satisfy the relation 
(18.1).  Since 
A
 and 
∗
A are both linear transformations, it suffices to check (18.1) for uequal to 
an arbitrary element of the basis 
{}
1
,...,
M
bb, say 
α
=ub, and for v equal to an arbitrary element 
of the basis 
{}
1
,...,
N
ee
, say 
k
v=e.  For this choice of u and v we obtain from (18.2) 
 

Sec. 18            •            Adjoint                                                107 
 
     
11
MM
kkkk
AAA
ααββαα
ββ
ββ
δ
==
⋅=⋅    =    =
∑∑
bAe  bb                                   (18.5)                                   
 
and likewise from (18.3) 
 
 
11
NN
ll
klklkk
ll
AAA
αααα
δ
∗∗ ∗∗
==
⎛⎞
⋅=⋅==
⎜⎟
⎝⎠
∑∑
Ab   ee    e                                (18.6)                                
 
Comparing (18.5) and (18.6) with (18.4), we see that the linear transformation 
∗
A defined by 
(18.3) satisfies the condition 
 
kk
αα∗
⋅=⋅bAe  Abe 
 
for all 1,...,
Mα= and 1,...,kN=, and hence also the condition (18.1) for all ∈uU and ∈vV. 
 
Uniqueness.  Assume that there are two functions 
1
:
∗
→AUV and 
2
:
∗
→AUV which 
satisfy (18.1).  Then 
 
()()
()
12
∗∗
⋅=   ⋅=⋅Au  vAu  v   u  Av 
 
Thus 
()
12
0
∗∗
−⋅=Au  Au  v 
 
Since the last formula must hold for all 
∈vV, the inner product properties show that 
 
12
∗∗
=Au   Au 
 
This formula must hold for every 
∈uU and thus 
 
12
∗∗
=AA 

108  Chap. 4      •      LINEAR TRANSFORMATIONS 
 
As a corollary to the preceding theorem we see that the adjoint 
∗
A of a linear 
transformation 
A is a linear transformation.  Further, the matrix 
k
A
α∗
⎡⎤
⎣⎦
 that characterizes 
∗
A 
by (18.3) is related to the matrix 
k
A
α
⎡⎤
⎣⎦
 that characterizes A by (18.2).  Notice the choice of 
bases in (18.2) and (18.3), however. 
 
Other properties of the adjoint are summarized in the following theorem. 
 
Theorem 18.2. 
  (18.7) 
(a)
 
()
∗
∗∗
+=+AB   A B         (18.7)
1 
(b) 
()
∗
∗∗
=ABB A          (18.7)
2
 
(c)
 
()
λλ
∗
∗
=AA          (18.7)
3
 
(d)
 
∗
=00           (18.7)
4
 
(e)
 
∗
=II           (18.7)
5
 
(f)
 
()
∗
∗
=AA          (18.7)
6
 
 
and 
(g)
 If 
A
 is nonsingular, so is 
*
Aand in addition, 
 
 
*11−−∗
=AA                                                           (18.8)                                                           
 
In (a), 
Aand B are in 
()
;LVU; in (b), 
()
;∈BLVUand 
()
;∈ALUW; in (c), ;λ∈C 
in(d), 
0 is the zero element; and in (e), I is the identity element in 
()
;LVV.  The proof of the 
above theorem is straightforward and is left as an exercise for the reader. 
 
Theorem 18.3.  If :→AVU is a linear transformation, then Vand U have the orthogonal 
decompositions 
 
 
()
()
RK
∗
=⊕AAV                                                   (18.9)                                                   
 
and 
 

Sec. 18            •            Adjoint                                                109 
 
()
()
RK
∗
=⊕AAU                                                  (18.10)                                                  
 
where 
 
 
()
()
RK
⊥
∗
=AA                                                    (18.11)                                                    
 
and 
 
 
()
()
RK
⊥
∗
=AA                                                    (18.12)                                                    
 
Proof. We shall prove (18.11) and (18.12) and then apply Theorem 13.4 to obtain (18.9) and 
(18.10).  Let 
u be and arbitrary element in 
()
K
∗
A.  Then for every
∈vV
, 
()
()
0
∗
⋅= ⋅=uAv    Auv
.  Thus 
()
K
∗
A
 is contained in 
()
R
⊥
A
.  Conversely, take 
()
R
⊥
∈uA
; 
then for every 
∈vV
, 
()
()
0
∗
⋅=⋅   =Au  v   u  Av, and thus 
()
∗
=Au0, which implies that 
()
K
∗
∈uA
 and that 
()
R
⊥
A
 is in 
()
K
∗
A
.  Therefore, 
()
()
RK
⊥
∗
=AA, which by Exercise 13.3 
implies (18.11).  Equation (18.11) follows by an identical argument with 
Α replaced by 
∗
A.  As 
mentioned above, (18.9) and (18.10) now follow from Theorem 13.4. 
 
Theorem 18.4.  Given a linear transformation:→AVU, then 
Α
 and 
∗
A have the same rank.  
 
Proof
. By application of Theorems 10.9 and 18.3, 
 
()
()
dimdimdimRK=+
*
AAV 
 
However, by (15.6), 
 
()()
dimdimdimRK=+AAV
 
 
Therefore, 
 
 
()
()
dimdimRR=
*
AA                                               (18.13)                                               

110  Chap. 4      •      LINEAR TRANSFORMATIONS 
 
which is the desired result. 
 
An endomorphism 
()
;∈ALVV is called Hermitian if 
∗
A=Aand skew-Hermitian if 
∗
=−AA.  It should be pointed out, however, that the terms symmetric and skew-symmetric are 
often used instead of Hermitian and skew-Hermitian for linear transformations defined on real 
inner product spaces.  The following theorem, which follows directly from the above definitions 
and from (18.1), characterizes Hermitian and skew-Hermitian endomorphisms. 
 
Theorem 18.5.  An endomorphism Ais Hermitian if and only if  
 
 
()()
1212
⋅=⋅vAv    Avv
                                                (18.14)                                                
 
for all 
12
,,∈vvV and it is skew-Hermitian if and only if 
 
 
()()
1212
⋅=−⋅vAvAvv                                               (18.15)                                               
 
for all 
12
,∈vvV. 
 
We shall denote by 
()
;SVVand 
()
;AVV the subsets of 
()
;LVV defined by 
 
()()
{}
;; and 
∗
=∈=AAA   ASVVLVV 
 
and 
 
()()
{}
;; and 
∗
=∈=−AAAAAVVLVV 
 
In the special case of a real inner product space, it is easy to show that 
()
;SVV
 and 
()
;AVV
 
are both subspaces of 
()
;LVV.  In particular, 
()
;LVV has the following decomposition: 
 
Theorem 18.6.  For a real inner product space, 

Sec. 18            •            Adjoint                                                111 
 
 
()()()
;; ;=⊕LVVSVVAVV                                      (18.16)                                      
 
Proof. An arbitrary element 
()
;∈LLVV
can always be written 
 
 
L=S+A                                                          (18.17)                                                          
 
where 
 
 
()()
11
and
22
TTTT
=+==−=−SLLSALL A 
 
Here the superscript 
T denotes that transpose, which is the specialization of the adjoint for a real 
inner product space.  Since 
()
;∈SSVVand 
()
;∈AAVV, (18.17) shows that 
 
()()()
;;;=LVVSVV+AVV
 
 
Now, let 
()()
;;∈∩BSVVAVV.  Then B must satisfy the conditions 
 
T
=BB                        and                        
T
=−BB 
 
Thus, 
=B0and the proof is complete. 
 
We shall see in the exercises at the end of Section 19 that a real inner product can be 
defined on 
()
;LVV in such a fashion that the subspace of symmetric endomorphisms is the 
orthogonal complement of the subspace of skew-symmetric endomorphisms.  For 
complex inner 
product spaces, however, 
()
;SVV and 
()
;AVV are not subspaces of 
()
;LVV.  For 
example, if  
()
;∈ASVV, then iA is in 
()
;AVV because  
()
ii i i
∗
∗∗
==−  =−AA A A, where 
(18.7)
3
 has been used. 
 
A linear transformation 
()
;∈ALVU is unitary if  

112  Chap. 4      •      LINEAR TRANSFORMATIONS 
 
 
2121
⋅=⋅Av    Avv    v                                                    (18.18)                                                    
 
for all 
12
,∈vvV.  However, for a real inner product space the above condition defines A to be 
orthogonal.  Essentially, (18.18) asserts that unitary (or orthogonal) linear transformations 
preserve the inner products. 
 
Theorem 18.7.  If A is unitary, then it is regular. 
 
Proof. Take 
12
==vv v in (18.18), and by (12.1) we find 
 
 
=Avv                                                         (18.19)                                                         
 
Thus, if 
Av = 0, then v=0, which proves the theorem. 
 
Theorem 18.8.  
()
;∈ALVU is unitary if and only if =Avv for all ∈vV. 
 
Proof. If A is unitary, we saw in the proof of Theorem 18.7 that 
=Avv
.  Thus, we shall 
assume 
=Avv for all ∈vVand attempt to derive (18.18).  This derivation is routine 
because, by the polar identity of Exercise 12.1, 
 
()()()
()
22
22
12121212
21iii⋅= + +  + −+    +Av   AvA  vvA  vvAvAv 
 
Therefore, by (18.19), 
 
()
()
22  22
12 12121212
212iii⋅=+++ −+  + =⋅Av   Avvvvvvvv    v
 
 
This proof cannot be specialized directly to a real inner product space since the polar identity is 
valid for a complex inner product space only.  We leave the proof for the real case as an exercise. 
 
If we require 
V and U to have the same dimension, then Theorem 15.10 ensures that a 
unitary transformation A is an isomorphism.  In the case we can use (18.1) and (18.18) and 
conclude the following 

Sec. 18            •            Adjoint                                                113 
Theorem 18.9.  Given a linear transformation
()
;∈ALVU, where dimdim=VU; then 
A
 is 
unitary if and only if it is an isomorphism whose inverse satisfies 
 
 
1−∗
=AA                                                          (18.20)                                                          
 
Recall from Theorem 15.5 that a regular linear transformation maps linearly independent 
vectors into linearly independent vectors.  Therefore if 
{}
1
,,
N
ee...
 is a basis for V and 
()
;∈ALVU is regular, then 
{}
1
,,
N
AeAe... is basis for 
()
RA which is a subspace in 
U
.  If 
{}
1
,,
N
ee... is orthonormal and 
A
 is unitary, it easily follows that 
{}
1
,,
N
AeAe... is also 
orthonormal.  Thus the image of an orthonormal basis under a unitary transformation is also an 
orthonormal set.  Conversely, a linear transformation which sends an orthonormal basis of 
V 
into an orthonormal basis of 
()
RA must be unitary.  To prove this assertion, let 
{}
1
,,
N
ee... be 
orthonormal, and let 
{}
1
,,
N
bb...
be an orthonormal set in 
,U
 where 
kk
=bAe,       1,,kN=....  
Then, if 
1
v and 
2
vare arbitrary elements of V, 
 
112 2
11
and
NN
kl
kl
kl
vv
==
==
∑∑
veve 
 
we have  
 
 
112 2
11
and
NN
kk
kl
kl
vv
==
==
∑∑
AvbAvb
 
 
and, thus, 
 
 
1212212212
111
NNN
klkl
kl
klk
vvvv
===
⋅=⋅=   =⋅
∑∑∑
Av   Avb    bv    v                           (18.21)                           
 
Equation (18.21) establishes the desired result. 
 
Recall that in Section 17 we introduced the 
general linear group 
()
GLV.  We define a 
subset 
()
UVof 
()
GLVby 
 

114  Chap. 4      •      LINEAR TRANSFORMATIONS 
()()
{}
1
and
−∗
=∈=AAAAUVGLV 
 
which is easily shown to be a 
subgroup.  This subgroup is called the unitary group ofV. 
 
In Section 17 we have defined projections 
:→PVV
 by the characteristic property 
 
 
2
=PP                                                            (18.22)                                                            
 
In particular, we have showed in Theorem 17.3 that 
Vhas the decomposition 
 
 
()()
RK=⊕PPV                                                   (18.23)                                                   
 
There are several additional properties of projections which are worthy of discussion here. 
 
Theorem 18.10.  If P is a projection, then 
()  ()
RK
⊥
∗
=⇔  =PPPP. 
 
Proof. First take 
∗
=PP and let v be an arbitrary element ofV.  Then by (18.23) 
 
v=u+w 
 
where 
u=Pv and −w=v   Pv.  Then 
 
()
() ()
()
()
()
()
2
0
∗
⋅⋅−
=⋅−  ⋅
=⋅−⋅
=⋅−   ⋅
=
uw=Pv  v  Pv
Pv    vPv    Pv
Pv    vP Pv    v
Pv    vP  v    v
 
 
where (18.22), (18.1), and the assumption that P is Hermitian have been used.  Conversely, 
assume 
0⋅=uw for all 
()
R∈uP and all 
()
K∈wP.  Then if 
1
v and 
2
v are arbitrary vectors in 
V 

Sec. 18            •            Adjoint                                                115 
 
()
1212   221    2
⋅= ⋅  +−  = ⋅Pv    vPvPvvPvPv   Pv 
 
and, by interchanging 
1
v and 
2
v 
 
212    1
⋅=⋅Pv    vPv    Pv 
 
Therefore, 
()
1221   12
⋅= ⋅=⋅Pv    vPv    vvPv 
 
This last result and Theorem 18.5 show that P is Hermitian. 
 
Because of Theorem 18.10, Hermitian projections are called 
perpendicular projections. 
In Section 13 we introduced the concept of an orthogonal complement of a subspace of an inner 
product space. A similar concept is that of an orthogonal pair of subspaces.  If 
1
V and 
2
Vare 
subspaces of 
V
, they are orthogonal, written 
12
⊥VV, if 
12
0⋅=vv for all 
11
∈vV and 
22
∈vV 
 
Theorem 18.11.  If 
1
V and 
2
V are subspaces of 
V
, 
1
P is the perpendicular projection of 
V
onto 
1
V, and 
2
P is the perpendicular projection of Ponto 
2
V, then 
12
⊥VV if and only if 
21
=PP    0. 
 
Proof. Assume that
12
⊥VV; then 
12
⊥
∈PvV for all ∈vV and thus 
21
=PPv   0 for all ∈vV 
which yields
21
=PP    0.  Next assume
21
=PP    0; this implies that 
12
⊥
∈PvV for every∈vV.  
Therefore 
1
V is contained in 
2
⊥
V and, as a result, 
12
⊥VV. 
 
 
Exercises 
 
18.1
 Prove Theorem 18.2 
18.2
 For a real inner product space, prove that 
()()
1
dim;1
2
NN=+SVV
 and 
()()
1
dim;1
2
NN=−AVV, where dimN=V.   
18.3
 Define 
() ()
:;;→ΦLVVLVV and 
()()
:;;→ΨLVVLVV by 

116  Chap. 4      •      LINEAR TRANSFORMATIONS 
 
()
1
2
T
=ΦAA+A      and            
()
1
2
T
=−ΨAAA 
 
where 
V
 is a real inner product space.  Show that Φ and Ψ are projections. 
18.4
 Let 
1
V and 
2
Vbe subspaces of 
V
 and let 
1
P be the perpendicular projection of 
V
 onto 
1
Vand 
2
Pbe the perpendicular projection of 
V
onto
2
V.  Show that 
12
+PP is a 
perpendicular projection if and only if
12
⊥VV. 
18.5
 Let 
1
P and 
2
P be the projections introduced in Exercise 18.4.  Show that 
12
−PP is a 
projection if and only if 
2
V is a subspace of
1
V. 
18.6
 Show that 
()
;∈ALVV
 is skew-symmetric =0⇔⋅vAv for all ∈vV, where V is real 
vector space 
18.7
 A linear transformation 
()
;∈ALVV is normal if 
∗∗
=AA   AA.  Show that A is normal 
if and only if 
 
121  2
∗∗
⋅= ⋅Av   AvA v   A v 
 
for all 
12
,vv in V. 
18.8
 Show that 
()
()
∗
⋅vA+Av is real for every∈vV and every
()
;∈ALVV. 
18.9
 Show that every linear transformation 
()
;∈ALVV, where 
V
 is a complex inner 
product space, has the unique decomposition 
 
i+A=B    C
 
 
where Band 
C are both Hermitian. 
18.10
 Prove Theorem 18.8 for real inner product spaces. Hint: A polar identity for a real inner 
product space is  
 
222
2⋅= + −  −uv   u  vuv 
 
18.11
 Let A be an endomorphism of 
3
R whose matrix relative to the standard basis is  
 
121
463
100
⎡⎤
⎢⎥
⎢⎥
⎢⎥
⎣⎦
 

Sec. 18            •            Adjoint                                                117 
 
What is 
()
KA
?  What is 
()
T
RA?  Check the results of Theorem 18.3 for this particular 
endomorphism. 

118  Chap. 4      •      LINEAR TRANSFORMATIONS 
 
Section 19.   Component Formulas 
 
In this section we shall introduce the components of a linear transformation and several 
related ideas.  Let
()
;∈ALVU
, 
{}
1
,,
N
ee...
 a basis forV, and 
{}
1
,,
M
bb...
 a basis for U.  The 
vector 
k
Ae is in U and, as a result, can be expanded in a basis of U in the form  
 
 
1
M
kk
A
α
α
α=
=
∑
Aeb                                                       (19.1)                                                       
 
The 
MN scalars 
k
A
α
 
()
1,,;1,,MkNα==......
 are called the components of A with respect to 
the bases.  If 
{}
1
,,
M
bb... is a basis of  U which is reciprocal to
{}
1
,,
M
bb..., (19.1) yields 
 
 
()
kk
A
αα
=⋅Aeb
                                                       (19.2)                                                       
 
Under change of bases in 
V and Udefined by [cf. (14.18) and (14.22)
1
] 
 
 
1
ˆ
ˆ
N
j
kkj
j
T
=
=
∑
ee
                                                          (19.3)                                                          
 
and 
 
 
1
ˆ
M
S
ααβ
β
β
=
=
∑
bb                                                         (19.4)                                                         
 
(19.2) can be used to derive the following transformation rule for the components ofA: 
 
 
   
11
ˆ
ˆ
MN
j
kjk
j
ASAT
ααβ
β
β
==
=
∑∑
                                                   (19.5)                                                   
 
where 

Sec. 19            •            Component Formulas  119 
 
()
   
ˆ
ˆ
ˆ
jj
A
ββ
=⋅Aeb 
 
If 
()
;∈ALVV, (19.1) and (19.5) specialize to 
 
 
1
N
q
kkq
q
A
=
=
∑
Aee                                                       (19.6)                                                       
 
and 
 
 
  
,1
ˆ
ˆ
N
qqsj
ksjk
sj
ATAT
=
=
∑
                                                     (19.7)                                                     
 
The 
trace of an endomorphism is a function 
()
tr :;→LVVCdefined by 
 
 
1
tr
N
k
k
k
A
=
=
∑
A                                                         (19.8)                                                         
 
It easily follows from (19.7) and (14.21) that 
trA
 is independent of the choice of basis ofV.  
Later we shall give a definition of the trace which does not employ the use of a basis. 
 
If 
()
;∈ALVU
, then 
()
;
∗
∈ALUV
.  The components of 
∗
Aare obtained by the same 
logic as was used in obtaining (19.1).  For example, 
 
 
1
N
k
k
k
A
αα∗∗
=
=
∑
Abe                                                      (19.9)                                                      
 
where the 
MN scalars 
k
A
α∗
 
()
1,...,    ;1,...,kNMα==
 are the components of 
∗
A with respect to 
{}
1
,...,
M
bband 
{}
1
,...,
N
ee.  From the proof of Theorem 18.1 we can relate these components of 
∗
A to those of Ain (19.1);  namely, 
 

120  Chap. 4      •      LINEAR TRANSFORMATIONS 
 
ss
AA
αα∗
=                                                         (19.10)                                                         
 
If the inner product spaces 
U and V are real, (19.10) reduces to  
 
 
T
ss
AA
αα
=
                                                         (19.11)                                                         
 
By the same logic which produced (19.1), we can also write 
 
 
1
M
kk
A
α
α
α
=
=
∑
Aeb
                                                     (19.12)                                                     
 
and  
 
 
11
MM
kk
k
AA
αα
αα
αα
==
==
∑∑
Aebb                                            (19.13)                                            
 
If we use (14.6) and (14.9), the various components of 
A
 are related by  
 
 
1111
NMNM
ss
kksksk
ss
AAe   bAeAb
ααβαβα
ββ
ββ
====
===
∑∑∑  ∑
                              (19.14)                              
 
where 
 
 
andbb b b
αβαββα
αβαββα
=⋅==⋅=bbbb                           (19.15)                           
 
A similar set of formulas holds for the components of 
∗
A.  Equations (19.14) and (19.10) can be 
used to obtain these formulas.  The transformation rules for the components defined by (19.12) 
and (19.13) are easily established to be 
 
 
()
11
ˆˆ
ˆ
MN
j
kkjk
j
ASAT
β
αααβ
β
==
=⋅=
∑∑
Aeb                                       (19.16)                                       

Sec. 19            •            Component Formulas  121 
 
 
()
11
ˆ
MN
kkjk
j
j
ASAT
αααβ
β
β
==
=⋅=
∑∑
Aeb                                       (19.17)                                       
 
and  
 
 
()
11
ˆˆ
MN
kkjk
j
j
ASAT
β
αααβ
β
==
=⋅=
∑∑
Aeb
                                       (19.18)                                       
 
where 
 
 
()
ˆ
ˆ
ˆ
jj
A
ββ
=⋅Aeb                                                     (19.19)                                                     
 
 
 
()
ˆ
ˆ
ˆ
jj
A
ββ
=⋅Aeb
                                                     (19.20)                                                     
 
and 
 
 
()
ˆ
ˆ
ˆ
jj
A
ββ
=⋅Aeb                                                     (19.21)                                                     
 
The quantities 
ˆ
S
β
α
 introduced in (19.16) and (19.18) above are related to the quantities 
S
α
β
 by 
formulas like (14.20) and (14.21). 
 
 
Exercises 
 
19.1 Express (18.20), written in the form
∗
ΑA=I, in components. 
19.2
 Verify (19.14) 
19.3
 Establish the following properties of the trace of endomorphisms of V: 
 
()
()
trdim
trtrtr
trtr
tr           tr
λλ
=
=+
=
=
I
A+BAB
AA
ABBA
V
 

122  Chap. 4      •      LINEAR TRANSFORMATIONS 
and  
trtr
∗
=AA 
 
19.4
 If 
()
,;∈ABLVV, where 
V
 is an inner product space, define 
 
tr
∗
⋅=ABAB 
 
Show that this definition makes 
()
;LVV into an inner product space. 
19.5
 In the special case when V is a real inner product space, show that the inner product of 
Exercise 19.4 implies that
()()
;;⊥AVVSVV. 
19.6
 Verify formulas (19.16)-(19.18). 
19.7
 Use the results of Exercise 14.6 along with (19.14) to derive the transformation rules 
(19.16)-(19.18) 
19.8
 Given an endomorphism 
()
;∈ALVV
, show that  
 
1
tr
N
k
k
k
A
=
=
∑
A 
 
where
()
qq
kk
A=⋅Aee.  Thus, we can compute the trace of an endomorphism from (19.8) 
or from the formula above and be assured the same result is obtained in both cases.  Of 
course, the formula above is also independent of the basis of
V. 
19.9
 Exercise 19.8 shows that  trAcan be computed from two of the four possible sets of 
components ofA.  Show that the quantities 
1
N
kk
k
A
=
∑
 and 
1
N
kk
k
A
=
∑
 are not equal to each 
other in general and, moreover, do not equal trA.  In addition, show that each of these 
quantities depends upon the choice of basis of 
V. 
19.10
 Show that  
 
()
()
kkk
kkk
AA
AA
ααα
ααα
∗∗
∗∗
=⋅=
=⋅=
Abe
Abe
 
 
and 
 

Sec. 19            •            Component Formulas  123 
()
kkk
AA
ααα
∗∗
=⋅=Abe

 
 

 125 
 
Chapter 5 
 
 
DETERMINANTS AND MATRICES 
 
 
In Chapter 0 we introduced the concept of a matrix and examined certain manipulations one 
can carry out with matrices.  In Section 7 we indicated that the set of 2 by 2 matrices with real 
numbers for its elements forms a ring with respect to the operations of matrix addition and matrix 
multiplication.  In this chapter we shall consider further the concept of a matrix and its relation to a 
linear transformation. 
 
Section 20.   The Generalized Kronecker Deltas and the Summation Convention 
 
Recall that a MN× matrix is an array written in any one of the forms 
 
 
11
111
1
1
1
1
111
11
1
1
,
,
N
N
jj
MM
MMN
N
N
N
jj
N
MMN
MM
AA
AA
AAAA
AA
AA
AA
AA
AAAA
AA
AA
α
α
α
α
⋅⋅⋅
⎡⎤
⋅⋅⋅
⎡⎤
⎢⎥
⎢⎥
⋅
⋅
⎢⎥
⎢⎥
⎢⎥
⎡⎤
⎡⎤
===⋅=
⎢⎥
⋅
⎣⎦
⎣⎦
⎢⎥
⎢⎥
⋅
⋅
⎢⎥
⎢⎥
⎢⎥
⎢⎥
⋅⋅⋅
⋅⋅⋅
⎣⎦
⎣⎦
⎡⎤
⎡⎤
⋅⋅⋅
⋅⋅⋅
⎢⎥
⎢⎥
⋅
⋅
⎢⎥
⎢⎥
⎢⎥
⎢⎥
⎡⎤⎡⎤
====
⋅
⋅
⎣⎦⎣⎦
⎢⎥
⎢⎥
⋅
⋅
⎢⎥
⎢⎥
⎢⎥
⎢⎥
⋅⋅⋅
⋅⋅⋅
⎣⎦
⎣⎦
          (20.1)          
 
Throughout this section the placement of the indices is of no consequence.  The components of the 
matrix are allowed to be complex numbers.  The set of 
MN×
 matrices shall be denoted by
MN×
M
.  
It is an elementary exercise to show that the rules of addition and scalar multiplication insure that 
MN×
M is a vector space.  The reader will recall that in Section 8 we used the set 
MN×
M as one of 
several examples of a vector space.  We leave as an exercise to the reader the fact that the 
dimension of 
MN×
M isMN. 
 
In order to give a definition of a determinant of a square matrix, we need to define a 
permutation and consider certain of its properties.  Consider a set of 
K elements
{}
1
,...,
K
αα.  A 
permutation is a one-to-one function from 
{}
1
,...,
K
αα to
{}
1
,...,
K
αα.  If σ is a permutation, it is 
customary to write 

126 Chap 5 • DETERMINANTS AND MATRICES 
 
 
12
12
()   ( )( )
K
K
ααα
σ
σα  σασα
⋅⋅⋅
⎛⎞
=
⎜⎟
⋅⋅⋅
⎝⎠
                                            (20.2)                                            
 
It is a known result in algebra that permutations can be classified into even and odd ones:  The 
permutation 
σ in (20.2) is even if an even number of pairwise interchanges of the bottom row is 
required to order the bottom row exactly like the top row, and σ is odd if  that number is odd.  For 
a given permutation σ, the number of  pairwise interchanges required to order the bottom row the 
same as  the top row is not unique, but it is proved in algebra that those numbers are either all even 
or all odd.  Therefore the definition for
σ to be even or odd is meaningful.  For example, the 
permutation 
 
 
123
213
σ
⎛⎞
=
⎜⎟
⎝⎠
 
 
is odd, while the permutation 
 
 
123
231
σ
⎛⎞
=
⎜⎟
⎝⎠
 
 
is even. 
 
            The            
parity of σ, denoted by 
σ
ε, is defined by 
 
 
1 if     is an even permutation
-1 if      is an odd permutation
σ
σ
ε
σ
+
⎧
⎪
=
⎨
⎪
⎩
 
 
All of the applications of permutations we shall have are for permutations defined on 
 ()KK N≤ 
positive integers selected from the set of N positive integers
{}
1, 2, 3,...,N
.  Let 
{}
1
,...,
K
ii
 and 
{}
1
,...,
K
jj be two subsets of 
{}
1, 2, 3,...,N.  If we order these two subsets and construct the two K-
tuples 
()
1
,...,
K
ii and
()
1
,...,
K
jj, we can define the generalized Kronecker delta as follows:  The 
generalized Kronecker delta, denoted by 
 
 
12
12
...
...
K
K
iii
jj    j
δ
 
 
is defined by 
 

Sec. 20 • Generalized Kronecker Delta 127 
 
()()
()( )
{}{ }
12
12
11
11
11
...
...
0 if the integers ,..., or ,..., are not distinct
0 if the integers ,..., and ,..., are distinct
   but the sets ,..., and ,..., are not equal
 if the intege
K
K
KK
KK
KK
ii    i
jj    j
iij j
iij j
iij j
σ
δ
ε
=
()( )
{}{ }
11
11
12
12
rs ,..., and ,..., are distinct
     and the sets ,..., and ,..., are equal, where
    
KK
KK
K
K
iij j
iij j
iii
jjj
σ
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⋅⋅⋅
⎪
⎛⎞
=
⎜⎟
⎪
⋅⋅⋅
⎝⎠
⎩
 
 
It follows from this definition that the generalized Kronecker delta is zero whenever the 
superscripts are not the same set of integers as the subscripts, or when the superscripts are not 
distinct, or when the subscripts are not distinct.  Naturally, when 
1K=
 the generalized Kronecker 
delta reduces to the usual one.  As an example, 
12
12
ii
jj
δhas the values 
 
 
1212131311
1221122112
1,     1,     0,     0,     0,     etc.δδ δδδ==−=== 
 
It can be shown that there are 
!!()!NK  N  K− nonzero generalized Kronecker deltas for given 
positive integers
KandN. 
 
An 
ε symbol is one of a pair of quantities 
 
 
1212
1212
......
12...
12.........
or
NN
NN
iiiii    i
N
Njjjjjj
εδε δ==                                      (20.3)                                      
 
For example, take 
3N=;  then 
 
 
123312231
132321213
112221222233
1
1
0,etc.
εεε
εεε
εεεε
===
===−
====
 
 
As an exercise, the reader is asked to confirm that 
 
 
11
11
......
......
NN
NN
iiii
jjjj
εε  δ=                                                            (20.4)                                                            
 
 An identity involving the quantity 
ijq
lms
δ is 
 
 
1
(2)
N
ijqij
lmqlm
q
Nδδ
=
=−
∑
                                                           (20.5)                                                           
 

128 Chap 5 • DETERMINANTS AND MATRICES 
To establish this identity, expand the left side of (20.5) in the form 
 
 
1
1
1
N
ijqijijN
lmqlmlmN
q
δδδ
=
=   +⋅⋅⋅+
∑
                                                        (20.6)                                                        
 
Clearly we need to verify (20.5) for the cases 
ij≠and 
{}{}
,,ijlm=
 only, since in the remaining 
cases (20.5) reduces to the trivial equation 
00=.  In the nontrivial cases, exactly two terms on the 
right-hand side of (20.6) are equal to zero:  one for 
iq= and one for jq=.  For ,,ijl, and mnot 
equal to 
q, 
ijq
lmq
δ
 has the same value as 
ij
lm
δ.  Therefore, 
 
 
1
(2)
N
ijqij
lmqlm
q
Nδδ
=
=−
∑
 
 
which is the desired result (20.5).  By the same procedure used above, it is clear that 
 
 
1
(1)
N
iji
ljl
j
Nδδ
=
=−
∑
                                                             (20.7)                                                             
 
Combining (20.5)and (20.7), we have 
 
 
11
(2)(1)
NN
ijqi
ljqj
jq
NNδδ
==
=− −
∑∑
                                                    (20.8)                                                    
 
Since 
1
N
i
i
j
Nδ
=
=
∑
, we have from (20.8) 
 
 
111
!
(2)(1)()
(3)!
NNN
ijq
ijq
ijq
N
NNN
N
δ
===
=− −  =
−
∑∑∑
                                       (20.9)                                       
 
Equation (20.9) is a special case of 
 
 
12
12
12
...
...
,   ,...,1
!
()!
K
K
K
N
iii
iii
iii
N
NK
δ
=
=
−
∑
                                                      (20.10)                                                      
 
Several other numerical relationships are 
 
 
111
111
12
.........
.........
,,...,1
()!
()!
RRKR
RRKR
RRK
N
iiiiii
jji   ijj
iii
NR
NK
δδ
+
+
++
=
−
=
−
∑
                                            (20.11)                                            
 
 
11
1
111
1
......
...
.........
,...,1
()!
KKN
K
KKNK
KN
N
iiii
ii
jji   ijj
ii
NKεεδ
+
+
+
=
=−
∑
                                      (20.12)                                      

Sec. 20 • Generalized Kronecker Delta 129 
 
 
1111 1
1
1
...............
...
,...,1
()!
KKNK NKK N
KN
KN
N
iiii    jjiijj
ii
ii
NKεδε
+++
+
+
=
=−
∑
                                     (20.13)                                     
 
 
11111
11    111
1
...............
...............
,...,1
()!
KKR  K RKKR
KKRKRKKR
KR
N
iiii  j jiiii
jjjj l    ljjl    l
jj
RKδδδ
+++
+++
+
=
=−
∑
                                    (20.14)                                    
 
 
1111
11    11
11
............
............
,...,1,...,1
()!
()!
()!
KKR  K RK
KKRKRK
KRKR
NN
iiiijjii
jjjj i   ijj
jjii
NK
RK
NR
δδδ
++
++
++
==
−
=−
−
∑∑
                          (20.15)                          
 
It is possible to simplify the formal appearance of many  of our equations if we adopt a 
summation convention:  We automatically sum every repeated index without writing the 
summation sign.  For example, (20.5) is written 
 
 
(2)
ijqij
lmqlm
Nδδ=−                                                            (20.16)                                                            
 
The occurrence of the subscript 
q and the superscript q implies the summation indicated in (20.5).  
We shall try to arrange things so that we always sum a superscript on a subscript.  It is important to 
know the range of a given summation, so we shall use the summation convention only when the 
range of summation is understood.  Also, observe that the repeated indices are 
dummy indices in 
the sense that it is unimportant which symbol is used for them.  For example, 
 
 
ijsijtijq
lmslmtlmq
δδδ==
 
 
Naturally there is no meaning to the occurrence of the same index more than twice in a given term.  
Other than the summation or dummy indices, many equations have 
free indices whose values in the 
given range 
{}
1,...,N are arbitrary.  For example, the indices ,,jkl and 
m
are free indices in 
(20.16).  Notice that every term in a given equation must have the same free indices; otherwise, the 
equation is meaningless. 
 
The summation convention will be adopted in the remaining portion of this text.  If we feel 
it might be confusing in some context, summations will be indicated by the usual summation sign. 
 
 
Exercises 
 
20.1     Verify equation (20.4). 
20.2     Prove     that     
dim
MN
MN
×
=M. 
 

130 Chap 5 • DETERMINANTS AND MATRICES 
 
Section 21.   Determinants 
 
In this section we shall use the generalized Kronecker deltas and the
εsymbols to define the 
determinant of a square matrix. 
 
The determinant of the 
NN× matrix 
ij
AA
⎡⎤
=
⎣⎦
, written detA, is a complex number 
defined by 
 
 
1
12
11121
21222
31
...
12
12
det
N
N
N
N
ii
iiiN
NNNN
AAA
AAA
A
AAAA
AAA
ε
⋅⋅⋅
⋅⋅⋅
=  ⋅=⋅⋅⋅
⋅
⋅
⋅⋅⋅
                              (21.1)                              
 
where all summations are from 1 to 
N.  If the elements of the matrix are written 
ij
A
⎡⎤
⎣⎦
, its 
determinant is defined to be 
 
 
12
1
11121
21222
12
...
12
det
N
N
N
N
iN
ii
ii
NNNN
AAA
AAA
AAAA
AAA
ε
⋅⋅⋅
⋅⋅⋅
⋅
==⋅⋅⋅
⋅
⋅
⋅⋅⋅
                             (21.2)                             
 
Likewise, if the elements of the matrix are written 
i
j
A
⎡⎤
⎣⎦
 its determinant is defined by 
 
 
12
1
111
12
222
12
...12
12
det
N
N
N
N
i
ii
iiN
NNN
N
AAA
AAA
AAAA
AAA
ε
⋅⋅⋅
⋅⋅⋅
⋅
==⋅⋅⋅
⋅
⋅
⋅⋅⋅
                             (21.3)                             
 

Sec. 21 • Determinants  131 
A similar formula holds when the matrix is written 
j
i
AA
⎡⎤
=
⎣⎦
.  The generalized Kronecker delta 
can be written as the determinant of ordinary Kronecker deltas.  For example, one can show, using 
(21.3), that 
 
 
ii
ijijij
kl
klk    llk
jj
kl
δδ
δδδ  δδ
δδ
==−
                                                     (21.4)                                                     
 
Equation (21.4) is a special case of the general result 
 
 
111
12
222
12
1
1
1
...
...
K
K
K
K
KK
K
iii
jjj
iii
jjj
ii
jj
ii
jj
δδδ
δδδ
δ
δδ
⋅⋅⋅
⋅⋅⋅
⋅
=
⋅
⋅
⋅ ⋅⋅⋅
                                                (21.5)                                                
 
It is possible to use (21.1)-(21.3) to show that 
 
 
1
111
...
...
det
N
NNN
ii
jjijij
AAAεε=⋅⋅⋅                                                   (21.6)                                                   
 
 
1
11
1
...
...
det
NNN
N
jjij
ij
ii
AAAεε=⋅⋅⋅                                                   (21.7)                                                   
and 
 
 
1
111
......
det
N
NNN
i
i
jjiijj
AAAεε=⋅⋅⋅
                                                   (21.8)                                                   
 
It is possible to use (21.6)-(21.8) and (20.10) with 
NK= to show that 
 
 
11
11
......
1
det
!
NN
NN
jj ii
iji j
AAA
N
εε=⋅⋅⋅                                                 (21.9)                                                 
 
 
11
11
......
1
det
!
NN
NN
ij
ij
jj ii
AAA
N
εε=⋅⋅⋅                                                (21.10)                                                
 
and 
 
 
1
1
11
...
...
1
det
!
NN
NN
jji
i
iijj
AAA
N
δ=⋅⋅⋅
                                                   (21.11)                                                   
 
Equations (21.9), (21.10) and (21.11) confirm the well-known result that a matrix and its transpose 
have the same determinant.  The reader is cautioned that the transpose mentioned here is that of the 

132 Chap 5 • DETERMINANTS AND MATRICES 
matrix and not of a linear transformation.  By use of this fact it follows that (21.1)-(21.3) could 
have been written 
 
 
1
1
...
1
det
N
N
ii
iNi
AAAε=⋅⋅⋅                                                       (21.12)                                                       
 
 
1
1
1
...
det
N
N
Ni
i
ii
AAAε=⋅⋅⋅
                                                       (21.13)                                                       
 
and 
 
 
1
1
...
1
det
N
N
ii
N
ii
AAAε=⋅⋅⋅                                                       (21.14)                                                       
 
A similar logic also yields 
 
 
1
111
...
...
det
N
NNN
ii
jjjiji
AAAεε=⋅⋅⋅                                                  (21.15)                                                  
 
 
1
11
1
...
...
det
NNN
N
jjji
ji
ii
AAAεε=⋅⋅⋅
                                                  (21.16)                                                  
and 
 
 
11
1
1
......
det
NNN
N
jjiij
j
ii
AAAεε=⋅⋅⋅                                                 (21.17)                                                 
 
The cofactor of the element 
s
t
A
 in the matrix 
i
j
AA
⎡⎤
=
⎣⎦
 is defined by 
 
 
11
1
12
...111
cof
tttN
N
iiii
i
s
tiiits tN
AAAAAεδ
−+
−+
=⋅⋅⋅⋅⋅⋅                                       (21.18)                                       
 
As an illustration of the application of (21.18), let 
3N= and 1st==.  Then 
 
 
1
1123
123
2332
2323
cof
ij  k
ijk
jk
jk
AAA
AA
AA    AA
εδ
ε
=
=
=−
 
 
Theorem 21.1. 
 
 
11
cofdetand   cofdet
NN
sstqsq
qtqtts
st
AAAAAAδδ
==
==
∑∑
                          (21.19)                          
 
Proof.  It follows from (21.18) that 
 

Sec. 21 • Determinants  133 
 
11
1
12
11
1
12
11
1
12
...111
1
...111
...111
cof
det
ttt N
N
tttN
N
tttN
N
N
iiii
i
sss
qtiiitqst   N
s
iiii
i
ii    itqtN
iiii
i
t
ii    itttN   q
t
q
AAAAAA  A
AAAA    A
AAAA    A
A
εδ
ε
εδ
δ
−+
−+
−+
−+
=
−+
−+
=⋅⋅⋅⋅⋅⋅
=⋅⋅⋅⋅⋅⋅
=⋅⋅⋅⋅⋅⋅
=
∑
 
 
where the definition (21.3) has been used.  Equation (21.19)
2
 follows by a similar argument. 
 
Equations (21.19) represent the classical Laplace expansion of a determinant.  In Chapter 0 
we promised to present a  proof for (0.29) for matrices of arbitrary order.  Equations (21.19) fulfill 
this promise. 
 
 
Exercises 
 
21.1     Verify equation (21.4). 
21.2     Verify equations (21.6)-(21.8). 
21.3     Verify equationa (21.9)-(21.11). 
21.4     Show that the cofactor of an element of an NN
× matrix A can be written 
 
 
1
12
112
...
...
1
cof
(1)!
NN
NN
jji
ii
jiijj
AAA
N
δ=⋅⋅⋅
−
                                            (21.20)                                            
 
21.5     If     A and 
B are NN× matrices use (21.8) to prove that 
 
 
detdet    detABAB=
 
 
21.6     If     
I is the 
NN×
 identity matrix show that det1I=. 
21.7     If     
A is a nonsingular matrix show, that det0A≠ and that 
 
 
1
1
det
det
A
A
−
= 
 
21.8     If     A is an 
NN× matrix, we define its KK× minor  (1)KN≤≤ to  be the determinant of 
any KK
× submatrix of 
A
, e.g., 
 

134 Chap 5 • DETERMINANTS AND MATRICES 
 
11
1
111
111
1
11
11
......
......
...
...
det
K
KKK
KKK
KK
K
KK
KK
ii
jj
iikk   ii
jjjjkk
ii
jj
iikk
kkjj
AA
AAA
AA
AA
δ
δ
⎡⎤
⋅⋅⋅
⎢⎥
⋅
⎢⎥
⎢⎥
⋅
≡=⋅⋅⋅
⎢⎥
⋅
⎢⎥
⎢⎥
⋅⋅⋅
⎣⎦
=⋅⋅⋅
                             (21.21)                             
 
In particular, any element 
i
j
A
 of A is a 11×minor of A.  The cofactor of the KK× minor 
1
1
...
...
K
K
ii
jj
A is defined by 
 
 
1
11
111
...
...
.....
1
cof
()!
NN
KK
KNKN
jji
iii
jjiijj
AAA
NK
δ
+
+
≡⋅⋅⋅
−
                                        (21.22)                                        
 
 
which is equal to the 
()()NK  NK−×−
 minor complementary to the KK× minor 
1
1
...
...
K
K
ii
jj
A 
and is assigned an appropriate sign.  Specifically, if 
1
(,...,    )
KN
ii
+
 and 
1
(,...,)
KN
jj
+
 are 
complementary sets of 
1
(  ,...,    )
K
ii and 
1
(   ,...,)
K
jj in  (1,...,    )N, then 
 
 
11
1
111
......
...
.........
cof(no summation)
NK N
K
KNKN
iiii
ii
jjjj   jj
AAδ
+
+
=
                                     (21.23)                                     
  
Clearly (21.22) generalizes (21.20).  Show that the formulas that generalize (21.19) to 
KK× minors in general are 
 
 
111
111
1
111
111
1
.........
.........
,...,1
.........
.........
,...,1
1
detcof
!
1
detcof
!
KKK
KKK
K
KKK
KKK
K
N
iiiij j
j jkkkk
kk
N
j jkkkk
iiiij j
kk
AAA
K
AAA
K
δ
δ
=
=
=
=
∑
∑
                                          (21.24)                                          
  
21.9
 If A is nonsingular, show that 
 
 
11
11
......
1
......
(det   ) cof()
KK
KK
iij j
jjii
AAA
−
=                                                   (21.25)                                                   
 
and that 
 
 
111
111
1
.........
1
.........
,...,1
1
()
!
KKK
KKK
K
N
iikkii
kkjjjj
kk
AA
K
δ
−
=
=
∑
                                               (21.26)                                               

Sec. 21 • Determinants  135 
 
In particular, if 1
K= and A is replaced by 
1
A
−
 then (21.25) reduces to 
 
 
11
() (det)cof
ij
ji
AAA
−−
=                                                      (21.27)                                                      
 
 
which is equation (0.30). 

136 Chap 5 • DETERMINANTS AND MATRICES 
 
Section 22.   The Matrix of a Linear Transformation 
 
In this section we shall introduce the matrix of a linear transformation with respect to a 
basis and investigate certain of its properties.  The formulas of Chapter 0 and Section 21 are purely  
numerical formulas independent of abstract vectors or linear transformations.  Here we show how a 
certain matrix can be associated with a linear transformation, and, more importantly, we show to 
what extent the relationship is basis dependent. 
 
If            (    ;   )
∈ALVU, 
{}
1
,...,
N
ee
is a basis for V, and 
{}
1
,...,
M
bb
 is a basis for U, then we can 
characterize 
A by the system (19.1): 
 
 
kk
A
α
α
=Aeb
                                                                (22.1)                                                                
 
where the summation is in force with the Greek indices ranging from 1 to 
M. The matrix of Awith 
respect to the bases 
{}
1
,...,
N
ee
and 
{}
1
,...,
M
bb
, denoted by (   ,    ,)
k
M
α
Ae  b is 
 
 
111
12
222
12
12
(, ,  )
N
N
k
MMM
N
AAA
AAA
M
AAA
α
⎡⎤
⋅⋅⋅
⎢⎥
⋅⋅⋅
⎢⎥
⎢⎥
⋅
=
⎢⎥
⋅
⎢⎥
⎢⎥
⋅
⎢⎥
⋅⋅⋅
⎢⎥
⎣⎦
Ae  b
                                      (22.2)                                      
 
As the above argument indicates, the matrix of 
A
depends upon the choice of basis for Vand U.  
However, unless this point needs to be stressed, we shall often write (   )
MA for the matrix of A 
and the basis dependence is understood.  We can always regard 
M as a function 
:(;)
MN
M
×
→LUVM.  It is a simple exercise to confirm that 
 
                                          ()()()
MMMλμλ   μ+=+ABAB                                                (22.3)                                                
 
for all    ,
λμ∈C and     ,(   ;    )∈ABLUV.  Thus M is a linear transformation.  Since 
 
 
() 0M=A 
 
implies 
=A0, M is one-to-one.  Since (; )LUV and 
MN×
M have the same dimension (see 
Theorem 16.1: and Exercise 20.2), Theorem 15.10 tells us that 
Mis an automorphism and, thus, 
(; )
LUV and 
MN×
M are isomorphic.  The dependence of (   ,    ,)
k
M
α
Ae  b on the bases can be 
exhibited by use of the transformation rule (19.5).  In matrix form (19.5) is 
 
 
1
ˆ
ˆ
(, ,  )(, ,  )
kk
MSMT
αα
−
=Ae  bAe  b                                                (22.4)                                                

Sec. 22 • Matrix of a Linear Transformation 137 
 
where 
S is the MM×matrix 
 
 
111
12
222
12
12
M
M
MMM
M
SSS
SSS
S
SSS
⎡⎤
⋅⋅⋅
⎢⎥
⋅⋅⋅
⎢⎥
⎢⎥
⋅
=
⎢⎥
⋅
⎢⎥
⎢⎥
⋅
⎢⎥
⋅⋅⋅
⎢⎥
⎣⎦
                                                  (22.5)                                                  
 
and 
T is the NN× matrix 
 
 
111
12
222
12
12
N
N
NNN
N
TTT
TTT
T
TTT
⎡⎤
⋅⋅⋅
⎢⎥
⋅⋅⋅
⎢⎥
⎢⎥
⋅
=
⎢⎥
⋅
⎢⎥
⎢⎥
⋅
⎢⎥
⋅⋅⋅
⎢⎥
⎣⎦
                                                  (22.6)                                                  
 
Of course, in constructing (22.4) from (19.5) we have used the fact expressed by (14.20) and 
(14.21) that the matrix 
1
T
−
 has components 
ˆ
j
k
T,    ,1,...jkN=. 
 
If A is an endomorphism, the transformation formula (22.4) becomes 
 
 
1
ˆˆ
(, , )(, , )
kqkq
MTMT
−
=Ae  eAe  e
                                                 (22.7)                                                 
 
We shall use (22.7) to motivate the concept of the 
determinant of an endomorphism.  The 
determinant of 
(, , )
kq
MAe  e
, written 
det(   ,    ,    )
kq
MAe  e
, can be computed by (21.3).  It follows from  
(22.7) and Exercises 21.5 and 21.7 that 
 
 
1
ˆˆ
det(   ,    ,    )(det   )(det(   ,    ,    ))(det)
ˆˆ
det(   ,    ,    )
kqkq
kq
MTMT
M
−
=
=
Ae  eAe  e
Ae  e
 
 
Thus, we obtain the important result that  det(   ,    ,    )
kq
MAe  e is independent of the choice of basis for 
V
.  With this fact we define the determinant of an endomorphism (   ;    )∈ALVV, written detA, by 
 
 
detdet(   ,    ,    )
kq
M=AAee
                                                        (22.8)                                                        
 

138 Chap 5 • DETERMINANTS AND MATRICES 
By the above argument, we are assured that 
detA is a property of 
A
 alone.  In Chapter 8, we shall 
introduce directly a definition for the determinant of an endomorphism without use of a basis. 
 
Given a linear transformation (   ;   )
∈ALVU, the adjoint 
*
(; )∈ALUV* is defined by the 
component formula (19.9)  Consistent with equations (22.1) and (22.2), equation (19.9) implies that  
 
 
*1* 2*
111
*1* 2*
222
*
*1* 2*
(,,)
M
M
k
M
NNN
AAA
AAA
M
AAA
α
⎡⎤
⋅⋅⋅
⎢⎥
⋅⋅⋅
⎢⎥
⎢⎥
⋅
=
⎢⎥
⋅
⎢⎥
⎢⎥
⋅
⎢⎥
⋅⋅⋅
⎢⎥
⎣⎦
Ab e
                                    (22.9)                                    
 
Notice that the matrix of 
*
A is referred to the reciprocal bases 
{}
k
eand 
{}
α
b.  If we now use 
(19.10) and the definition (22.2) we see that 
 
 
*
(,,)    (,,)
kT
k
MM
α
α
=Ab eAeb                                                 (22.10)                                                 
 
where the complex conjugate of a matrix is the matrix formed by taking the complex conjugate of 
each component of the given matrix.  Note that in (22.10) we have used the definition of the 
transpose of a matrix in Chapter 0.  Equation (22.10) gives a simple comparison of the component 
matrices of a linear transformation and its adjoint.  If the vector spaces are real, (22.10) reduces to 
 
                                                                 ()   ()
TT
MM=AA                                                            (22.11)                                                            
 
where the basis dependence is understood.  For an endomorphism (   ;    )
∈ALVV, we can use 
(22.10) and (22.8) to show that 
 
 
*
detdet=AA                                                              (22.12)                                                              
 
Given a 
MN×
 matrix 
k
AA
α
⎡⎤
=
⎣⎦
, there correspond 
M1N× row matrices and 
N
1M× 
column matrices.  The 
row rank of the matrix A is equal to the number of linearly independent 
row matrices and the 
column rank of A is equal to the number of linearly independent column 
matrices.  It is a property of matrices, which we shall prove, that the row rank 
equals the column 
rank.  This common rank in turn is equal to the rank of the linear transformation whose matrix is 
A.  The theorem we shall prove is the following. 
 
Theorem 22.1.  
k
AA
α
⎡⎤
=
⎣⎦
 is an MN× matrix, the row rank of A equals the column rank of A. 
 

Sec. 22 • Matrix of a Linear Transformation 139 
Proof.  Let (    ;   )∈ALVU and let ()
k
MAA
α
⎡⎤
==
⎣⎦
A with respect to bases 
{}
1
,...,
N
ee
 for Vand 
{}
1
,...,
M
bb for 
U
.  We can define a linear transformation 
:
M
→BUC
 by 
 
 
12
(   ,,...,)
M
uuu=Bu 
 
where 
u
α
α
=ub.  Observe that B is an isomorphism.  The product BA is a linear transformation 
M
→VC
;  further, 
 
 
()
12
(),,...,
M
kk  kkkk
AA   AAA
αα
αα
===BAeBbBb 
 
Therefore 
k
BAe is an M-tuple whose elements are those of the kth column matrix of A.  This 
means  dim   ()
RBA= column rank of A.  Since Bis an isomorphism, BA and A have the same 
rank and thus column rank of dim   (   )
AR=A.  A similar argument applied to the adjoint mapping 
*
A shows that row rank of 
*
dim   ()AR=A.  If we now apply Theorem 18.4 we find the desired 
result. 
 
In what follows we shall only refer to the rank of a matrix rather than to the row or the 
column rank. 
 
Theorem 22.2.  An endomorphism 
A
 is regular if and only if det0≠A. 
 
Proof.  If  det0≠A, equations (21.19) or, equivalently, equation (0.30), provide us with a formula 
for the direct calculation of 
1−
A so A is regular.  If A is regular, then 
1−
A exists, and the results 
of Exercises 22.3 and 21.7 show that 
det0≠A. 
 
Before closing this section, we mention here that among the four component matrices 
k
A
α
⎡⎤
⎣⎦
, 
[]
k
A
α
, 
k
A
α
⎡⎤
⎣⎦
, and 
k
A
α
⎡⎤
⎣⎦
 defined by (19.2), (19.16), (19.17), and (19.18), respectively, the 
first one, 
k
A
α
⎡⎤
⎣⎦
, is most frequently used.  For example, it is that particular component matrix of an 
endomorphism A that we used to define the determinant of A, as shown in (22.8).  In the sequel, 
if we refer to the component matrix of a transformation or an endomorphism, then unless otherwise 
specified, we mean the component matrix of the first kind. 
 
 
Exercises 
 
22.1
 If A and B are endomorphisms, show that 
 
 
()(   )(   )anddetdetdetMMM==ABABABAB 
 
22.2
 If :→AVV is a skew-Hermitian endomorphism, show that 
 

140 Chap 5 • DETERMINANTS AND MATRICES 
 
dim
det(  1)det=−AA
V
 
 
22.3
 If            (   ;    )∈ALVV is regular, show that 
 
 
11
()  ()MM
−−
=AA 
 
22.4
 If           (   ;    )∈PLVV is a projection, show that there exists a basis for Vsuch that 
 
 
1
1
dim   (   )
0
1
()
0
0
0
0
RR
M
NR
⎡⎤
⎫
⎢⎥
⎪
⎢⎥
⎪
⋅
⎢⎥
⎪
=
⎢⎥
⎬
⋅
⎢⎥
⎪
⎢⎥
⎪
⋅
⎢⎥
⎪
⎭
⎢⎥
=
⎢⎥
⎫
⎢⎥
⎪
⎢⎥
⎪
⎢⎥
⋅
⎪
⎢⎥
−
⎬
⋅
⎢⎥
⎪
⎢⎥
⎪
⋅
⎢⎥
⎪
⎢⎥
⎣⎦
⎭
P
P 
 
22.5
 If            (   ;    )∈ALVVis Hermitian, show that the component version of this fact is 
 
 
,,and
iiii
ijjijjikjjik
AAAAeAeA===
 
 
where 
ikik
e=⋅ee, and where 
{}
1
,...,
N
ee
 is the basis used to express the transformation 
formulas (19.16)-(19,18) in matrix notation. 
22.6
 If            (   ;    )∈ALVV we have seen it has four sets of components which are denoted by 
j
i
A
, 
i
j
A, 
ij
A, and 
ij
A.  Prove that  det
ij
A
⎡⎤
⎣⎦
 and  det
ij
A
⎡⎤
⎣⎦
 do depend upon the choice of basis for 
V. 
22.7
 Show that 
 
                                                                det()det
N
αα=AA                                                          (22.13)                                                          
 
for all scalars 
α.  In particular, if =AI, then from Exercise 21.6 
 
                                                                     det()
N
αα=I                                                               (22.14)                                                               
 

Sec. 22 • Matrix of a Linear Transformation 141 
22.8     If     
A is an endomorphism of an N-dimensional vector space V, show that  det()λ−AI is a 
polynomial of degree N in 
λ.

142 Chap 5 • DETERMINANTS AND MATRICES 
 
Section 23.   Solution of Systems of Linear Equations 
 
In this section we shall examine the problem system of 
M equations in 
N
 unknowns of 
the form 
 
                                                           ,1,...,,1,...,
k
k
Av   uM    kN
αα
α===                                    (23.1)                                    
 
where the 
MN coefficients 
k
A
α
and the M data u
α
 are given.  If we introduce bases bases 
{}
1
,...,
N
ee for a vector space 
V
and 
{}
1
,...,
M
bb for a vector space 
U
, then (23.1) can be viewed as 
the component formula of a certain vector equation 
 
 
=Avu                                                                     (23.2)                                                                     
 
which immediately yields the following theorem. 
 
Theorem 23.1.  (23.1) has a solution if and only if u is in    (   )RA. 
 
Another immediate implication of (23.2) is the following theorem. 
 
Theorem 23.2.  If (23.1) has a solution, the solution is unique if and only if 
A
 is regular. 
 
Given the system system of equations (23.1), the associated homogeneous system is the set 
of equations 
 
 
0
k
k
Av
α
=
                                                                   (23.3)                                                                   
 
Theorem 23.3.  The set of solutions of the homogeneous system (23.3) whose coefficient matrix is 
of rank 
R form a vector space of dimension NR−. 
 
Proof.  Equation (23.3) can be regarded as the component formula of the vector equation 
 
 
Av = 0
 
 
which implies that 
k
v solves (23.3) if and only if (   )K∈vA.  By (15.6) 
dim    (   )divdim   (   )KRNR=−   =−
AAV. 
 
From Theorems 15.7 and 23.3 we see that if there are fewer equations than unknowns (i.e. 
MN<
), the system (23.3) always has a nonzero solution.  This assertion is clear because 
0NRNM−≥ − >, since 
()RA
 is a subspace of U and dimM=U. 
 
If in (23.1) and (23.3) 
MN=, the system (23.1) has a solution for all 
k
u if and only if 
(23.3) has the trivial solution 
12
0
N
vvv=  =⋅⋅⋅=   = only.  For, in this circumstance, Ais regular 

Sec. 23 • Systems of Linear Equations 143 
and thus invertible.  This means  det0
k
A
α
⎡⎤
≠
⎣⎦
, and we can use (21.19) and write the solution of 
(23.1) in the form 
 
 
1
1
(cof)
det
N
j
j
k
vAu
A
αα
α
α
=
=
⎡⎤
⎣⎦
∑
                                                   (23.4)                                                   
 
which is the classical Cramer's rule and is to N dimension of equation (0.32). 
 
 
Exercise 
 
23.1     Given the system of equations (23.1), the augmented matrix of the system is the matrix 
obtained from 
k
A
α
⎡⎤
⎣⎦
 by the addition of a co1umn formed from 
1
,...,
N
uu.  Use Theorem 
23.1 and prove that the system (23.1) has a solution if and only if the rank of 
k
A
α
⎡⎤
⎣⎦
 equals 
the rank of the augmented matrix.

 
 

 145 
_______________________________________________________________________________ 
Chapter 6 
 
 
SPECTRAL DECOMPOSITIONS 
 
 
In this chapter we consider one of the more advanced topics in the study of linear 
transformations.  Essentially we shall consider the problem of analyzing an endomorphism by 
decomposing it into elementary parts. 
 
Section 24.   Direct Sum of Endomorphisms 
 
If 
A
is an endomorphism of a vector space V, a subspace 
1
V of V is said to be 
A
-
invariant if 
A
 maps 
1
V to 
1
V.  The most obvious example of an 
A
-invariant subspace is the null 
space 
()KA
.  Let 
12
,,...,
L
AAA be endomorphisms of V;  then an endomorphism 
A
 is the direct 
sum of 
12
,,...,
L
AAA if 
 
12L
=++⋅⋅⋅+AA AA                                                    (24.1)                                                    
 
and 
 
 
ij
ij=≠AA0
                                                      (24.2)                                                      
 
For example, equation (17.18) presents a direct sum decomposition for the identity linear 
transformation. 
 
Theorem 24.1.  If (;)∈ALV U and 
12L
=⊕  ⊕⋅⋅⋅⊕VVVV, where each subspace 
j
V is A-
invariant, then 
A has the direct sum decomposition (24.1), where each 
i
A is given by 
 
 
iii
=AvAv                                                            (24.3)
1
 
 
for all 
ii
∈vV and 
 
 
ij
=Av0                                                             (24.3)
2
 
  (24.3) 

146 Chap 6 • SPECTRAL DECOMPOSITIONS 
 
For all 
jj
∈vV, ij≠, for 1,...,iL=.  Thus the restriction of Ato 
j
V coincides with that of 
j
A;  
further, each 
i
V is 
j
A
-invariant, for all 
1,...,jL=
. 
 
Proof.
  Given the decomposition 
12L
=  ⊕  ⊕⋅⋅⋅⊕VVVV, then 
∈vV
 has the unique representation 
1L
=  +⋅⋅⋅+vvv, where each 
ii
∈vV.  By the converse of Theorem 17.4, there exist L projections 
1
,...,
L
PP (19.18) and also ()
jj
∈vPV.  From (24.3), we have 
jj
=AAP, and since 
j
V is A-
invariant, 
jj
=APP A.  Therefore, if ij≠, 
 
 
iji   jij
===A AAP APAAP P0
 
 
where (17.17)
2
 has .been used.  Also 
 
 
121   2
12
()
LL
L
+   +⋅⋅⋅+   =   +   +⋅⋅⋅+
=++⋅⋅⋅+
=
AAA APAPAP
AP   PP
A
 
 
where (17.18) has been used. 
 
When the assumptions of the preceding theorem are satisfied, the endomorphism Ais said 
to be 
reduced by the subspaces 
1
,...,
L
VV.  An important result of this circumstance is contained in 
the following theorem. 
 
Theorem 24.2.  Under the conditions of Theorem 24.1, the determinant of the endomorphism A is 
given by 
 
12
detdetdetdet
L
=⋅⋅⋅AAA  A                                              (24.4)                                              
 
where 
k
A denotes the restriction of Ato 
k
V for all 1,...,kL=. 
 
The proof of this theorem is left as an exercise to the reader. 
 
 
Exercises 
 
24.1     Assume     that     
(; )∈ALVV
 is reduced by 
1
,...,
L
VV and select for the basis of V the union 
of some basis for the subspaces 
1
,...,
L
VV.  Show that with respect to this basis 
()MA
has 
the following 
block form: 

Sec. 24 • Direct Sum of Endomorphisms 147 
 
 
 
1
2
0
()
0
L
A
A
M
A
⎡⎤
⎢⎥
⎢⎥
=⋅
⎢⎥
⎢⎥
⋅
⎢⎥
⎢⎥
⎣⎦
A
                                              (24.5)                                              
 
where 
j
A is the matrix of the restriction of A to 
j
V. 
24.2     Use the result of Exercise 24.1 and prove Theorem 24.2. 
24.3     Show that if 
12L
=  ⊕  ⊕⋅⋅⋅⊕VVVV, then (; )∈ALVV is reduced by 
1
,...,
L
VV if and only if 
jj
=APP A for 1,...,jL=, where 
j
P is the projection on 
j
V given by (17.21). 

148 Chap 6 • SPECTRAL DECOMPOSITIONS 
 
 
Section 25  Eigenvectors and Eigenvalues 
 
Given an endomorphism (   ;    )∈A
LVV, the problem of finding a direct sum decomposition 
of 
A
is closely related to the study of the spectral properties of 
A
.  This concept is central in the 
discussion of 
eigenvalue problems. 
 
A scalar 
λ is an eigenvalue of (   ;    )∈ALVV if there exists a nonzero vector ∈vV such 
that 
 
 
λ=Avv                                                               (25.1)                                                               
 
The vector 
v in (25.1) is called an eigenvector of A corresponding to the eigenvalue λ.  
Eigenvalues and eigenvectors are sometimes refered to as 
latent roots and latent vectors, 
characteristic roots and characteristic vectors, and proper values and proper vectors.  The set of 
all eigenvalues of Ais the spectrum of A, denoted by    (   )
σA.  For any (   )λσ∈A, the set 
 
 
{}
()λλ=∈   =vAvvVV 
 
is a subspace of 
V
, called the eigenspace or characteristic subspace corresponding to λ.  The 
geometric multiplicity of 
λ is the dimension of ()λV. 
 
Theorem 25.1.  Given any 
()λσ∈A, the corresponding eigenspace ()λV is A-invariant. 
 
The proof of this theorem involves an elementary use of (25.1).  Given  any 
()λσ∈A, the 
restriction of 
v to (   )λV, denoted by
()λ
A
V
, has the property that 
 
 
()λ
λ=Au u
V
                                                            (25.2)                                                            
 
for all 
u in 
()λV
.  Geometrically, 
()λ
A
V
 simply amplifies u by the factor λ.  Such linear 
transformations are often called 
dilatations. 
 
Theorem 25.2.  
(; )∈ALVV is not regular if and only if 0 is an eigenvalue of A; further, the 
corresponding eigenspace is 
()KA. 
 

Sec. 25 • Eigenvectors and Eigenvalues 149 
 
The proof is obvious.  Note that (25.1) can be written as  ()
λ−=AIv0, which shows that 
 
                                                                  ()()
Kλλ=−AIV                                                       (25.3)                                                       
 
Equation (25.3) implies that 
λ is an eigenvalue of 
A
 if and only if λ−AI is singular.  Therefore, 
by Theorem 22.2, 
 
                                                               ()   det() 0
λσλ∈⇔−=AAI                                               (25.4)                                               
 
The polynomial    (   )
fλ of degree dimN=V defined by 
 
                                                                 () det()
fλλ=−AI                                                      (25.5)                                                      
 
is called the 
characteristic polynomial of 
A
.  Equation (25.5) shows that the eigenvalues of 
A
are 
roots of the characteristic equation 
 
                                                                         () 0
fλ=                                                               (25.6)                                                               
 
 
Exercises 
25.1     Let     A be an endomorphism whose matrix (   )MA relative to a particular basis is a 
triangular matrix, say 
 
 
111
12
2
2
()
0
N
N
N
AAA
A
M
A
⎡⎤
⋅⋅⋅
⎢⎥
⎢⎥
⎢⎥
⋅⋅
=
⎢⎥
⋅⋅
⎢⎥
⎢⎥
⋅⋅
⎢⎥
⎢⎥
⎣⎦
A
 
 
What is the characteristic polynomial of A?  Use (25.6) and determine the eigenvalues of 
A. 
25.2     Show that the eigenvalues of a Hermitian endomorphism of an inner product space are all 
real numbers and the eigenvalues of a skew-Hermitian endomorphism are all purely 
imaginary numbers (including the number 
00i=). 

150 Chap 6 • SPECTRAL DECOMPOSITIONS 
 
25.3     Show that the eigenvalues of a unitary endomorphism all have absolute value 
1 and the 
eigenvalues of a projection are either 1 or 0. 
25.4     Show that the eigenspaces corresponding to different eigenvalues of a Hermitian 
endomorphism are mutually orthogonal. 
25.5     If     
(; )∈BLV V is regular, show that 
1−
BAB and A have the same characteristic equation.  
How are the eigenvectors of 
1−
BAB related to those of A? 

Sec. 26 • The Characteristic Polynomial 151 
 
 
Section 26    The Characteristic Polynomial 
 
In the last section we found that the eigenvalues of 
(; )∈ALVV are roots of the 
characteristic polynomial 
 
 
() 0fλ=                                                               (26.1)                                                               
 
where 
 
 
() det()fλλ=−AI                                                      (26.2)                                                      
 
If 
{}
12
,...,ee
 is a basis for V, then by (19.6) 
 
 
j
kkj
A=Aee                                                            (26.3)                                                            
 
Therefore, by (26.2) and (21.11), 
 
 
1
11
111
...
...
1
()()  ()
!
NNN
NNN
jjii
ii
iijjjj
fAA
N
λδλδ λδ=−⋅⋅⋅−                                 (26.4)                                 
 
If (26.4) is expanded and we use (20.11), the result is 
 
 
1
11
()()()()
NN
NN
fλλμλμλμ
−
−
=−  +  −   +⋅⋅⋅+   − +
                                (26.5)                                
 
where 
 
 
1
1
11
,...,
,...,
1
!
jj
jj
qqi
i
jiiqq
AA
j
μδ=⋅⋅⋅                                                 (26.6)                                                 
 
Since    (   )
fλ is defined by (26.2), the coefficients 
j
μ,       1,...,jN=, are independent of the choice of 
basis for 
V.  These coefficients are called the fundamental invariants of 
A
.  Equation (26.6) 
specializes to 
 

152 Chap 6 • SPECTRAL DECOMPOSITIONS 
 
 
{}
22
12
1
tr    ,(tr    )tr,anddet
2
N
μμμ==−=AAAA                   (26.7)                   
 
where (21.4) has been used to obtain (26.7)
2
.  Since    (   )fλ is a Nth degree polynomial, it can be 
factored into the form 
 
 
12
12
() ()()()
L
ddd
L
fλλλ λλ   λλ=   −−   ⋅⋅⋅   −
                                      (26.8)                                      
 
where 
1
,...,
L
λλ are the distinct foots of    (   )0fλ= and 
1
,...,
L
dd are positive integers which must 
satisfy 
1
L
j
j
dN
=
=
∑
.  It is in writing (26.8) that we have made use of the assumption that the scalar 
field is complex.  If the scalar field is real, the polynomial (26.5), generally, cannot be factored. 
 
In general, a scalar field is said to be 
algebraically closed if every polynomial equation has 
at least one root in the field, or equivalently, if every polynomial, such as 
()fλ, can be factored 
into the form (26.8).  It is a theorem in algebra that the complex field is algebraically closed.  The 
real field, however, is not algebraically closed.  For example, if 
λ is real the polynomial equation 
2
()1 0fλλ=+= has no real roots.  By allowing the scalar fields to be complex, we are assured 
that every endomorphism 
has at least one eigenvector.  In the expression (26.8), the integer 
j
dis 
called the 
algebraic multiplicity of the eigenvalue 
j
λ
.  It is possible to prove that the algebraic 
multiplicity of an eigenvalue is not less than the geometric multiplicity of the same eigenvalue.  
However, we shall postpone the proof of this result until Section 30. 
 
An expression for the invariant 
j
μ can be obtained in terms of the eigenvalues if (26.8) is 
expanded and the results are compared to (26.5).  For example, 
 
 
12
11122
12
tr
det
L
LL
ddd
NL
dddμλλ    λ
μλλλ
==++⋅⋅⋅+
== ⋅⋅⋅
A
A
                                           (26.9)                                           
 
The next theorem we want to prove is known as the Cayley-Hamilton theorem.  Roughly 
speaking, this theorem asserts that an endomorphism satisfies its own characteristic equation.  To 
make this statement clear, we need to introduce certain ideas associated with polynomials of 
endomorphisms.  As we have done in several places in the preceding chapters, if (   ;    )∈A
LVV, 
2
A is defined by 
 
 
2
=AAA 

Sec. 26 • The Characteristic Polynomial 153 
 
 
Similarly, we define by induction, starting from 
 
 
0
=AI 
 
and in general 
 
 
11kkk−−
≡=AAA  AA 
 
where 
kis any integer greater than one.  If k and l are positive integers, it is easily established by 
induction that 
 
 
kllklk+
==AA    AAA                                                   (26.10)                                                   
 
Thus 
k
A and 
l
Acommute.  A polynomial in Ais an endomorphism of the form 
 
 
1
011
()
MM
MM
gαααα
−
−
=+ +⋅⋅⋅++AA AAI
                                 (26.11)                                 
 
where 
M is a positive integer and 
0
,...,
M
αα are scalars.  Such polynomials have certain of the 
properties of polynomials of scalars.  For example, if a scalar polynomial 
 
 
1
011
()
MM
MM
gttttαααα
−
−
=+ +⋅⋅⋅++ 
 
can be factored into 
 
 
01   2
()()()   ()
M
gttttαηη  η=−−⋅⋅⋅− 
 
then the polynomial (26.11) can be factored into 
 
 
012
()()()  ()
M
gαηη  η=− −⋅⋅⋅−AAIAIAI                                   (26.12)                                   
 
The order of the factors in (26.12) is not important since, as a result of (26.10), the factors 
commute.  Notice, however, that the product of two nonzero endomorphisms can be zero.  Thus the 
formula 

154 Chap 6 • SPECTRAL DECOMPOSITIONS 
 
 
12
() ()gg=AA0                                                       (26.13)                                                       
 
for two polynomials 
1
g and 
2
g generally does not imply one of the factors is zero.  For example, 
any projection P satisfies the equation -
=P(P   I)0, but generally P and     -PI are both nonzero. 
 
Theorem 26.1.  (Cayley-Hamilton). If    (   )
fλ is the characteristic polynomial (26.5) for an 
endomorphism A, then 
 
 
1
11
()()()()
NN
NN
fμμμ
−
−
=−  +  −   +⋅⋅⋅+   − +  =AAAAI0                         (26.14)                         
 
Proof.  :  The proof which we shall now present makes major use of equation (0.29) or, 
equivalently, (21.19).  If 
adj()λ−AI is the endomorphism whose matrix is adj
pp
qq
Aλδ
⎡⎤
−
⎣⎦
, where 
()
p
q
AM
⎡⎤
=
⎣⎦
A
, then by (26.2) and (0.29) (see also Exercise 40.5) 
 
                                                       (adj())()(   )f
λλλ−−=AIAII                                           (26.15)                                           
 
By (21.18) it follows that  adj()
λ−AI is a polynomial of degree 
1N−
 in 
λ
.  Therefore 
 
 
12
0121
adj()()()()
NN
NN
λλ λλ
−−
−−
−=− +− +⋅⋅⋅+ −+AIBBBB                   (26.16)                   
 
where 
01
,...,
N−
BB are endomorphisms determined by A.  If we now substitute (26.16) and (26.15) 
into (26.14) and require the result to hold for all 
λ, we find 
 
 
0
011
122
211
1
NNN
NN
μ
μ
μ
μ
−−−
−
=
+=
+=
⋅
⋅
⋅
+=
=
BI
BA  BI
BA   BI
BABI
BAI
                                                  (26.17)                                                  
 

Sec. 26 • The Characteristic Polynomial 155 
 
Now we multiply (26.17)
1
 . by  ()
N
−A, (26.17)
2
 by 
1
()
N−
−A, (26.17)
3
 by 
2
()
N−
−A,...,(26.17)
k
 by 
1
()
Nk−+
−A, etc., and add the resulting N equations, to find 
 
 
1
11
()()()
NN
NN
μμμ
−
−
−+− +⋅⋅⋅+ −+=AAAI0                               (26.18)                               
 
which is the desired result. 
 
 
Exercises 
 
26.1     If     
dimN=V is odd, and the scalar field is R, prove that the characteristic equation of any 
(; )∈A
LVV has at least one eigenvalue. 
26.2     If     
dimN=V is odd and the scalar field is R, prove that if  det0(   0)><A, then A has at 
least one positive (negative) eigenvalue. 
26.3     If     
dimN=V is even and the scalar field is 
R
 and det0<A, prove that A has at least one 
positive and one negative eigenvalue. 
26.4     Let     
(; )∈ALVV be regular.  Show that 
 
 
111
det()()detdet()
N
λλλ
−−−   −
−=−−AIAAI 
 
where 
dimN=V. 
26.5     Prove that the characteristic polynomial of a projection     :
→PVV is 
 
                                                           () (1)(1   )
NNL
fλλλ
−−
=−− 
 
where 
dimN=V and dim   (    )L=PV. 
26.6     If     (   ;    )
∈ALVV is regular, express 
1−
A as a polynomial in A. 
26.7     Prove directly that the quantity 
j
μ
 defined by (26.6) is independent of the choice of basis 
for 
V. 
26.8     Let     
C be an endomorphism whose matrix relative to a basis 
{}
1
,...,
N
ee is a triangular 
matrix of the form 
 

156 Chap 6 • SPECTRAL DECOMPOSITIONS 
 
 
0100
010
()
0
01
00
M
⋅⋅⋅
⎡⎤
⎢⎥
⎢⎥
⋅⋅⋅
⎢⎥
⎢⎥
=
⋅⋅⋅
⎢⎥
⎢⎥
⋅⋅
⎢⎥
⎢⎥
⎢⎥
⎣⎦
C                                         (26.19)                                         
 
i.e. 
C maps 
1
e to 0, 
2
e to 
1
e, 
3
e to 
2
e,....  Show that a necessary and sufficient condition 
for the existence of a component matrix of the form (26.19) for an endomorphism 
C is that 
 
 
1
but
NN−
=≠C0C  0
                                           (26.20)                                           
 
 
We call such an endomorphism 
C a nilcyclic endomorphism and we call the basis 
{}
1
,...,
N
ee for the form (26.19) a cyclic basis for C. 
26.9     If     
1
,...,
N
φφ denote the fundamental invariants of 
1−
A, use the results of Exercise 26.4 to 
show that 
 
 
1
(det)
jNj
φμ
−
−
=A 
 
for 
1,...,jN=. 
26.10   It follows from (26.16) that 
 
 
1
adj
N−
=BA 
 
Use this result along with (26.17) and show that 
 
 
12
11
adj()()
NN
N
μμ
−−
−
=−   +  −    +⋅⋅⋅+AAAI 
 
26.11   Show   that   
 
 
1
tr(adj   )
N
μ
−
=A 
 

Sec. 26 • The Characteristic Polynomial 157 
 
and, from Exercise (26.10), that 
 
 
{}
12
112
1
tr()tr()tr()
1
NN
NN
N
μμμ
−−
−−
=−−+    −+⋅⋅⋅+−
−
AAA 

158 Chap 6 • SPECTRAL DECOMPOSITIONS 
 
 
Section 27.   Spectral Decomposition for Hermitian Endomorphisms 
 
An important problem in mathematical physics is to find a basis for a vector space 
V in 
which the matrix of a given (   ;    )
∈ALVV is diagonal.  If we examine (25.1), we see that if V has 
a basis of eigenvectors of A, then (   )
MA is diagonal and vice versa;  further, in this case the 
diagonal elements of (   )
MA are the eigenvalues of A.  As we shall see, not all endomorphisms 
have matrices which are diagonal.  Rather than consider this problem in general, we specialize here 
to the case where A,.then(   )
MA is Hermitian and show that every Hermitian endomorphism has a 
matrix which takes on the diagonal form.  The general case will be treated later, in Section 30.  
First, we prove a general theorem for arbitrary endomorphisms about the linear independence of 
eigenvectors. 
 
Theorem 27.1.  If 
1
,...,
L
λλ are distinct eigenvalues of (; )∈ALVV and if 
1
,...,
L
uu are 
eigenvectors corresponding to them, then 
{}
1
,...,
L
uu form a linearly independent set. 
 
Proof.  If  
{}
1
,...,
L
uu
 is not linearly independent, we choose a maximal, linearly independent 
subset, say 
{}
1
,...,
S
uu, from the set 
{}
1
,...,
L
uu;  then the remaining vectors can be expressed 
uniquely as linear combinations of 
{}
1
,...,
S
uu, say 
 
 
111SSS
αα
+
=+⋅⋅⋅+uuu                                                   (27.1)                                                   
 
where 
1
,...,
S
αα are not all zero and unique, because 
{}
1
,...,
S
uu
 is linearly independent.  Applying  
A
 to (27.1) yields 
 
 
11111
()( )
SSSSS
λαλαλ
++
=+⋅⋅⋅+uuu                                          (27.2)                                          
 
Now if 
1
0
S
λ
+
=, then 
1
,...,
S
λλ are nonzero because the eigenvalues are distinct and (27.2) 
contradicts the linear independence of 
{}
1
,...,
S
uu;  on the other hand, if 
1
0
S
λ
+
≠, then we can 
divide (27.2) by 
1S
λ
+
, obtaining another expression of 
1S+
u as a linear combination of 
{}
1
,...,
S
uu 
contradicting the uniqueness of the coefficients 
1
,...,
S
αα.  Hence in any case the maximal linearly 
independent subset cannot be a proper subset of 
{}
1
,...,
L
uu
;  thus 
{}
1
,...,
L
uu
 is linearly 
independent. 
 

Sec. 27 • Hermitian Endomorphisms 159 
 
As a corollary to the preceding theorem, we see that if the geometric multiplicity is equal to 
the algebraic multiplicity for each eigenvalue of 
A
, then the vector space V admits the direct sum 
representation 
 
 
12
()( )( )
L
λλλ=⊕⊕⋅⋅⋅⊕VV   VV 
 
where 
1
,...,
L
λλ are the distinct eigenvalues of A.  The reason for this representation is obvious, 
since the right-hand side of the above equation is a subspace having the same dimension as 
V
;  
thus that subspace is equal to 
V.  Whenever the representation holds, we can always choose a 
basis of 
V formed by bases of the subspaces 
1
(   ),...,    ()
L
λλVV.  Then this basis consists entirely of 
eigenvectors of Abecomes a diagonal matrix, namely, 
 
 
1
1
2
2
0
()
0
L
L
M
λ
λ
λ
λ
λ
λ
⎡⎤
⎢⎥
⋅
⎢⎥
⋅
⎢⎥
⎢⎥
⎢⎥
⎢⎥
⎢⎥
⋅
⎢⎥
⎢⎥
⋅
=
⎢⎥
⎢⎥
⎢⎥
⋅
⎢⎥
⋅
⎢⎥
⎢⎥
⎢⎥
⋅
⎢⎥
⎢⎥
⋅
⎢⎥
⎢⎥
⎣⎦
A
                 (27.3)
1
 
 
where each 
K
λ is repeated 
k
d times, 
k
d being the algebraic as well as the geometric multiplicity of 
K
λ.  Of course, the representation of 
V
 by direct sum of eigenspaces of A is possible if A has 
dimN=V distinct eigenvalues.  In this case the matrix of A taken with respect to a basis of 
eigenvectors has the diagonal form 
 

160 Chap 6 • SPECTRAL DECOMPOSITIONS 
 
 
1
2
3
0
()
0
N
M
λ
λ
λ
λ
⎡⎤
⎢⎥
⎢⎥
⎢⎥
⎢⎥
=⋅
⎢⎥
⎢⎥
⋅
⎢⎥
⋅
⎢⎥
⎢⎥
⎣⎦
A
                                      (27.3)
2
 
 
If the eigenvalues of 
v are not all distinct, then in general the geometric multiplicity of an 
eigenvalue may be less than the algebraic multiplicity.  Whenever the two multiplicities are 
different for at least one eigenvalue of A, it is no longer possible to find any basis in which the    
matrix of 
A
 is diagonal.  However, if V is an inner product space and if 
A
 is Hermitian, then a 
diagonal matrix of 
A
 can always be found; we shall now investigate this problem. 
 
Recall that if 
u and v are arbitrary vectors in V, the adjoint 
*
A of (   ;    )∈ALVV is 
defined by 
 
 
*
⋅=⋅Au  vu  A v
                                                         (27.4)                                                         
 
As usual, if the matrix of A is referred to a basis 
{}
k
e
, then the matrix of 
*
A is referred to the 
reciprocal basis 
{}
k
e and is given by [cf. (18.4)] 
 
 
*
()    ()
T
MM=AA                                                        (27.5)                                                        
 
where the overbar indicates the complex conjugate as usual.  If 
A
 Hermitian, i.e., if 
*
=AA, then 
(27.4) reduces to 
 
 
⋅=⋅Au  vu  Av
                                                          (27.6)                                                          
 
for all 
,∈uvV
. 
 
Theorem 27.2.  The eigenvalues of a Hermitian endomorphism are all real. 
 
Proof.  Let  (   ;    )∈ALVV be Hermitian.  Since λ=Auu for any eigenvalue λ, we have 

Sec. 27 • Hermitian Endomorphisms 161 
 
 
 
λ
⋅
=
⋅
Au  u
uu
                                                             (27.7)                                                             
 
Therefore we must show that 
⋅Au  u is real or, equivalently, we must show ⋅= ⋅Au  uAu  u.  By 
(27.6) 
 
 
⋅=⋅ = ⋅Au  uu  AuAu  u                                                   (27.8)                                                   
 
where the rule 
⋅=⋅uv  vu has been used.  Equation (27.8) yields the desired result. 
 
Theorem 27.3.  If Ais Hermitian and if 
1
V is an A-invariant subspace of 
V
, then 
1
⊥
V
 is also A-
invariant . 
 
Proof. If 
1
∈vV and 
1
⊥
∈uV, then ⋅=Av  u0 because 
1
∈AvV.  But since Ais Hermitian, 
0⋅=⋅ =Av  uv  Au.  Therefore, 
1
⊥
∈AuV, which proves the theorem. 
 
Theorem 27.4.  If 
A is Hermitian, the algebraic multiplicity of each eigenvalue equa1s the 
geometric multiplicity. 
 
Proof   Let   
0
()λV be the characteristic subspace associated with an eigenvalue 
0
λ.  Then the 
geometric multiplicity of 
0
λ is 
0
dim    (    )Mλ=V.  By Theorems 13.4 and 27.3 
 
 
00
()()λλ
⊥
⊕V=VV
                                                     (27.9)                                                     
 
Where 
0
()λV and 
0
()λ
⊥
V are A-invariant.  By Theorem 24.1 
 
 
12
=+AA A 
 
and by Theorem 17.4 
 
 
12
=+IP P 
 

162 Chap 6 • SPECTRAL DECOMPOSITIONS 
 
where 
1
P projects V onto 
0
()λV, 
2
P projects  V onto 
0
()λ
⊥
V, 
11
=AAP, and 
22
=AAP.  By 
Theorem 18.10, 
1
P and 
2
P are Hermitian and they also commute with A.  Indeed, for any ∈vV, 
10
()λ∈PvV and 
20
()λ
⊥
∈PvV
, and thus 
10
()λ∈AP vV and 
20
()λ
⊥
∈AP vV
.  But since 
 
 
1212
()=+= +AvA  PP   vAP v    AP v 
 
we see that 
1
AP v is the 
0
()λV component of Av and 
2
AP vis the 
0
()λ
⊥
V component of Av.  
Therefore 
 
 
112 2
,==PAv    APvP Av    AP v 
 
for all 
∈vV, or, equivalently 
 
 
112 2
,==P AAPP AAP 
 
Together with the fact that 
1
P, and 
2
P are Hermitian, these equations imply that 
1
A and 
2
A are 
also Hermitian.  Further, A is reduced by the subspaces 
0
()λV and 
0
()λ
⊥
V
, since 
 
 
2
121   212
==A AAP APA P P 
 
Thus if we select a basis 
{}
1
,...,
N
ee such that 
{}
1
,...,
M
ee span 
0
()λV and 
{}
1
,...,
MN+
ee span 
0
()λ
⊥
V, then by the result of Exercise 24.1 the matrix of A to 
{}
k
e
 takes the form 
 
 
11
1
1
11
1
1
0
()
0
M
MM
M
MM
MN
NN
MN
AA
AA
M
AA
AA
++
+
+
⎡⎤
⋅⋅⋅
⎢⎥
⋅⋅
⎢⎥
⎢⎥
⋅⋅
⎢⎥
⋅⋅
⎢⎥
⎢⎥
⋅⋅⋅
=
⎢⎥
⋅⋅⋅
⎢⎥
⎢⎥
⋅⋅
⎢⎥
⋅⋅
⎢⎥
⎢⎥
⋅⋅
⎢⎥
⋅⋅⋅
⎢⎥
⎣⎦
A
 

Sec. 27 • Hermitian Endomorphisms 163 
 
 
and the matrices of 
1
A and 
2
A are 
 
 
11
1
1
1
2
11
1
1
0
()
00
00
()
0
M
MM
M
MM
MN
NN
MN
AA
AA
M
M
AA
AA
++
+
+
⎡⎤
⋅⋅⋅
⎢⎥
⋅⋅
⎢⎥
⎢⎥
⋅⋅
⎢⎥
⋅⋅
⎢⎥
⎢⎥
⋅⋅⋅
=
⎢⎥
⎢⎥
⎢⎥
⎢⎥
⎢⎥
⎢⎥
⎢⎥
⎢⎥
⎣⎦
⎡⎤
⎢⎥
⎢⎥
⎢⎥
⎢⎥
⎢⎥
⎢⎥
=
⎢⎥
⋅⋅⋅
⎢⎥
⎢⎥
⋅⋅
⎢⎥
⋅⋅
⎢⎥
⎢⎥
⋅⋅
⎢⎥
⋅⋅⋅
⎢⎥
⎣⎦
A
A
                       (27.10)                       
 
which imply 
 
 
0
0
1()   2
()
det()det() det()
λ
λ
λλλ
⊥
−=  −−AIA IA  I
V
V
 
 
By (25.2), 
0
10()λ
λ=AI
V
; thus by (21.21) 
 

164 Chap 6 • SPECTRAL DECOMPOSITIONS 
 
 
0
02
()
det()()   det()
M
λ
λλλλ
⊥
−=−−AIA  I
V
 
 
On the other hand, 
0
λ is not an eigenvalue of 
2
A.  Therefore 
 
 
0
2
()
det()0
λ
λ
⊥
−≠AI
V
 
 
Hence the algebraic multiplicity of 
0
λ equals M, the geometric multiplicity. 
 
The preceding theorem implies immediately the important result that 
V
 can be represented 
by a direct sum of eigenspaces of A if A is Hermitian.  In particular, there exists a basis 
consisting entirely in eigenvectors of A, and the matrix of A relative to this basis is diagonal.  
However, before we state this result formally as a theorem, we first strengthen the result of 
Theorem 27.1 for the special case that A is Hermitian. 
 
Theorem 27.5.  If A is Hermitian, the eigenspaces corresponding to distinct eigenvalues 
1
λ and 
2
λ are orthogonal. 
 
Proof.  Let  
111
λ=Auu and 
222
λ=Auu.  Then 
 
 
1121212212
λλ⋅= ⋅=⋅ = ⋅u   uAu   uu   Auu   u 
 
Since 
12
λλ≠, 
12
0⋅=uu, which proves the theorem. 
 
The main theorem regarding Hermitian endomorphisms is the following. 
 
Theorem 27.6.  If A is a Hermitian endomorphism with (distinct) eigenvalues 
12
,,...,
N
λλλ, then 
V has the representation 
 
 
112
()( )( )
L
λλλ=⊕⊕⋅⋅⋅⊕VV   VV                                         (27.11)                                         
 
where the eigenspaces     (    )
k
λV are mutually orthogonal. 
 

Sec. 27 • Hermitian Endomorphisms 165 
 
The proof of this theorem is a direct consequence of Theorems 27.4 and 27.5 and the 
remark following the proof of Theorem 27.1. 
 
Corollary (Spectral Theorem).  If A is a Hermitian endomorphish with (distinct) eigenvalues 
1
,...,
L
λλ then 
 
 
1
N
jj
j
λ
=
=
∑
AP                                                          (27.12)                                                          
 
where 
j
P is the perpendicular projection of V onto (    )
j
λV, for 1,...,jL=. 
 
Proof.  By Theorem 27.6, 
A
 has a representation of the form (24.1).  Let u be an arbitrary element 
of 
V
, then, by (27.11), 
 
 
1L
=+⋅⋅⋅+uuu                                                       (27.13)                                                       
 
where             (    )
jj
λ∈uV.  By (24.3), (27.13), and (25.1) 
 
 
jjjjjj
λ===Au   AuAuu 
 
But 
jj
=uPu
; therefore 
 
                                                                                     (no                                                                                     sum)
jjj
λ=AP 
 
which, with (24.1) proves the corollary. 
 
The reader is reminded that the L perpendicular projections satisfy the equations 
 
 
1
L
j
j=
=
∑
PI                                                             (27.14)                                                             
 
 
2
jj
=PP
                                                             (27.15)                                                             
 

166 Chap 6 • SPECTRAL DECOMPOSITIONS 
 
 
*
jj
=PP                                                              (27.16)                                                              
and 
 
 
ji
ij=≠PP    0                                                    (27.17)                                                    
 
These equations follow from Theorems 14.4, 18.10 and 18.11.  Certain other endomorphisms also 
have a spectral representation of the form (27.12);  however, the projections are not perpendicular 
ones and do not obey the condition (27.17). 
 
Another Corollary of Theorem 27.6 is that if A is Hermitian, there exists an orthogonal 
basis for 
V consisting entirely of eigenvectors of 
A
.  This corollary is clear because each 
eigenspace is orthogonal to the other and within each eigenspace an orthogonal basis can be 
selected.  (Theorem 13.3 ensures that an orthonormal basis for each eigenspace can be found.)  
With respect to this basis of eigenvectors, the matrix of 
A
is clearly diagonal.  Thus the problem of 
finding a basis for 
V such that (   )MA is diagonal is solved for Hermitian endomorphisms. 
 
If    (   )fA is any polynomial in the Hermitian endomorphism, then (27.12),(27.15) and 
(27.17) can be used to show that 
 
 
1
()( )
L
jj
j
ffλ
=
=
∑
AP                                                    (27.18)                                                    
 
where    (   )
fλ is the same polynomial except that the variable A is replaced by the scalar 
λ
.  For 
example, the polynomial 
2
P has the representation 
 
 
22
1
N
jj
j
λ
=
=
∑
AP 
 
In general,    (   )fA is Hermitian if and only if    (    )
j
fλ is real for all 1,...,jL=.  If the eigenvalues of    
A are all nonnegative, then we can extract Hermitian roots of A by the following rule: 
 
 
1/1/
1
N
kk
jj
j
λ
=
=
∑
AP                                                       (27.19)                                                       
 
where 
1/
0
k
j
λ≥.  Then we can verify easily that 
()
1/
k
k
=AA.  If Ahas no zero eigenvalues, then 

Sec. 27 • Hermitian Endomorphisms 167 
 
 
 
1
1
1
L
j
j
j
λ
−
=
=
∑
AP                                                        (27.20)                                                        
 
which is easily confirmed. 
 
A Hermitian endomorphism A is defined to be 
 
 
positive definite0
positive semidefinite0
  if   
negative semidefinite0
negative definite0
>
⎧⎫⎧⎫
⎪⎪⎪⎪
≥
⎪⎪⎪⎪
⋅
⎨⎬⎨⎬
≤
⎪⎪⎪⎪
⎪⎪⎪⎪
<
⎩⎭⎩⎭
vAv 
 
all nonzero 
v,  It follows from (27.12) that 
 
 
1
L
jj    j
j
λ
=
⋅=⋅
∑
vAvv  v                                                    (27.21)                                                    
 
where 
 
 
1
L
j
j=
=
∑
vv 
 
Equation (27.21) implies the following important theorem. 
 
Theorem 27.7  A Hermitian endomorphism A is 
 
positive definite
positive semidefinite
negative semidefinite
negative definite
⎧⎫
⎪⎪
⎪⎪
⎨⎬
⎪⎪
⎪⎪
⎩⎭
 
 
if and only if every eigenvalue of A is 
 

168 Chap 6 • SPECTRAL DECOMPOSITIONS 
 
 
0
0
0
0
>
⎧⎫
⎪⎪
≥
⎪⎪
⎨⎬
≤
⎪⎪
⎪⎪
<
⎩⎭
 
 
As corollaries to Theorem 27.7 it follows that positive-definite and negative-definite 
Hermitian endomorphisms are regular [see (27.20)], and positive-definite and positive-semidefinite 
endomorphisms possess Hermitian roots. 
 
Every complex number 
z has a polar representation in the form 
i
zre
θ
=, where 
0r≥
.  It 
turns out that an endomorphism also has polar decompositions, and we shall deduce an important 
special case here. 
 
Theorem 27.8.  (Polar Decomposition Theorem).  Every automorphism 
A
 has two unique 
multiplicative decompositions 
 
 
and==ARUAVR
                                           (27.22)                                           
 
 
where R is unitary and 
U and V are Hermitian and positive definite. 
 
Proof.  : For each automorphism A, 
*
AA is a positive-definite Hermitian endomorphism, and 
hence it has a spectral decomposition of the form 
 
 
*
1
L
jj
j
λ
=
=
∑
AAP                                                        (27.23)                                                        
 
where         0,1,...,
j
jLλ>=.  We define U by 
 
 
()
1/ 2
*1/2
1
L
jj
j
λ
=
==
∑
UAAP                                                (27.24)                                                
 
Clearly 
U is Hermitian and positive definite.  Since U is positive definite it is regular.  We now 
define 
 

Sec. 27 • Hermitian Endomorphisms 169 
 
 
1−
=RAU                                                            (27.25)                                                            
 
By this formula R is regular and satisfies 
 
 
*1*1121−−−−
===RR   U AAUU UUI
                                        (27.26)                                        
 
Therefore Ris unitary.  To prove the uniqueness, assume 
 
 
11
=RUR U                                                           (27.27)                                                           
 
and we shall prove 
1
=RR and 
1
=UU.  From (27.27) 
 
 
2**2
11   111
()(  )== =   =UURRURURURURUU 
 
Since the positive-definite Hermitian square root of 
2
U is unique, we find 
1
=UU.  Then (27.27) 
implies 
1
=RR.  The decomposition (27.22)
2
 follows either by defining 
2*
=VAA, or, 
equivalently, by defining 
*
=VRUR and repeating the above argument. 
 
A more general theorem to the last one is true even if A is not required to be regular.  
However, in this case R is not unique. 
 
Before closing this section we mention again that if the scalar field is real, then a Hermitian 
endomorphism is symmetric, and a unitary endomorphism is orthogonal.  The reader may rephrase 
the theorems in this and other sections for the real case according to this simple rule. 
 
 
Exercises 
 
27.1     Show that Theorem 27.5 remains valid if A is unitary or skew-Hermitian rather than 
Hermitian. 
27.2     If     
(; )∈ALVV is Hermitian, show that A is positive semidefinite if  and only if the 
fundamental invariants of A are nonnegative. 
27.3     Given an endomorphism 
A, the exponential of A is an endomorphism expA defined by 
the series 

170 Chap 6 • SPECTRAL DECOMPOSITIONS 
 
 
 
1
1
exp
!
j
j
j
∞
=
=
∑
AA                                                       (27.28)                                                       
 
It is possible to show that this series converges in a definite sense for all A.  We shall 
examine this question in Section 65.  Show that if 
(; )∈ALVV is a Hermitian 
endomorphism given by the representation (27.12), then 
 
 
1
exp
j
L
j
j
e
λ
=
=
∑
AP
                                                       (27.29)                                                       
 
This result shows that the series representation of  expA is consistent with (27.18). 
27.4     Suppose     that     A is a positive-definite Hermitian endomorphism;  give a definition for  logA 
by power series and one by a formula similar to (27.18), then show that the two definitions 
are consistent.    Further, prove that 
 
 
log exp=AA 
 
27.5     For     any     
(; )∈ALVV show that 
*
AA and 
*
AA are Hermitian and positive semidefinite.  
Also, show that A is regular if and only if 
*
AA and 
*
AA are positive definite. 
27.6     Show     that     
kk
=AB where k is a positive integer, generally does not imply =AB even if 
both A and B are Hermitian. 

Sec. 28 • Illustrative Examples   171 
 
 
Section 28.   Illustrative Examples 
 
In this section we shall illustrate certain of the results of the preceding section by working 
selected numerical examples.    For simplicity, the basis of 
V
 shall be taken to be the orthonormal 
basis 
{}
1
,...,
N
ii
 introduced in (13.1).  The vector equation (25.1) takes the component form 
 
 
kjjk
Avvλ=
                                                             (28.1)                                                             
 
where 
 
 
jj
v=vi                                                                (28.2)                                                                
 
and 
 
 
jkjk
A=Aii                                                             (28.3)                                                             
 
Since the basis is orthonormal, we have written all indices as subscripts, and the summation 
convention is applied in the usual way. 
 
Example 1. Consider a real three-dimensional vector space V.  Let the matrix of an 
endomorphism 
(; )∈ALVV be 
 
 
110
()   1 2 1
011
M
⎡⎤
⎢⎥
=
⎢⎥
⎢⎥
⎣⎦
A
                                                      (28.4)                                                      
 
Clearly 
A is symmetric and, thus, the theorems of the preceding section can be applied.  By direct 
expansion of the determinant of ()
Mλ−AI the characteristic polynomial is 
 
                                                            () (  )(1   )(3   )
fλλλλ=−− −                                                 (28.5)                                                 
 
Therefore the three eigenvalues of 
A
 are distinct and are given by 
 

172 Chap 6 • SPECTRAL DECOMPOSITIONS 
 
 
123
0,1,3λλλ===                                                     (28.6)                                                     
 
The ordering of the eigenvalues is not important.  Since the eigenvalues are distinct, their 
corresponding characteristic subspaces are one-dimensional.  For definiteness, let 
()p
v be an 
eigenvector associated with 
p
λ.  As usual, we can represent 
()p
v
 by 
 
 
()()pp
kk
v=vi                                                            (28.7)                                                            
 
Then (28.1), for 1
p=, reduces to 
 
 
(1)(1)(1)(1)(1)(1)(1)
121    2323
0,20,0vvv  vvvv+=   + +=   +=
                              (28.8)                              
 
The general solution of this linear system is 
 
 
(1)(1)(1)
123
,,vtv  tvt==−=                                        (28.9)                                        
 
for all 
t∈R.  In particular, if 
(1)
v is required to be a unit vector, then we can choose 13t=±, 
where the choice of sign is arbitrary, say 
 
 
(1)(1)(1)
123
13,13,13vvv==−=                             (28.10)                             
 
So 
 
 
(1)
12 3
(13)(13)(13)=−+viii                                       (28.11)                                       
 
Likewise we find for 
2p= 
 
 
(2)
13
(12 )(12 )=−vii                                               (28.12)                                               
 
and for 3
p= 
 
 
(3)
123
(1    6)(26)(1    6)=+ +viii                                      (28.13)                                      
 

Sec. 28 • Illustrative Examples   173 
 
It is easy to check that 
{}
(1)( 2)(3)
,,vv v is an orthonormal basis. 
 
By (28.6) and (27.12), 
A
 has the spectral decomposition 
 
 
12 3
01 3=++APP P                                                     (28.14)                                                     
 
where 
k
P is the perpendicular projection defined by 
 
 
()()()
,,
kkj
kk
jk==≠PvvPv0
                                       (28.15)                                       
 
for       1, 2, 3
k=.  In component form relative to the original orthonormal basis 
{}
123
,,ii i
 these 
projections are given by 
 
 
() ()
(no sum on   )
kk
kjj    l   i
vvk=Pii                                      (28.16)                                      
 
This result follows from (28.15) and the transformation law (22.7) or directly from the 
representations 
 
 
(1)( 2)(3)
1
(1)(3)
2
(1)( 2 )( 3)
3
(13 )(1 2)(16 )
(13 )( 26 )
(13 )(1 2)(16 )
=++
=−+
=−+
ivv  v
ivv
ivv  v
                                      (28.17)                                      
 
since the coefficient matrix of  
{}
123
,,ii i relative to 
{}
(1)( 2 )( 3)
,,vv v is the transpose of that of 
{}
(1)( 2 )( 3)
,,vv vrelative to 
{}
123
,,ii i. 
 
There is a result, known as Sylvester’s Theorem, which enables one to compute the 
projections directly.  We shall not prove this theorem here, but we shall state the formula in the 
case 
when the eigenvalues are distinct.  The result is 
 

174 Chap 6 • SPECTRAL DECOMPOSITIONS 
 
 
1;
1;
()
()
N
kjk k
j
N
kjk kj
λ
λλ
=≠
=≠
−
=
−
IA
P


                                                  (28.18)                                                  
 
The advantage of this formula is that one does not need to know the eigenvectors in order to find 
the projections.  With respect to an arbitrary basis, (28.18) yields 
 
 
1;
1;
(()())
()
()
N
kjk k
j
N
kjk kj
MM
M
λ
λλ
=≠
=≠
−
=
−
IA
P


                                        (28.19)                                        
 
Example 2.  To illustrate (28.19), let 
 
 
22
()
21
M
⎡⎤
=
⎢⎥
−
⎣⎦
A                                                      (28.20)                                                      
 
with respect to an orthonormal basis.  The eigenvalues of this matrix are easily found to be 
12
2,3λλ=−   =.  Then, (28.19) yields 
 
 
1221
10   2 21   2
1
()()
01    2  12 4
5
Mλλλ
−
⎛⎞
⎡⎤⎡ ⎤⎡  ⎤
=−   −=
⎜⎟
⎢⎥⎢ ⎥⎢  ⎥
−−
⎣⎦⎣ ⎦⎣  ⎦
⎝⎠
P
 
 
and 
 
 
2121
10   2 242
1
()()
01    2  121
5
Mλλλ
⎛⎞
⎡⎤⎡ ⎤⎡⎤
=−   −=
⎜⎟
⎢⎥⎢ ⎥⎢⎥
−
⎣⎦⎣ ⎦⎣⎦
⎝⎠
P
 
 
The spectral theorem for this linear transformation yields the matrix equation 
 
 
12   42
23
()
2421
55
M
−
⎡⎤⎡ ⎤
=−+
⎢⎥⎢ ⎥
−
⎣⎦⎣ ⎦
A 
 
 
Exercises 
 

Sec. 28 • Illustrative Examples   175 
 
28.1     The matrix of (   ;    )
∈ALVV with respect to an orthonormal basis is 
 
21
()
12
M
⎡⎤
=
⎢⎥
⎣⎦
A 
 
(a)        Express        (   )
MA in its spectral form. 
(b)       Express       
1
()M
−
A in its spectral form. 
(c) Find the matrix of the square root of A. 
28.2     The matrix of (   ;    )
∈ALVV with respect to an orthonormal basis is 
 
 
302
()   0 2 0
103
M
⎡⎤
⎢⎥
=
⎢⎥
⎢⎥
⎣⎦
A
 
 
Find an orthonormal basis for 
V relative to which the matrix of Ais diagonal. 
28.3     If     (   ;    )∈A
LVV is defined by 
 
 
112
212
22   2
31
(21)(3   2)
22
=−
=++−
Aiii
Aiii
 
and 
 
 
33
=Aii 
 
where 
{}
123
,,ii i is an orthonormal basis for 
V
, determine the linear transformations R, 
U, and V in the polar decomposition theorem. 

176 Chap 6 • SPECTRAL DECOMPOSITIONS 
 
 
Section 29    The Minimal Polynomial 
 
In Section 26 we remarked that for any given endomorphism Athere exist some 
polynomials 
()ft such that 
 
 
()f=A0                                                              (29.1)                                                              
 
For example, by the Cayley-Hamilton theorem, we can always choose, 
()ft to be the characteristic 
polynomial of 
A.  Another obvious choice can be found by observing the fact that since (; )LVV 
has dimension 
2
N, the set 
 
 
{}
2
012
,,,...,
N
=AIAA  A 
 
is necessarily linearly dependent and, thus there exist scalars 
{}
2
01
,    ,...,
N
αα  α, not all equal to zero, 
such that 
 
 
2
2
1
01
N
N
ααα++⋅⋅⋅+=IAA 0                                               (29.2)                                               
 
For definiteness, let us denote the set of all polynomials 
f satisfying the condition (29.1) for a 
given A by the symbol     (   )A
P.  We shall now show that     (   )AP has a very simple structure, called 
a 
principal ideal. 
 
In general, an ideal 
Iis a subset of an integral domain 
D
 such that the following two 
conditions are satisfied: 
1.         If         
f and g belong to I, so is their sum fg+. 
2.         If         
f belongs to I and h arbitrary element of D, then fhhf= also belong to I. 
 
We recall that the definition of an integral domain is given in Section 7.  Of course, 
D itself 
and the subset 
{}
0 consisting in the zero element of D are obvious examples of ideals, and these 
are called 
trivial ideals or improper ideals.  Another example of ideal is the subset ⊂ID 
consisting in all multiples of a particular element 
g∈D, namely 
 

Sec. 29 • The Minimal Polynomial                                                             177 
 
 
{}
,hg  h=∈ID                                                         (29.3)                                                         
 
It is easy to verily that this subset satisfies the two conditions for an ideal.  Ideals of the special 
form (29.3) are called 
principal ideals. 
 
For the set 
()AP we choose the integral domain D to be the set of all polynomials with 
complex coefficients.  (For real vector space, the coefficients are required to be real, of course.)  
Then it is obvious that 
()AP is an ideal in D, since if f and g satisfy the condition (29.1), so 
does their sum 
fg+ and similarly if f satisfies (29.1) and h is an arbitrary polynomial, then 
 
 
( )()   ()()   ()hfhfh===AAAA00                                           (29.4)                                           
 
The fact that 
()AP is actually a principal ideal is a standard result in algebra, since we have the 
following theorem. 
 
Theorem 29.1.  Every ideal of the polynomial domain is a principal ideal. 
 
Proof.  We assume that the reader is simi1iar with the operation of division for polynomials.  If f 
and 
0g≠ are polynomials, we can divide f by g and obtain a remainder r having  degree less 
than 
g, namely 
 
 
()()() ()rtf t    htgt=−
                                                     (29.5)                                                     
 
Now, to prove that 
()AP
 can be represented by the form (29.3), we choose a polynomial 
0g≠
 
having the lowest degree in 
()AP
.  Then we claim that 
 
 
{}
(),hg  h=∈APD                                                      (29.6)                                                      
 
To see this, we must show that every (   )
f∈AP can be devided through by g without a remainder.  
Suppose that the division of 
f by g yields a remainder r as shown in (29.5).  Then since     (   )AP 
is an ideal and since    ,(   )
fg∈AP, (29.5) shows that (   )r∈AP also.  But since the degree of r is 
less than the degree of    ,(   )
gr∈AP is possible if and only if 0r=.  Thus fhg=, so the 
representation (29.6) is valid. 
 

178 Chap 6 • SPECTRAL DECOMPOSITIONS 
 
A corollary of the preceding theorem is the fact that the nonzero polynomial 
g having the 
lowest degree in     (   )A
P is unique to within an arbitrary nonzero multiple of a scalar.  If we require 
the leading coefficient of 
g to be 1 , then g becomes unique, and we call this particular 
polynomial 
g the minimal polynomial  of the endomorphism A. 
 
We pause here to give some examples of minimal polynomials. 
 
Example 1. The minimal polynomial of the zero endomorphism 
0
 is the polynomial    ( )    1ft= of 
zero degree, since by convention 
 
 
0
()  1f==000                                                          (29.7)                                                          
 
In general, if 
≠A0, then the minimal polynomial of 
A
 is at least of degree 
1
, since in this case 
 
 
0
11=≠AI0                                                            (29.8)                                                            
 
Example 2.  Let P be a nontrivial projection.  Then the minimal polynomial g of P is 
 
 
2
()gt    t    t=−                                                            (29.9)                                                            
 
For, by the definition of a projection, 
 
 
2
()−=−=PPPPI0                                                   (29.10)                                                   
 
and since P is assumed to be non trivial, the two lower degree divisors 
t and 1t− no longer 
satisfy the condition (29.1) for P. 
 
Example 3. For the endomorphism C whose matrix is given by (26.19) in Exercise 26.8, the 
minimal polynomial 
g is 
 
                                                                          ()
N
gt    t=                                                             (29.11)                                                             
 
since we have seen in that exercise that 
 

Sec. 29 • The Minimal Polynomial                                                             179 
 
 
1
but
NN−
=≠C0C  0 
 
For the proof of some theorems in the next section we need several other standard results in 
the algebra of polynomials.  We summarize these results here. 
 
Theorem 29.2.  If 
f and g are polynomials, then there exists a greatest common divisor d which 
is a divisor (i.e., a factor) of 
f and g and is also a multiple of every common divisor of f and g. 
 
Proof.  We define the ideal I in the polynomial domain 
D
 by 
 
 
{}
,   ,hfkg   h  k≡+   ∈ID
                                                 (29.12)                                                 
 
By Theorem 29.1, 
I is a principal ideal, and thus it has a representation 
 
 
{}
, hd   h≡∈ID
                                                       (29.13)                                                       
 
We claim that 
d is a greatest common divisor of f and g.  Clearly, d is a common divisor of f 
and 
g, since f and g are themselves members of I, so by (29.13) there exist h and k in D 
such that 
 
 ,
fhdgkd==                                                  (29.14)                                                  
 
On the other hand, since 
d is also a member of I, by (29.12) there exist also p and q in D such 
that 
 
 
dpfqg=+                                                          (29.15)                                                          
 
Therefore if 
c is any common divisor of f and g, say 
 
 ,
facgbc==                                                  (29.16)                                                  
 
then from (29.15) 
 
                                                              ()
dpaqbc=+                                                        (29.17)                                                        

180 Chap 6 • SPECTRAL DECOMPOSITIONS 
 
 
so 
d
 is a multiple of 
c
.  Thus 
d
 is a greatest common divisor of f and g. 
 
By the same argument as before, we see that the greatest common divisor 
d
 of f and g is 
unique to within a nonzero scalar factor.  So we can render 
d unique by requiring its leading 
coefficient to be 
1
.  Also, it is clear that the preceding theorem can be extended in an obvious way 
to more than two polynomials.  If the greatest common divisor of 
1
,...,
L
ff is the zero degree 
polynomial 1 , then 
1
,...,
L
ff are said to be relatively prime.  Similarly 
1
,...,
L
ff are pairwise prime 
if each pair 
,,
ij
ffi  j≠
, from 
1
,...,
L
ff is relatively prime. 
 
Another important concept associated with the algebra of polynomials is the concept of the 
least common multiple. 
 
Theorem 29.3.  If 
f and g are polynomials, then there exists a least common multiple m 
which is a multiple of 
f and g and is a divisor of every common multiple of f and g. 
 
The proof of this theorem is based on the same argument as the proof of the preceding 
theorem, so it is left as an exercise. 
 
 
Exercises 
 
29.1     If the eigenvalues of an endomorphism 
A
 are all single roots of the characteristic equation, 
show that the characteristic polynomial of 
A
 is also a minimal polynomial of 
A
. 
29.2     Prove Theorem 29.3. 
29.3     If     
f and g are nonzero polynomials and if d is their greatest common divisor, show that 
then 
 
 
/mfgd= 
 
is their least common multiple and, conversely, if 
m is their least common multiplies, then 
 
 /
dfgm= 
 

Sec. 29 • The Minimal Polynomial                                                             181 
 
is their greatest common divisor.

182 Chap 6 • SPECTRAL DECOMPOSITIONS 
 
 
Section 30.   Spectral Decomposition for Arbitrary Endomorphisms 
 
As we have mentioned in Section 27, not all endomorphism have matrices which are 
diagonal.  However, for Hermitian endomorphisms, a decomposition of the endomorphism into a 
linear combination of projections is possible and is given by (27.12).  In this section we shall 
consider the problem in general and we shall find decompositions which are, in some sense, closest 
to the simple decomposition (27.12) for endomorphisms in general. 
 
We shall prove some preliminary theorems first.  In Section 24 we remarked that the null 
space 
()KA is always A-invariant.  This result can be generalized to the following 
 
Theorem 30.1.  If .
f is any polynomial, then the null space (())KfA is A-invariant. 
 
Proof.  Since the multiplication of polynomials is a commutative operation, we have 
 
 
()    ()ff=AAAA                                                        (30.1)                                                        
 
Hence if 
(())Kf∈vA,  then 
 
 
()()ff===AAv   A  Av   A0  0                                              (30.2)                                              
 
which shows that 
(())Kf∈AvA.  Therefore (())KfA is A-invariant. 
 
Next we prove some theorems which describe the dependence of 
(())KfA on the choice of 
f. 
 
Theorem 30.2.  If 
f is a multiple of g, say 
 
 
fhg=                                                                (30.3)                                                                
 
Then 
 
 
(   (   ))(   (   ))KfKg⊃AA                                                     (30.4)                                                     

Sec. 30 •           Arbitrary           Endomorphisms                                                            183 
 
 
Proof.  This result is a general property of the null space.  Indeed, for any endomorphisms B and 
C we always have 
 
                                                                 ()  ()
KK⊂BCC                                                         (30.5)                                                         
 
So if we set    (   )
h=AB and    (   )g=AC, then (30.5) reduces to (30.4). 
 
The preceding theorem does not imply that     (   (   ))
KgA is necessarily a proper subspace of 
(())
KfA, however.  It is quite possible that the two subspaces, in fact, coincide.  For example, if 
g and hence f both belong to     (   )AP, then    (   )(   )gf==AA0, and thus 
 
                                                      (())(())()
KfKgK===AA0V                                           (30.6)                                           
 
However, if 
m is the minimal polynomial of A, and if f is a proper divisor of m(i.e., m is not a 
divisor of 
f) so that (   )f∉AP, then     (   (   ))KfA is strictly a proper  subspace of (   (   ))Km=AV.  
We can strengthen this result to the following 
 
Theorem 30.3.  If f is a divisor (proper or improper) of the minimal polynomial m of A, and if 
g is a proper divisor of f, then     (   (   ))KgA is strictly a proper subspace of     (   (   ))KfA. 
 
Proof.  By assumption there exists a polynomial h such that 
 
 
mhf=
                                                                (30.7)                                                                
 
We set 
 
 
khg=
                                                                (30.8)                                                                
 
Then 
k is a proper divisor of m, since by assumption, 
g
 is a proper divisor of 
f
.  By the remark 
preceding the theorem, 
(( ))KkA
 is strictly a subspace of V, which is equal to 
(())KmA
.  Thus 
there exists a vector 
v such that 
 
                                                           ()()()
kgh=≠AvA    Av  0                                                  (30.9)                                                  
 

184 Chap 6 • SPECTRAL DECOMPOSITIONS 
 
which implies that the vector (   )
h=uAv does not belong to     (   (   ))KgA.  On the other hand, from 
(30.7) 
 
                                                   ()()()()
ffhm====AuA    AvAv   0                                       (30.10)                                       
 
which implies that 
u
 belongs to     (   (   ))KfA.  Thus     (   (   ))KgA is strictly a proper subspace of 
(())
KfA. 
 
The next theorem shows the role of the greatest common divisor in terms of the null space. 
 
Theorem 30.4.  Let 
f and g be any polynomials, and suppose that 
d
 is their greatest common 
divisor.  Then 
 
                                                         (   (   ))(   (   ))(   (   ))
KdK fKg=∩AAA                                          (30.11)                                          
 
Obviously, this result can be generalized for more than two polynomials. 
 
Proof.  Since  
d
 is a common divisor of f and g, the inclusion 
 
                                                         (   (   ))(   (   ))(   (   ))
KdK fKg⊂∩AAA                                         (30.12)                                         
 
follows readily from Theorem 30.2.  To prove the reversed inclusion, 
 
                                                         (   (   ))(   (   ))(   (   ))
KdK fKg⊃∩AAA                                         (30.13)                                         
 
recall that from (29.15) there exist polynomials 
p and q such that 
 
 
dpfqg=+                                                          (30.14)                                                          
 
and thus 
 
                                                      ()   ()()   ()()
dpfqg=+AAAAA                                           (30.15)                                           
 
This equation means that if (   (   ))(   (   ))
KfKg∈∩vA   A, then 

Sec. 30 •           Arbitrary           Endomorphisms                                                            185 
 
 
 
()()()()()
()()
dpfqg
pq
=+
=+=
AvAAvA    Av
A0A0  0
                                       (30.16)                                       
 
so that (   (   ))Kd∈vA.  Therefore (30.13) is valid and hence (30.11). 
 
A corollary of the preceding theorem is the fact that if 
f and g are relatively prime then 
 
 
{}
(   (   ))(   (   ))KfKg∩=AA0                                              (30.17)                                              
 
since in this case the greatest common divisor of 
f and g is    ( )    1dt=, so that 
 
 
{}
0
(( ))(  )()KdKK===AAI0                                           (30.18)                                           
 
Here we have assumed 
≠A0 of course. 
 
Next we consider the role of the least common multiple in terms of the null space. 
 
Theorem 30.5.  Let 
f and g be any polynomials, and suppose that 
l
 is their least common 
multiplier.  Then 
 
                                                          ( (   ))(   (   ))(   (   ))KlK fKg
=+AAA                                          (30.19)                                          
 
 
where the operation on the right-hand side of (30.19) is the sum of subspaces defined in Section 10.  
Like the result (30.11), the result (30.19) can be generalized in an obvious way for more than two 
polynomials. 
 
The proof of this theorem is based on the same argument as the proof of the preceding 
theorem, so it is left as an exercise.  As in the preceding theorem, a corollary of this theorem is that 
if 
f and g are relatively prime (pairwise prime if there are more than two polynomials) then 
(30.19) can be strengthened to 
 
                                                          ( (   ))(   (   ))(   (   ))KlK fKg
=⊕AAA                                          (30.20)                                          

186 Chap 6 • SPECTRAL DECOMPOSITIONS 
 
 
Again, we have assumed that 
≠A0.  We leave the proof of (30.20) also as an exercise. 
 
Having summarized the preliminary theorems, we are now ready to state the main theorem 
of this section. 
 
Theorem 30.6.  If 
m is the minimal polynomial of A which is factored into the form 
 
 
1
11
()   ()()()()
L
aa
LL
mtttm tm  tλλ=  −    ⋅⋅⋅  −≡⋅⋅⋅                                 (30.21)                                 
 
where 
1
,...,
L
λλ are distinct and 
1
,...,
L
aa are positive integers, then V has the representation 
 
 
1
((   ))((   ))
L
KmKm=⊕⋅⋅⋅⊕AAV                                         (30.22)                                         
 
Proof.  Since  
1
,...,
L
λλ are distinct, the polynomials 
1
,...,
L
mm are pairwise prime and their least 
common multiplier is 
m.  Hence by  (30.20) we have 
 
 
1
(   (   ))((   ))((   ))
L
KmKmKm=⊕⋅⋅⋅⊕AAA                                    (30.23)                                    
 
But since     (   )0m=A,     (   (   ))Km=A
V, so that (30.22) holds. 
 
Now from Theorem 30.1 we know that each subspace     ((   )),.1,...,
i
KmiL=A is A-invariant;  
then from (30.22) we see that A is reduced by the subspaces 
1
((   )),...,    ((   ))
L
KmKmAA.  k{  ,{  )))  
.1 kfmL(6)).  Therefore, the results of Theorem 24.1 can be applied.  In particular, if we choose a 
basis for each     ((   ))
i
KmA and form a basis for V by the union of these bases, then the matrix of A 
takes the form (24.5) in Exercise 24.1.  The next theorem shows that, in some sense, the 
factorization (30.21) gives also the minimal polynomials of the restrictions of 
A
 to the various 
A
-
invariant subspaces from the representation (30.22). 
 
Theorem 30.7.  Each factor 
k
m of 
m
 is the minimal polynomial of the restriction of A to the 
subspace     ((   ))
k
KmA.  More generally, any product of factors, say 
1M
mm⋅⋅⋅ is the minimal 
polynomial of the restriction of A to the corresponding subspace 
1
((   ))((   ))
M
KmKm⊕⋅⋅⋅⊕AA 
 

Sec. 30 •           Arbitrary           Endomorphisms                                                            187 
 
Proof   : We prove the special case of one factor only, say 
1
m;  the proof of the general case of 
several factors is similar and is left as an exercise.  For definiteness, let 
A denote the restriction of 
A to 
1
(())KmA  Then 
1
()mA is equal to the restriction of 
1
()mA to 
1
(())KmA, and, thus 
1
()m=A0
, which means that 
1
()m∈AP
.  Now if g is any proper divisor of 
1
m, then ()g≠A0;  
for otherwise, we would have 
2
() ()()
L
gmm⋅⋅⋅AAA, contradicting the fact that m is minimal for 
A.  Therefore 
1
m is minimal for A and the proof is complete. 
 
The form (24.5), generally, is not diagonal.  However, if the minimal polynomial (30.21) 
has simple roots only, i.e., the powers 
1
,...,
L
aa are all equal to 1, then 
 
                                                                    ()
ii
mλ=−AA I                                                       (30.24)                                                       
 
for all i.  In this case, the restriction of 
A to     ((   ))
i
KmA coincides with 
i
λI on that subspace, 
namely 
 
 
()
i
iλ
λ=AI
V
                                                           (30.25)                                                           
 
Here we have used the fact from (30.24), that 
 
                                                                 ()    ( ())
ii
Kmλ=AV                                                     (30.26)                                                     
 
Then the form (24.5) reduces to the diagonal form (27.3)
1
 or (27.3)
2
. 
 
The condition that 
m
 has simple roots only turns out to be necessary for the existence of a 
diagonal matrix for A also, as we shall now see in the following theorem. 
 
Theorem 30.8.  An endomorphism A has a diagonal matrix if and only if its minimal polynomial 
can be factored into distinct factors all of the first degree. 
 
Proof.  Sufficiencyhas already been proven.  To prove necessity, assume that the matrix of A 
relative to some basis 
{}
1
,...,
N
ee
 has the form (27.3)
1
.  Then the polynomial 
 
1
()   ()   ()
L
mtttλλ=−⋅⋅⋅−                                                (30.27)                                                
 

188 Chap 6 • SPECTRAL DECOMPOSITIONS 
 
is the minimal polynomial of 
A.  Indeed, each basis vector 
i
e is contained in the null space 
()
k
Kλ−AI for one particular k.  Consequently (   (   ))
i
Km∈eA for all 1,...,iN=, and thus 
 
                                                                     (())Km
=AV                                                        (30.28)                                                        
 
or equivalently 
 
                                                                        ()m
=A0                                                            (30.29)                                                            
 
which implies that 
()m∈AP.  But since 
1
,,,
L
λλ are distinct, no proper divisor of m still belongs 
to     (   )A
P.  Therefore 
m
 is the minimal polynomial of A. 
 
As before, when the condition of the preceding theorem is satisfied, then we can define 
projections 
1
,...,
L
PP by 
 
 
1
()(),()( )
L
iiij
j
ji
RKλλ
=
≠
==⊕PPVV
                                    (30.30)                                    
 
for all 1,...,iL=, and the diagonal form (27.3)
1
 shows that 
A
 has the representation 
 
 
11
1
L
LLii
i
λλλ
=
=   +⋅⋅⋅+    =
∑
APPP                                            (30.31)                                            
 
It should be noted, however, that in stating this result we have not made use of any inner product, 
so it is not meaningful to say whether or not the projections 
1
,...,
L
PP are perpendicular;  further, the 
eigenvalues 
1
,...,
L
λλ are generally complex numbers.  In fact, the factorization (30.21) for m in 
general is possible only if the scalar field is algebraically closed, such as the complex field used 
here.  If the scalar field is the real field, we should define the factors 
1
,...,
L
mm of m to be powers 
of irreducible polynomials, i.e., polynomials having no proper divisors.  Then the decomposition 
(30.22) for 
V remains valid, since the argument of the proof is based entirely on the fact that the 
factors of  
m are pairwise prime. 
 
Theorem 30.8 shows that in order to know whether or not A has a diagonal form, we must 
know the roots and their multiplicities in the minimal polynomial 
m of A.  Now since the 

Sec. 30 •           Arbitrary           Endomorphisms                                                            189 
 
characteristic polynomial 
f of A belongs to     (   )AP, 
m
 is a divisor of f.  Hence the roots of 
m
 
are always roots of 
f,  The next theorem gives the converse of this result. 
Theorem 30.9.  Each eigenvalue of 
A
 is a root of the minimal polynomial m of 
A
 and vice 
versa. 
 
Proof.  Sufficiency has already been proved.  To prove necessity, let 
λ be an eigenvalue of A.  
Then we wish to show that the polynomial 
 
 
()gt    tλ=−                                                           (30.32)                                                           
 
is a divisor of 
m.  Since g is of the first degree, if g is not a divisor of m, then m and g are 
relatively prime.  By (30.17) and the fact that 
()m∈AP
, we have 
 
 
{}
(   (   ))(   (   ))(   (   ))(   (   ))KmKgKgKg=∩=∩=0A AA AV                        (30.33)                        
 
But this is impossible, since     (   (   ))KgA, being the characteristic subspace corresponding to the 
eigenvalue 
λ, cannot be of zero dimension. 
 
The preceding theorem justifies our using the same notations 
1
,...,
L
λλ for the (distinct) 
roots of the minimal polynomial 
m, as shown in (30.21).  However it should be noted that the root 
i
λ generally has a smaller multiplicity in m than in f, because m is a divisor of f.  The 
characteristic polynomial 
f yields not only the (distinct) roots 
1
,...,
L
λλ of m, it determines also 
the dimensions of their corresponding subspaces 
1
((   )),...,    ((   ))
L
KmKmAA in the decomposition 
(30.22)  This result is made explicit in the following theorem. 
 
Theorem 30.10.  Let 
k
d denote the algebraic multiplicity of the eigenvalue 
k
λ as before; i.e., 
k
d is 
the multiplicity of 
k
λ in f [cf. (26.8)].  Then we have 
 
                                                                dim    ((   ))
kk
Kmd=A                                                    (30.34)                                                    
 
Proof   We prove this result by induction.  Clearly, it is valid for all A having a diagonal form, 
since in this case (   )
kk
mλ=−AA  I, so that     ((   ))
k
KmA is the characteristic subspace corresponding 
to 
k
λ and its dimension is the geometric multiplicity as well as the algebraic multiplicity of 
k
λ.  
Now assuming that the result is valid for all A whose minimal polynomial has at most 
M multiple 
roots, where 
0M= is the starting induction hypothesis, we wish to show that the same holds for 

190 Chap 6 • SPECTRAL DECOMPOSITIONS 
 
all A whose minimal polynomial has 1
M+ multiple roots.  To see this, we make use of the 
decomposition (30.23) and, for definiteness, we assume that 
1
λ is a multiple root of m.  We put 
 
 
2
((   ))((   ))
L
KmKm=⊕⋅⋅⋅⊕AAU                                         (30.35)                                         
 
Then 
U is A=invariant.  From Theorem 30.7 we know that the minimal polynomial m
U
 of the 
restriction A
U
 is 
 
 
2L
mmm=⋅⋅⋅
U
                                                        (30.36)                                                        
 
which has at most 
M multiple roots.  Hence by the induction hypothesis we have 
 
 
22
dimdim    ((   ))dim    ((   ))
LL
KmKmdd=+⋅⋅⋅+=  +⋅⋅⋅+AAU                     (30.37)                     
 
But from (26.8) and (30.23) we have also 
 
 
12
12
dim    ((   ))    dim    ((   ))dim    ((   ))
L
L
Nd dd
NKmKmKm
=++⋅⋅⋅+
=++⋅⋅⋅+AAA
                      (30.38)                      
 
Comparing (30.38) with (30.37), we see that 
 
 
11
dim    ((   ))dKm=A                                                     (30.39)                                                     
 
Thus the result (30.34) is valid for all A whose minimal polynomial has 1
M+ multiple roots, and 
hence in general. 
 
An immediate consequence of the preceding theorem is the following. 
 
Theorem 30.11.  Let 
k
b be the geometric multiplicity of 
k
λ, namely 
 
                                                dim    (    )dim    ((   ))dim    ()
kk  kk
bKgKλλ≡≡   ≡−AAIV                            (30.40)                            
 
Then we have 

Sec. 30 •           Arbitrary           Endomorphisms                                                            191 
 
 
                                       11
kkk
bda≤≤−+                                                     (30.41)                                                     
 
where 
k
d is the algebraic multiplicity of 
k
λ and 
k
a is the multiplicity of  
k
λ in m,as shown in 
(30.21).  Further, 
kk
bd= if and only if 
 
 
2
(())   (())
kk
KgKg=AA                                                  (30.42)                                                  
 
Proof.    If 
k
λ is a simple root of 
m
, i.e., 1
k
a= and 
kk
gm=, then from (30.34) and (30.40) we 
have 
kk
bd=.  On the other hand, if 
k
λ is a multiple root of m, i.e., 1
k
a> and 
k
a
kk
mg=
, then the 
polynomials 
(1)
2
,,...,
k
a
kkk
ggg
−
 are proper divisors of 
k
m.  Hence by Theorem 30.3 
 
 
(1)
2
(    )((   ))((   ))((   ))((   ))
k
a
kkkkk
KgKgKgKmλ
−
=⊂⊂⋅⋅⋅⊂⊂AAA AV               (30.43)               
 
where the inclusions are strictly proper and the dimensions of the subspaces change by at least one 
in each inclusion.  Thus (30.41) holds. 
 
The second part of the theorem can be proved as follows: If 
k
λ is a simple root of m, then 
kk
mg=, and, thus 
2
((   ))((   ))(    )
kkk
KgKgλ==AAV
.  On the other hand, if 
k
λ is not a simple root 
of 
m, then 
k
m is at least of second degree.  In this case 
k
g and 
2
k
g are both divisors of m.  But 
since 
k
g is also a proper divisor of 
2
k
g, by Theorem 30.3,     ((   ))
k
KgA is strictly a proper subspace 
of 
2
(())
k
KgA, so that (30.42) cannot hold, and the proof is complete. 
 
The preceding three theorems show that for each eigenvalue 
k
λ of A, generally there are 
two nonzero A-invariant subspaces, namely, the eigenspace (    )
k
λV and the subspace     ((   ))
k
KmA.  
For definiteness, let us call the latter subspace the characteristic subspace corresponding to 
k
λ and 
denote it ‘by the more compact notation     (    )
k
λU.  Then (    )
k
λV is a subspace of     (    )
k
λU in general, 
and the two subspaces coincide if and only if 
k
λ is a simple root of m.  Since 
k
λ is the only 
eigenvalue of the restriction of A to     (    )
k
λU, by the Cayley-Hamilton theorem we have also 
 
                                                             ()   (())
k
d
kk
Kλλ=−AIU                                                 (30.44)                                                 
 

192 Chap 6 • SPECTRAL DECOMPOSITIONS 
 
where 
k
d is the algebraic multiplicity of 
k
λ, which is also the dimension of     (    )
k
λU.  Thus we can 
determine the characteristic subspace directly from the characteristic polynomial of  A by (30.44). 
 
Now if we define 
k
P to be the projection on     (    )
k
λU in the direction of the remaining 
(),
j
jkλ≠U, namely 
 
1
()   (),()()
L
kkkj
j
jk
RKλλ
=
≠
==⊕PPUU
                                         (30.45)                                         
 
and we define 
k
B to be 
kk
λ−AP on     (    )
k
λU and 0 on     (    ),
j
jkλ≠U, then A has the spectral 
decomposition by a direct sum 
 
 
1
()
L
jjj
j
λ
=
=+
∑
APB                                                     (30.46)                                                     
 
where 
 
 
2
1,...,
,  1
j
jj
jjjjj
a
jjj
jL
ad
⎫
=
⎪
⎪
===
⎬
⎪
=≤≤
⎪
⎭
PP
PBB PB
B0
                              (30.47)                              
 
 
,,1,...,
,
jk
jkkj
jk
jkjkL
⎫
=
⎪
==   ≠  =
⎬
⎪
=
⎭
PP0
PBB P0
BB0
                                  (30.48)                                  
 
In general, an endomorphism B satisfying the condition 
 
 
a
=B0                                                              (30.49)                                                              
 
for some power 
a is called nilpotent.  From (30.49) or from Theorem 30.9 the only eigenvalue of a 
nilpotent endomorphism is 
0
, and the lowest power a satisfying (30.49) is an integer    ,1aaN≤≤, 
such that 
N
t is the characteristic polynomial of B and 
a
t is the minimal polynomial of B.  In view 
of (30.47) we see that each endomorphism 
j
B in the decomposition (30.46) is nilpotent and can be 

Sec. 30 •           Arbitrary           Endomorphisms                                                            193 
 
regarded also as a nilpotent endomorphism on     (    )
j
λU.  In order to decompose A further from 
(30.46) we must determine a spectral decomposition for each 
j
B
.  This problem is solved in 
general as follows. 
 
First, we recall that in Exercise  26.8 we have defined a nilcylic endomorphism 
C to be a 
nilpotent endomorphism such that 
 
 
1
but
NN−
=≠C0C  0                                           (30.50)                                           
 
Where 
N
 is the dimension of the underlying vector space 
V
.  For such an endomorphism we can 
find a cyclic basis 
{}
1
,...,
N
ee
 which satisfies the conditions 
 
 
11
1211
,,...,
NN
NN
−−
−
===Ce 0    Ce e   Ce  e
                                   (30.51)                                   
 
or, equivalently 
 
 
12
1221
,,...,
NN
NNN
−−
−
===Ce  e   C e e   Ce  e                                   (30.52)                                   
 
so that the matrix of 
C takes the simple form (26.19).  Indeed, we can choose 
N
e to be any vector 
such that 
1N
N
−
≠Ce  0;  then the set 
{}
1
,...,
N
ee defined by (30.52) is linearly independent and thus 
forms a cyclic basis for 
C.  Nilcyclic endomorphisms constitute only a special class of nilpotent 
endomorphisms, but in some sense the former can be regarded as the building blocks for the latter.  
The result is made precise by the following theorem. 
 
Theorem 30.12.  Let B be a nonzero nilpotent endomorphism of 
V in general, say B satisfies the 
conditions 
 
 
1
but
aa−
=≠B0B  0
                                           (30.53)                                           
 
for some integer 
a between 1  and N.  Then there exists a direct sum decomposition for V: 
 
 
1M
=⊕⋅⋅⋅⊕VV    V                                                      (30.54)                                                      
 
and a corresponding direct sum decomposition for B (in the sense explained in Section 24): 

194 Chap 6 • SPECTRAL DECOMPOSITIONS 
 
 
 
1M
=+⋅⋅⋅+BBB                                                      (30.55)                                                      
 
such that each 
j
B is nilpotent and its restriction to 
j
V is nilcyclic.  The subspaces 
1
,...,
M
VV in the 
decomposition (30.54) are not unique, but their dimensions are unique and obey the following 
rules:  The maximum of the dimension of 
1
,...,
M
VV is equal to the integer 
a
 in (30.53); the number 
a
N of subspaces among 
1
,...,
M
VV having dimension 
a
 is given by 
 
 
1
dim    ()
a
a
NNK
−
=−B
                                                  (30.56)                                                  
 
More generally, the number 
b
N of subspaces among 
1
,...,
M
VV having dimensions greater than or 
equal to 
b is given by 
 
 
1
dim    ()    dim    ()
bb
b
NKK
−
=−BB                                           (30.57)                                           
 
for all 1,...,ba=.  In particular, when 
1b=
, 
1
N is equal to the integer M in (30.54), and (30.57) 
reduces to 
 
                                                                            dim    (   )MK
=B                                                        (30.58)                                                        
 
Proof.  We prove the theorem by induction on the dimension of 
V
.  Clearly, the theorem is valid 
for one-dimensional space since a nilpotent endomorphism there is simply the zero endomorphism 
which is nilcylic.  Assuming now the theorem is valid for vector spaces of dimension less than or 
equal to 
1N−, we shall prove that the same is valid for vector spaces of dimension N. 
 
Notice first if the integer 
a in (30.53) is equal to N, then B is nilcyclic and the assertion is 
trivially satisfied with 1
M=, so we can assume that 1aN<<.  By (30.53)
2
, there exists a vector 
a
∈eV such that 
1a
a
−
≠Be  0.  As in (30.52) we define 
 
 
1
11
,...,
a
aa
−
−
=Be  e   Be                                                    (30.59)                                                    
 
Then the set 
{}
1
,...
a
ee is linearly independent. We put 
1
V to ‘be the subspace generated by 
{}
1
,...
a
ee.  Then by definition 
1
dima=V, and the restriction of B on 
1
V is nilcyclic. 
 

Sec. 30 •           Arbitrary           Endomorphisms                                                            195 
 
Now recall that for any subspace of a vector space we can define a factor space (cf. Section 
11).  As usual we denote the factor space of 
V over 
1
V by 
1
VV.  From the result of Exercise 11.5 
and (30.59), we have 
 
 
1
dim1NaN=−< −VV                                               (30.60)                                               
 
Thus we can apply the theorem to the factor space 
1
VV.  For definiteness, let us use the notation 
of Section 11, namely, if ∈v
V, then v denotes the equivalence set of v in 
1
VV.  This notation 
means that the superimposed bar is the canonical projection from 
V to 
1
VV.  From (30.59) it is 
easy to see that 
1
V is B-invariant.  Hence if u and v belong to the same equivalence set, so do 
Bu andBv.  Therefore we can define an endomorphism 
B on the factor space 
1
VV, by 
 
 
=BvBv                                                             (30.61)                                                             
 
for all ∈v
V or equivalently for all 
1
∈vVV,  Applying  (30.60) repeatedly, we have also 
 
 
kk
=Bv   Bv                                                           (30.62)                                                           
 
 
for all integers 
k.  In particular, B is nilpotent and 
 
 
a
=B0                                                              (30.63)                                                              
 
By the induction hypothesis we can then find a direct sum decomposition of the form 
 
 
11p
=⊕⋅⋅⋅⊕VV UU
                                                    (30.64)                                                    
 
for the factor space 
1
VV and a corresponding direct sum decomposition 
 
 
1p
=+⋅⋅⋅BF    F
                                                         (30.65)                                                         
 

196 Chap 6 • SPECTRAL DECOMPOSITIONS 
 
for 
B.  In particular, there are cyclic bases in the subspaces 
1
,...,
p
UU
  for the nilcyclic 
endomorphisms which are the restrictions of 
1
,,,,,
p
FF to the corresponding subspaces.  For 
definiteness, let 
{}
1
,...,
b
ff be a cyclic basis in 
1
U, say 
 
 
1
11
,,...,
bb
bbbb
−
−
===Bf    0B  f    fBff
                                      (30.66)                                      
 
From (30.63), is necessarily less than or equal to 
a. 
 
From (30.66)
1
 and (30.62) we see that 
b
b
Bf belongs to 
1
V and, thus can be expressed as a 
linear combination of 
{}
1
,...,
a
ee, say 
 
 
1
1111
()
ba
baaaaa
ααα  αα
−
−
=    +⋅⋅⋅+    =+⋅⋅⋅++BfeeBBIe                         (30.67)                         
 
Now there are two possibilities: (i) 
b
b
=Bf    0
 or (ii) 
b
b
≠Bf    0
.  In case (i) we define as before 
 
 
1
11
,...,
b
bbb
−
−
==Bf  f   Bf  f                                                  (30.68)                                                  
 
Then 
{}
1
,...,
b
ff is a linearly independent set in V, and we put 
2
V to be the subspace generated by 
{}
1
,...,
b
ff
,  On the other hand, in case (ii) from (30.53) we see that b is strictly less than a;  
moreover, from (30.67) we have 
 
 
1
111
()
aa ab
baba    aabab
ααα α
−−
−+−+
=    =+⋅⋅⋅+=+⋅⋅⋅+0BfBB  eee
                    (30.69)                    
 
which implies 
 
 
1
0
aba
αα
−+
==⋅⋅⋅=  =                                                   (30.70)                                                   
 
or equivalently 
 
 
1
1
()
bab
baba
αα
−
−
=+⋅⋅⋅+BfBB  e                                           (30.71)                                           
 
Hence we can choose another vector     '
b
f in the same equivalence set of 
b
f by 

Sec. 30 •           Arbitrary           Endomorphisms                                                            197 
 
 
 
1
111
'()
ab
bbababbaba
αααα
−−
−+−
=−+⋅⋅⋅+=−   −⋅⋅⋅−ff   BIef  ee                     (30.72)                     
 
which now obeys the condition 
'
b
b
=Bf0
, and we can proceed in exactly the same way as in case 
(i).  Thus in any case every cyclic basis 
{}
1
,...,
b
ff
 for 
1
U gives rise to a cyclic set 
{}
1
,...,
b
ff in V. 
 
Applying this result to each one of the subspaces 
1
,...,
p
UU
 we obtain cyclic sets 
{}
1
,...,
b
ff, 
{}
1
,...,
c
gg
,..., and subspaces 
21
,...,
p+
VV generated by them in V.  Now it is clear that the union of 
{}{}{}
111
,...,,      ,...,,,...,
ab c
ee ffgg,..., form a basis of 
V
 since from (30.59), (30.60), and (30.64) 
there are precisely N vectors in the union; further, if we have 
 
 
111  11 1
 
aabbc c
ααβ   βγ   γ+⋅⋅⋅+    +    +⋅⋅⋅+    +   +⋅⋅⋅+    +⋅⋅⋅=eeffgg0                     (30.73)                     
 
then taking the canonical projection to 
1
VV yields 
 
 
1111
 
bbc  c
ββγγ+⋅⋅⋅+    +   +⋅⋅⋅+    +⋅⋅⋅=ffgg0
 
 
which implies 
 
 
11
 =
bc
ββγ   γ⋅⋅⋅=   =  =⋅⋅⋅=  =⋅⋅⋅=0 
 
and substituting this result back into (30.73) yields 
 
 
1
0
a
αα=⋅⋅⋅=   = 
 
Thus 
Vhas a direct sum decomposition given by (30.54) with 1Mp=+ and B has a 
corresponding decomposition given by (30.55) where 
1
,...,
M
BB have the prescribed properties. 
 
Now the only assertion yet to be proved is equation (30.57).  This result follows from the 
general rule that for any nilcylic endomorphism 
C on a L-dimensional space we have 
 

198 Chap 6 • SPECTRAL DECOMPOSITIONS 
 
 
1
1for  1
dim    ()    dim()
0for  
kk
kL
K
kL
−
≤≤
⎧
−=
⎨
>
⎩
CC 
 
Applying this rule to the restriction of 
j
B to V for all 1,...,jM= and using the fact that the 
kernel of 
k
B is equal to the direct sum of the kernel of the restriction of ()
k
j
B for all 1,...,jM=, 
prove easily that (30.57) holds.  Thus the proof is complete. 
 
In general, we cannot expect the subspaces 
1
,...,
M
VV in the decomposition (30.54) to be 
unique.  Indeed, if there are two subspaces among 
1
,...,
M
VV having the same dimension, say 
12
dimdima==VV, then we can decompose the direct sum 
12
⊕VV in many other ways, e.g., 
 
 
12
12
⊕= ⊕VVV V                                                     (30.74)                                                     
 
and when we substitute (30.74) into (30.54) the new decomposition 
 
 
12
3
N
=⊕⊕⊕⋅⋅⋅⊕VV V VV 
 
possesses exactly the same properties as the original decomposition (30.54).  For instance we can 
define 
1
V and 
2
V to be the subspaces generated by the linearly independent cyclic set 
{}
1
,...,
a
ee

 
and 
{}
1
,...,
a
ff

, where we choose the starting vectors 
a
e

 and 
a
f

 by 
 
 
,
aaaaaa
αβγδ=+=+eeffef


 
 
provided that the coefficient matrix on the right hand side is nonsingular. 
 
If we apply the preceding theorem to the restriction of 
k
λ−AI on 
k
U, we see that the 
inequality (30.41) is the best possible one in general.  Indeed, (30.41)
2
 becomes an equality if and 
only if 
k
U has the decomposition 
 
 
1kkkM
=⊕⋅⋅⋅⊕UUU                                                     (30.75)                                                     
 
where the dimensions of the subspaces 
1
,...,
kkM
UU are 
 

Sec. 30 •           Arbitrary           Endomorphisms                                                            199 
 
 
12
dim,dimdim1
kkkkM
a==⋅⋅⋅==UUU 
 
If there are more than one subspaces among 
1
,...,
kkM
UU having dimension greater than one, then 
(30.41)
2
 is a strict inequality. 
 
The matrix of the restriction of 
k
λ−AI to 
k
U relative to the union of the cyclic basis for 
1
,...,
kkM
UU has the form 
 
 
1
2
0
0
k
k
k
kM
A
A
A
A
⎡⎤
⎢⎥
⎢⎥
⋅
⎢⎥
=
⎢⎥
⋅
⎢⎥
⎢⎥
⋅
⎢⎥
⎣⎦
                                          (30.76)                                          
 
where each submatrix 
kj
A in the diagonal of 
k
A has the form 
 
 
10
1
1
0
k
k
kj
k
A
λ
λ
λ
⎡⎤
⎢⎥
⎢⎥
⋅
⎢⎥
=
⎢⎥
⋅
⎢⎥
⎢⎥
⋅
⎢⎥
⎣⎦
                                            (30.77)                                            
 
Substituting (30.76) into (24.5) yields the Jordan normal form for A: 
 

200 Chap 6 • SPECTRAL DECOMPOSITIONS 
 
 
11
12
2
0
0
()0
0
M
A
A
M
A
A
⎡⎤
⎢⎥
⎢⎥
⋅
⎢⎥
⎢⎥
⋅
⎢⎥
⎢⎥
⋅
⎢⎥
=
⎢⎥
⎢⎥
⎢⎥
⋅
⎢⎥
⎢⎥
⋅
⎢⎥
⋅
⎢⎥
⎢⎥
⎣⎦
A
                        (30.78)                        
 
The Jordan normal form is an important result since it gives a geometric interpretation of an 
arbitrary endomorphism of a vector space.  In general, we say that two endomorphisms A and 'A 
are similar if the matrix of A relative to a basis is identical to the matrix of 'A relative to another 
basis.  From the transformation law (22.7), we see that A and 'A are similar if and only if there 
exists a nonsingular endomorphism T such that 
 
 
1
' 
−
=ATAT                                                         (30.79)                                                         
 
Clearly, (30.79) defines an equivalence relation on 
(; )LVV.  We call the equivalence sets relative 
to (30.79) the conjugate subsets of 
(; )LVV.  Now for each (; )∈ALVV the Jordan normal form 
of A is a particular matrix of A and is unique to within an arbitrary change of ordering of the 
various square blocks on the diagonal of the matrix.  Hence A and 'Aare similar if and only if 
they have the same Jordan normal form.  Thus the Jordan normal form characterizes the conjugate 
subsets of 
(; )LVV 
 
 
Exercises 
 
30.1     Prove Theorem 30.5. 
30.2 Prove the general case of Theorem 30.7. 
30.3     Let     U be an unitary endomorphism of an inner product space 
V.  Show that U has a 
diagonal form. 
30.4     If     
V is a real inner product space, show that an orthogonal endomorphism Q in general 
does not have a diagonal form, but it has the spectral form 

Sec. 30 •           Arbitrary           Endomorphisms                                                            201 
 
 
 
11
11
1
10
1
()
1
cossin
0sincos
cossin
sincos
LL
LL
M
θθ
θθ
θθ
θθ
⎡⎤
⎢⎥
⋅
⎢⎥
⋅
⎢⎥
⎢⎥
⋅
⎢⎥
⎢⎥
⎢⎥
−
⎢⎥
⎢⎥
⋅
⎢⎥
⋅
⎢⎥
=
⎢⎥
⋅
⎢⎥
−
⎢⎥
⎢⎥
−
⎢⎥
⎢⎥
⎢⎥
⋅
⎢⎥
⋅
⎢⎥
⎢⎥
−
⎢⎥
⎢⎥
⎣⎦
Q
 
 
where the angles 
1
,...,
L
θθ may or may not be distinct. 
30.5 Determine whether the endomorphism A whose matrix relative to a certain basis is 
 
 
110
()   0 1 0
011
M
⎡⎤
⎢⎥
=
⎢⎥
−
⎢⎥
⎣⎦
A
 
 
can have a diagonal matrix relative to another basis.  Does the result depend on whether the 
scalar field is real or complex? 
30.6
 Determine the Jordan normal form for the endomorphism 
A
 whose matrix relative to a 
certain basis is 
 
 
111
()   0 11
001
M
⎡⎤
⎢⎥
=
⎢⎥
⎢⎥
⎣⎦
A
 

 
 
 

 203 
 
Chapter 7 
 
 
TENSOR ALGEBRA 
 
 
The concept of a tensor is of major importance in applied mathematics.  Virtually every 
discipline in the physical sciences makes some use of tensors.  Admittedly, one does not always 
need to spend a lot of time and effort to gain a computational facility with tensors as they are 
used in the applications.  However, we take the position that a better understanding of tensors is 
obtained if we follow a systematic approach, using the language of finite-dimensional vector 
spaces.  We begin with a brief discussion of linear functions on a vector space. Since in the 
applications the scalar field is usually the real field, from now on we shall consider real vector 
spaces only. 
 
Section 31.  Linear Functions, the Dual Space 
 
Let V be a real vector space of dimension .N  We consider the space of linear functions 
()
;LV R from 
V
 into the real numbers .R  By Theorem 16.1, 
()
dim;dim=N=LV RV.  
Thus Vand 
()
;LVR
are isomorphic.  We call 
()
;LVR
 the dual Space of ,V and we denote it 
by the special notation 
∗
V.  To distinguish elements of V from those of 
∗
V, we shall call the 
former elements vectors and the latter elements covectors.  However, these two names are 
strictly relative to each other.  Since 
∗
V is a N-dimensional vector space by itself, we can apply 
any result valid for a vector space in general to 
∗
V as well as to 
.V
  In fact, we can even define 
a dual space 
()
∗
∗
V for 
∗
V just as we define a dual space 
∗
V for .V  In order not to introduce 
too many new concepts at the same time, we shall postpone the second dual space
()
∗
∗
V until the 
next section.  Hence in this section 
V shall be a given N-dimensional space and 
∗
V shall denote 
its dual space.  As usual, we denote typical elements of 
Vby ,, ,...uvw.  Then the typical 
elements of 
∗
Vare denoted by ,,,...
** *
uvw.  However, it should be noted that the asterisk here is 
strictly a convenient notation, not a symbol for a function from 
Vto 
∗
V.  Thus 
∗
u is not related 
in any particular way to 
.u  Also, for some covectors, such as those that constitute a dual basis to 
be defined shortly, this mutation becomes rather cumbersome.  In such cases, the notation is 
simply abandoned.  For instance, without fear of ambiguity we denote the null covector in 
∗
V by 
the same notation as the null vector in     ,
V namely   ,0 instead of 
∗
0
. 
If              ,
∗∗
∈vV then 
∗
v
 is a linear function fromV to R, i.e., 
 
:
∗
→vVR
 

204  Chap. 7 • TENSOR ALGEBRA 
 
such that for any vectors    ,   ,∈uv
V and scalars    ,   ,αβ∈R 
 
()()()
αβ α   β
∗∗∗
+=  +vu v  vu  vv 
 
Of course, the linear operations on the right hand side are those of 
R while those on the left hand 
side are linear operations in 
.V  For a reason the will become apparent later, it is more 
convenient to denote the value of 
∗
v at v by the notation ,
∗
vv.  Then the bracket 
 , 
 
operation can be viewed as a function 
 
 ,   :
∗
×→VV  R 
 
It is easy to verify that this operation has the following properties: 
 
 
*
(),,,
(),,,
(    )   For any given    ,, vanishes for all  if and only if   *.
(   )   Similarly, for any given   ,, vanishes for all   * if and only if .
i
ii
iii
iv
αβ  α   β
αβ α    β
∗∗∗∗
∗∗∗
∗∗
∗
+= +
+=   +
∈=
∈=
vuv   vv  uv
vu vvuvv
vvvvv 0
vvvvv 0
V
V
(31.1) 
 
The first two properties define 
,
 to be a bilinear operation on 
∗
×VV, and the last two 
properties define 
, to be a definite operation.  These properties resemble the properties of an 
inner product, so that we call the operation 
, the scalar product.  As we shall see, we can 
define many concepts associated with the scalar product similar to corresponding concepts 
associated with an inner product.  The first example is the concept of the 
dual basis, which is the 
counterpart of the concept of the reciprocal basis. 
 
If 
{}
1
,
N
ee... is a basis for V, we define the dual basis to be a basis 
{}
1
,
N
ee...for 
∗
Vsuch that  
 
 
,
jj
ii
δ=ee                                                          (31.2)                                                          
 

Sec. 31 • Linear Functions, the Dual Space 205 
for all   ,1,,    .
ijN=...  The reader should compare this condition with the condition (14.1) that 
defines the reciprocal basis.  By exactly the same argument as before we can prove the following 
theorem. 
 
Theorem 31.1.  The dual basis relative to a given basis exists and is unique. 
 
Notice that we have dropped the asterisk notation for the covector 
j
e in a dual basis; the 
superscript alone is enough to distinguish 
{}
1
,
N
ee... from  
{}
1
,.
N
ee...  However, it should be 
kept in mind that, unlike the reciprocal basis, the dual basis is a basis for ,
∗
Vnot a basis for .V  
In particular, it makes no sense to require a basis be the same as its dual basis.  This means the 
component form of a vector 
∈vV relative to a basis 
{}
1
,
N
ee...
, 
 
 
i
i
v=ve
                                                              (31.3)                                                              
 
must never be confused with the component form of a covector 
∗∗
∈vV relative to the dual 
basis, 
 
 
i
i
v
∗
=ve                                                             (31.4)                                                             
 
In order to emphasize the difference of these two component forms, we call 
i
v
 the contravariant 
components
 of vand 
i
v the covariant components of .
∗
v  A vector has contravariant 
components only and a covector has covariant components only.  The terminology for the 
components is not inconsistent with the same terminology defined earlier for an inner product 
space, since we have the following theorem. 
 
Theorem 31.2.  Given any inner product on     ,V there exists a unique isomorphism  
 
 :
∗
→GVV                                                         (31.5)                                                         
 
which is induced by the inner product in such a way that 
 
 
,,,=⋅∈Gv wv  wv wV                                            (31.6)                                            
 

206  Chap. 7 • TENSOR ALGEBRA 
Under this isomorphism the image of any orthonormal basis 
{}
1
,,
N
ii... is the dual basis 
{}
1
,,
N
ii...
, namely 
 
 
,1,,
k
k
kN==Gii...                                                 (31.7)                                                 
 
and, more generally, if 
{}
1
,
N
ee... is an arbitrary basis, then the image of its reciprocal basis 
{}
1
,
N
ee..., is the dual basis 
{}
1
,,
N
ee... namely 
 
 
,1,,
kk
kN==Gee...                                               (31.8)                                               
 
Proof.  Since we now consider only real vector spaces and real inner product spaces, the right-
hand side of (31.6), clearly, is a linear function of 
w for each .∈vV  Thus G is well defined by 
the condition (31.6).  We must show that 
Gis an isomorphism.  The fact that G is a linear 
transformation is obvious, since the right-hand side of (31.6) is linear in 
v for each .∈wV  
Also, 
G is one-to-one because, from (31.6), if ,=Gu    Gv then ⋅=⋅uw  vwfor all w and thus 
.=uv  Now since we already know that  dimdim
∗
=VV, any one-to-one linear transformation 
from 
V
 to 
∗
V is necessarily onto and hence an isomorphism.  The proof of (31.8) is obvious, 
since by the definition of the reciprocal basis we have 
 
 
,,1,,
ii
jj
ijNδ⋅==ee... 
 
and by the definition of the dual basis we have 
 
 
,,,1,,
ii
jj
ijNδ==ee... 
 
Comparing these definitions with (31.6), we obtain 
 
 
,,, 1,,
ii
jj
ijN===Geee  e... 
 
which implies (31.8) because 
{}
1
,
N
ee...
is a basis of .V 
 

Sec. 31 • Linear Functions, the Dual Space 207 
Because of this theorem, if a particular inner product is assigned on     ,
V then we can 
identify 
Vwith
∗
V by suppressing the notation for the isomorphisms 
G
and 
1
.
−
G  In other 
words, we regard a vector
v also as a linear function on :V 
 
 
,≡⋅vwv w
                                                         (31.9)                                                         
 
According to this rule the reciprocal basis is identified with the dual basis and the inner product 
becomes the scalar product.  However, since a vector space can be equipped with many inner 
products, unless a particular inner product is chosen, we cannot identify 
V
 with 
∗
V in general. 
In this section, we shall not assign any particular inner product in     ,
Vso V and 
∗
Vare different 
vector spaces. 
 
We shall now derive some formulas which generalize the results of an inner product 
space to a vector space in general.  First, if
∈vV and 
∗∗
∈vV
 are arbitrary, then their scalar 
products 
,
∗
vv can be computed in component form as follows:  Choose a basis 
{}
i
e and its 
dual basis 
{}
i
efor V and ,
∗
V respectively, so that we can express    and
∗
vv in component form 
(31.3) and (31.4).  Then from (31.1) and (31.2) we have 
 
 
,,,
ijjijii
iji    jiji
vvvvvvvvδ
∗
== ==vve   eee                             (31.10)                             
 
which generalizes the formula (14.16).  Applying (31.10) to ,
i∗
=ve we obtain 
 
 
,
ii
v=ev
                                                          (31.11)                                                          
 
which generalizes the formula (14.15)
1
; similarly applying (31.10) to ,
j
=ve we obtain 
 
 
,
ii
v
∗
=ve                                                         (31.12)                                                         
 
which generalizes the formula (14.15)
2
 
 

208  Chap. 7 • TENSOR ALGEBRA 
Next recall that for inner product spaces 
Vand Uwe define the adjoint 
∗
Aof a linear 
transformation 
:→AVU
 to be a linear transformation :
∗
→AUVsuch that the following 
condition [cf.(18.1)] is satisfied: 
 
                                                      ,,
∗
⋅= ⋅∈  ∈uAv  AuvuvUV 
 
If we do not make use of any inner product, we simply replace this condition by  
 
 
,,, ,
∗∗∗ ∗∗
=∈∈uAvAuvuvUV                               (31.13)                               
 
then 
∗
A is a linear transformation from to,
∗∗
UV 
 
 
:
∗∗∗
→AUV 
 
and is called the 
dual of .A  By the same argument as before we can prove the following 
theorem. 
 
Theorem 31.3
.  For every linear transformation :→AVU there exists a unique dual 
:
∗∗   ∗
→AUV satisfying the condition (31.13). 
 
If we choose a basis 
{}
1
,
N
ee... for 
V
and a basis 
{}
1
,,
M
bb... for 
U
 and express the 
linear transformation 
A by (18.2) and the linear transformation  
∗
A by (18.3), where 
{}
α
b and 
{}
k
e are now regarded as the dual bases of 
{}
α
b and 
{}
,
k
e respectively, then (18.4) remains 
valid in the more general context, except that we now have 
 
 
kk
AA
αα∗
=                                                         (31.14)                                                         
 
since we no longer consider complex spaces.  Of course, the formulas (18.5) and (18.6) are now 
replaced by 
 
 
,
kk
A
αα
=bAe
                                                     (31.15)                                                     
 

Sec. 31 • Linear Functions, the Dual Space 209 
and 
 
 
,
kk
A
αα∗∗
=Ab  e                                                    (31.16)                                                    
 
respectively. 
 
For an inner product space the orthogonal complement of a subspace     of
UV is a 
subspace 
⊥
U given by [cf.(13.2)] 
 
 
{}
0forall
⊥
=⋅=∈vu vuUU 
 
By the same token, if 
V is a vector space in general, then we define the orthogonal complement 
of 
U to be the subspace of
⊥∗
UVgiven by 
 
 
{}
,0forall
⊥∗∗
== ∈vvuuUU
                                      (31.17)                                      
 
In general if 
∈vVand 
∗∗
∈vV
 are arbitrary, then    and
∗
vv are said to be orthogonal  to each 
other if 
,0.
∗
=vv  We can prove the following theorem by the same argument used previously 
for inner product spaces. 
Theorem 31.4.  If U is a subspace of ,V then 
 
                                                          dimdimdim
⊥
+=UUV                                              (31.18)                                              
 
However, 
V
 is no longer the direct sum of     and
⊥
UUsince 
⊥
U is a subspace of ,
∗
V not a 
subspace of 
.V 
 
Using the same line of reasoning, we can generalize Theorem 18.3 to the following. 
 
Theorem 31.5
.  If 
:→AVU
 is a linear transformation, then 
 

210  Chap. 7 • TENSOR ALGEBRA 
 
()
()
KR
⊥
∗
=AA                                                     (31.19)                                                     
 
and 
 
 
()
()
KR
⊥
∗
=AA                                                     (31.20)                                                     
 
Similarly, we can generalize Theorem 18.4 to the following. 
 
Theorem 31.6
.  Any linear transformation Aand its dual 
∗
Ahave the same rank. 
 
 Finally, formulas for transferring from one basis to another basis can be generalized from 
inner product spaces to vector spaces in general.  If 
{}
1
,
N
ee...
and 
{}
1
ˆˆ
,
N
ee...
are bases for     ,V 
then as before we can express one basis in component form relative to another, as shown by 
(14.17) and (14.18).  Now suppose that 
{}
1
,
N
ee...and 
{}
1
ˆˆ
,
N
ee...are the dual bases of 
{}
1
,
N
ee... 
and
{}
1
ˆˆ
,
N
ee...
, respectively.  Then it can be verified easily that  
 
 
ˆ
ˆˆ
,
qqkqqk
kk
TT==eeee                                               (31.21)                                               
 
where 
q
k
T
and 
ˆ
q
k
T are defined by (14.17) and (14.18), i.e., 
 
 
ˆ
ˆˆ
,
qq
kkqkkq
TT==eeee                                               (31.22)                                               
 
From these relations if 
∈vV and 
∗∗
∈vV have the component forms (31.3) and (31.4) relative 
to 
{}
i
eand 
{}
i
eand the component forms 
 
 
ˆˆ
i
i
υ=ve                                                            (31.23)                                                            
 
and 
 
 
ˆˆ
i
i
υ
∗
=ve                                                           (31.24)                                                           
 
relative to 
{}
ˆ
i
eand 
{}
ˆ
,
i
erespectively, then we have the following transformation laws: 
 
 
ˆ
k
qqk
Tυυ=                                                          (31.25)                                                          
 

Sec. 31 • Linear Functions, the Dual Space 211 
 
ˆ
ˆ
qqk
k
Tυυ=                                                          (31.26)                                                          
 
 
ˆˆ
kkq
q
Tυυ=
                                                          (31.27)                                                          
 
and 
 
 
ˆ
ˆ
q
kkq
Tυυ=                                                          (31.28)                                                          
 
which generalize the formulas (14.24)-(14.27), respectively. 
 
 
Exercises 
 
31.1
 If 
:→AVU
 and :→BUWare linear transformations, show that 
 
()
∗
∗∗
=BAA B   
 
31.2 If :→AVUand     :→BVUare linear transformations, show that  
 
()
αβαβ
∗
∗∗
+=+AB   A B
 
 
and that 
 
∗
=⇔=A0 A0 
 
These two conditions mean that the operation of taking the dual is an isomorphism 
()
()
:;;
∗∗
∗→LVULU V. 
31.3 
:→AVUis an isomorphism, show that 
:
∗∗∗
→AUV
is also an isomorphism; 
moreover, 
 
()()
1
1
∗−
−∗
=AA 
 
31.4     If     
Vhas the decomposition 
12
=⊕VUU and if :→PVV is the projection of Von 
1
Ualong 
2
,Ushow that 
∗
Vhas the decomposition 
12
∗⊥⊥
=⊕VUUand that   :
∗∗   ∗
→PVVis 
the projection of 
∗
Von 
2
⊥
Ualong 
1
.
⊥
U 
31.5     If     
12
andUUare subspaces of 
,V
show that 
 
()()
121    21 21   2
and
⊥
⊥⊥⊥⊥⊥
+==+UUU    UU UU   U∩∩
 

212  Chap. 7 • TENSOR ALGEBRA 
 
31.6
 Show that the linear transformation :
∗
→GVVdefined in Theorem 31.2 obeys the 
condition 
 
,,=Gv wGw  v 
 
31.7     Show that an inner product on 
∗
Vis induced by an inner product on Vby the formula 
 
 
11∗∗−∗−∗
⋅≡  ⋅vw  GvGw                                                (31.29)                                                
 
where
G
is the isomorphism defined in Theorem 31.2. 
31.8     If     
{}
i
eis a basis for Vand 
{}
i
eis its dual basis in ,
∗
Vshow that 
{}
i
Geis the reciprocal 
basis of 
{}
i
ewith respect to the inner product on 
∗
Vdefined by (31.29) in the preceding 
exercise. 

Sec. 32            •            Second Dual Space, Canonical Isomorphism 213 
 
Section 32.  The Second Dual space, canonical Isomorphisms  
 
In the preceding section we defined the dual space 
∗
Vof any vector space 
V
to be the 
space of linear functions 
()
;LVRfrom Vto .R  By the same procedure we can define the dual 
space 
()
of
∗
∗∗
VVby 
 
 
()()
()
()
;;;
∗
∗∗
==VLVRLLVRR                                     (32.1)                                     
 
For simplicity let us denote this space by 
∗∗
V, called the second dual space of .V Of course, the 
dimension of 
∗∗
Vis the same as that of     ,Vnamely 
 
                                                         dimdimdim
∗∗∗
==VV  V                                              (32.2)                                              
 
Using the system of notation introduced in the preceding section, we write a typical element of 
∗∗
V by 
.
∗∗
v
  Then 
∗∗
v
is a linear function on 
∗
V 
 
 :
∗∗∗
→vVR 
 
Further, for each 
∗∗
∈vV we denote the value of 
∗∗
Vat 
∗
vby ,.
∗∗   ∗
vv The ,operation is now 
a mapping from 
∗∗∗
×VVto Rand possesses the same four properties given in the preceding 
section. 
 
Unlike the dual space ,
∗
Vthe second dual space 
∗∗
Vcan always be identified as 
Vwithout using any inner product. The isomorphism 
 
 :
∗∗
→JVV                                                         (32.3)                                                         
 
is defined by the condition 
 
 
,,,  ,  
∗∗∗∗
=∈∈Jv  vv   vvvVV                                    (32.4)                                    
 
Clearly, 
J
is well defined by (32.4) since for each 
∈vV
the right-hand side of (32.4) is a linear 
function of .
∗
v  To see that Jis an isomorphism, we notice first that Jis a linear transformation, 
because for each 
∗∗
∈vVthe right-hand side is linear in .v  Now Jalso one-to-one, since if 
,
=JvJuthen (32.4) implies that  
 
 
,,,
∗∗∗∗
=∈vvvuvV                                             (32.5)                                             

214  Chap. 7 • TENSOR ALGEBRA 
 
which then implies 
.=uv  From (32.2), we conclude that the one-to-one linear transformation J 
is onto, and thus 
J is an isomorphism.  We summarize this result in the following theorem. 
 
Theorem 32.1.  There exists a unique isomorphism J from to
∗∗
VVsatisfying the condition 
(32.4). 
 
Since the isomorphism
Jis defined without using any structure in addition to the vector 
space structure on     ,
Vits notation can often be suppressed without any ambiguity.  We shall 
adopt such a convention here and identify any 
∈vVas a linear function on 
∗
V 
 
:
∗
→vVR
 
 
by the condition that defines 
J, namely  
 
 
,,,forall
∗∗∗∗
=∈vvv vvV                                        (32.6)                                        
 
In doing so, we allow the same symbol 
v to represent two different objects: an element of the 
vector space 
Vand a linear function on the vector space ,
∗
Vand the two objects are related to 
each other through the condition (32.6). 
 
To distinguish an isomorphism such as 
,Jwhose notation may be suppressed without 
causing any ambiguity, from an isomorphism such as
,G
defined by (31.6), whose notation may 
not be suppressed, because there are many isomorphisms of similar nature, we call the former 
isomorphism a 
canonical or natural isomorphism.  Whether or not an isomorphism is canonical 
is usually determined by a convention, not by any axioms.  A general rule for choosing a 
canonical isomorphism is that the isomorphism must be defined without using any additional 
structure other than the basic structure already assigned to the underlying spaces; further, by 
suppressing the notation of the canonical isomorphism no ambiguity is likely to arise.  Hence the 
choice of a canonical isomorphism depends on the basic structure of the vector spaces.  If we 
deal with inner product spaces equipped with particular inner products, the isomorphism 
Gcan 
safely be regarded as canonical, and by choosing 
Gto be canonical, we can achieve much 
economy in writing.  On the other hand, if we consider vector spaces without any pre-assigned 
inner product, then we cannot make all possible isomorphisms 
Gcanonical, otherwise the 
notation becomes ambiguous. 
 
It should be noticed that not every isomorphism whose definition depends only on the 
basis structure of the underlying space can be made canonical.  For example, the operation of 
taking the dual: 
 
()
()
:;;
∗∗
∗→LVULU V 
 
 

Sec. 32            •            Second Dual Space, Canonical Isomorphism 215 
is defined by using the vector space structure of and 
VUonly.  However, by suppressing the 
notation 
,∗ we encounter immediately much ambiguity, especially when Uis equal to 
.V
 
Surely, we do not wish to make every endomorphism 
:→AVVself-adjoint!  Another example 
will illustrate the point even clearer.  The operation of taking the opposite vector of any vector is 
an isomorphism 
 
:−→VV 
 
which is defined by using the vector space structure alone.  Evidently, we cannot suppress the 
minus sign without any ambiguity. 
 
To test whether the isomorphism 
Jcan be made a canonical one without any ambiguity, 
we consider the effect of this choice on the notations for the dual basis and the dual of a linear 
transformation.  Of course we wish to have 
{}
,
i
ewhen considered as a basis for ,
∗∗
Vto be the 
dual basis of 
{}
,
i
eand     ,Awhen considered as a linear transformation from 
∗∗
V to 
∗∗
U, to be the 
dual of 
.
∗
A
  These results are indeed correct and they are contained in the following. 
 
Theorem 32.2.  Given any basis 
{}
i
efor 
,V
then the dual basis of its dual basis 
{}
i
e
is 
{}
.
i
Je 
 
Proof.  This result is more or less obvious. By definition, the basis 
{}
i
eand its dual basis 
{}
i
eare 
related by 
 
 
,
ii
jj
δ=ee 
 
for   ,1,,    .
ijN=...  From (32.4) we have 
 
 
,,
ii
jj
=Je   ee  e 
 
Comparing the preceding two equations, we see that 
 
 
,
ii
jj
δ=Je   e 
 
which means that 
{}
1
,,
N
JeJe...
 is the dual basis of 
{}
1
,.
N
ee... 
 
Because of this theorem, after suppressing the notation for 
J we say that 
{}
{}
and
j
i
ee
are 
dual relative to each other.  The next theorem shows that the relation holds between and.
∗
AA 
 

216  Chap. 7 • TENSOR ALGEBRA 
Theorem 32.3.  Given any linear transformation :,→AVUthe dual of its dual 
∗
A is 
-1
JAJ
UV
.  
Here 
J
V
denotes the isomorphism from Vto 
∗∗
Vdefined by (32.4) and J
U
denotes the 
isomorphism from to
∗∗
UUdefined by a similar condition. 
 
Proof.  By  definition  Aand 
∗
Aare related by (31.13) 
 
,,
∗∗∗
=uAvAuv 
 
for all ,.
∗∗
∈∈uvUV  From (32.4) we have 
 
,,,,,
∗∗∗∗∗∗
==uAvJAvuAuvJvAu
UV
 
 
Comparing the preceding three equations, we see that 
 
 
()
-1
,,
∗∗∗
=JAJ  Jv uJvAu
UVVV
 
 
Since 
J
V
is an isomorphism, we can rewrite the last equation as 
 
 
-1
,,
∗∗∗∗∗ ∗∗
=JAJv  uv  Au
UV
 
 
Because 
∗∗∗∗
∈vVand 
∗∗
∈uUare arbitrary, it follows that 
-1
JAJ
UV
is the dual of .
∗
A  So if we 
suppress the notations for and,
JJ
UV
then     and
∗
AAare the duals relative to each other. 
 
A similar result exists for the operation of taking the orthogonal complement of a 
subspace; we have the following result. 
 
Theorem 32.4.
  Given any subspace of,UVthe orthogonal complement of its orthogonal 
complement 
⊥
U is 
()
JU
. 
 
We leave the proof of this theorem as an exercise.  Because of this theorem we say that 
and
⊥
UUare orthogonal to each other.  As we shall see in the next few sections, the use of 
canonical isomorphisms, like the summation convention, is an important device to achieve 
economy in writing.  We shall make use of this device whenever possible, so the reader should 
be prepared to allow one symbol to represent two or more different objects. 
 
The last three theorems show clearly the advantage of making 
J
a canonical 
isomorphism, so from now on we shall suppress the symbol for
J.  In general, if an isomorphism 
from a vector space 
Vto a vector space Uis chosen to be canonical, then we write 
 

Sec. 32            •            Second Dual Space, Canonical Isomorphism 217 
 
≅VU                                                              (32.7)                                                              
 
In particular, we have 
 
 
∗∗
≅VV                                                             (32.8)                                                             
 
 
Exercises 
 
32.1     Prove theorem 32.4. 
32.2     Show that by making 
Ja canonical isomorphism essentially we have identified V with 
,,,
∗∗∗∗∗∗
VV... and 
∗
V with ,,
∗∗∗∗∗∗∗
VV....  So a symbol    or,
∗
vvin fact, represents 
infinitely many objects. 

218  Chap. 7 • TENSOR ALGEBRA 
 
Section 33.  Multilinear Functions, Tensors 
 
In Section 31 we discussed the concept of a linear function and the concept of a bilinear 
function.  These concepts can be generalized in an obvious way to 
multilinear functions.  In 
general if 
1
,,,
s
...VVis a collection of vector spaces, then a s-linear function is a function 
 
 
1
:
s
××→AVVR"                                                    (33.1)                                                    
 
that is linear in each of its variables while the other variables are held constant.  If the vector 
spaces 
1
,,,
s
...VVare the vector space Vor its dual space ,
∗
V then Ais called a tensor on .V  
More specifically, a 
tensor of order (p, q) on     ,Vwhere     andpqare positive integers, is a (p+q)-
linear function 
 
 
timestimes
**
pq
×××××→VVVVR""
	
	

                                           (33.2)                                           
 
We shall extend this definition to the case 0
pq== and define a tensor of order 
()
0, 0to be a 
scalar in 
.R A tensor of order 
()
,0pis a pure contravariant tensor of order
p
and a tensor of 
order 
()
0,q
is a pure covariant tensor of order .q  In particular, a vector ∈vVis a pure 
contravariant tensor of order one.  This terminology, of course, is defined relative to a given 
vector space 
Vas we have explained in Section 31.  If a tensor is not a pure contravariant tensor 
or a pure covariant tensor, then it is a mixed tensor, and for a mixed tensor of order 
()
,,pq  pis 
the contravariant order and q is the covariant order. 
 
For definiteness, we denote the set of all tensors of order 
()
,pqon 
V
by the symbol 
()
p
q
TV
.  However, the set of pure contravariant tensors of order pshall be denoted simply by 
()
p
TVand the set of pure covariant tensors of order qshall be denoted simply 
()
q
TV.  Of 
course, tensors of order 
()
0, 0form the set 
R
and 
 
 
()()
1
1
,
∗
==TV    VTV    V
                                             (33.3)                                             
 
Here we have made use of the identification of with
∗∗
VV as explained in Section 32. 
 
We shall now give some examples of tensors. 
 
Example 1.  If Ais an endomorphism of     ,
Vthen we define a function 
ˆ
:
∗
×→AVV  Rby 
 

Sec. 33            •            Multilinear Functions, Tensors 219 
 
()
ˆ
,,
∗∗
≡Av vv Av                                                    (33.4)                                                    
 
for all and.
∗∗
∈∈vvVV Clearly 
ˆ
A
is bilinear and thus 
()
1
1
ˆ
.
∈ATV  As we shall see later, it is 
possible to establish a canonical isomorphism from 
()
,LV Vto 
()
1
1
TVin such a way that the 
endomorphism Ais identified with the bilinear function 
ˆ
.A
  Then the same symbol Ashall 
represent two objects, namely, an endomorphism of 
Vand a bilinear function of .
∗
×VV  Then 
(33.4) becomes simply 
 
 
()
,,
∗∗
≡Av vv Av                                                    (33.5)                                                    
 
Under this canonical isomorphism, the identity automorphism of 
V
is identified with the scalar 
product, 
 
 
()
,, ,
∗∗ ∗
==Iv vv Ivv v                                              (33.6)                                              
 
Example 2.  If 
vis a vector in and
∗
vVis a covector in ,
∗
V then we define a function 
 
 
:
∗∗
⊗×→vvVV  R                                                   (33.7)                                                   
 
by 
 
 
()
,,,
∗∗∗∗
⊗≡vvuu   uvvu                                             (33.8)                                             
 
for all ,.
∗∗
∈∈uuVV  Clearly, 
∗
⊗vvis a bilinear function, so 
()
1
1
.
∗
⊗∈vvTV  If we make 
use of the canonical isomorphism to be established between 
()
1
1
TVand 
()
;,LV V the tensor 
∗
⊗vv corresponds to an endomorphism of Vsuch that 
 
 
()
,,
∗∗∗∗
⊗=⊗vvuu   uvvu 
 
or equivalently 
 
 
,,   ,
∗∗   ∗ ∗
=⊗uv vuuv vu 
 
for all ,.
∗∗
∈∈uuVV  By the bilinearity of the scalar product, the last equation can be rewritten 
in the form 
 

220  Chap. 7 • TENSOR ALGEBRA 
 
,,,
∗∗∗    ∗
=⊗uvuv   uvvu 
 
Then by the definiteness of the scalar product, we have 
 
 
*
,,forall
∗
=⊗∈vuv v  vuuV                                       (33.9)                                       
 
which defines 
∗
⊗vv
as an endomorphism of .V  The tensor or the endomorphism 
∗
⊗vv
is 
called the tensor product of    and.
∗
vv 
 
Clearly, the tensor product can be defined for arbitrary number of vectors and covectors.  
Let 
1
,,
p
...vvbe vectors in V and 
1
,,
q
vv... be covectors in .
∗
V  Then we define a function 
 
 
1
1
timestimes
:**
q
p
pq
⊗⊗ ⊗⊗⊗    ×× ×××→vvvvVVVVR""""
	
	

 
 
by 
 
 
()
11
11
11
11
,, ,,,
,,,,
qp
pq
pq
pq
⊗⊗ ⊗⊗⊗
≡
vvvvuuuu
uvu v   vuv u
""......
""
                       (33.10)                       
 
for all 
1
,,
q
∈uuV... and 
1
,,
q∗
∈uuV....  Clearly this function is 
()
pq+−linear, so that 
()
1
1
qp
pq
⊗⊗ ⊗⊗⊗∈vvvvTV"", is called the tensor product of 
1
1
,, and,,.
q
p
vv vv...... 
 
Having seen some examples of tensors on 
,Vwe turn now to the structure of the set 
()
p
q
TV
.  We claim that 
()
p
q
TV
 has the structure of a vector space and the dimension of 
()
p
q
TV
 is equal to 
()
pq
N
+
, where N is the dimension of .V  To make 
()
p
q
TV
a vector space, 
we define the operation of addition of any 
()
,
p
q
∈ABTVand the scalar multiplication of Aby 
α∈Rby 
 
 
()
()
()()
1
1
11
11
,, ,,
,, ,,,, ,,
p
q
pp
qq
+
≡+
ABvvv   v
Avv vvBvv vv
......
......  ......
                      (33.11)                      
 
and 
 
 
()
()()
11
11
,, ,,,, ,,
pp
qq
αα≡Avvv   vAvvv   v......   ......
                         (33.12)                         
 

Sec. 33            •            Multilinear Functions, Tensors 221 
respectively, for all 
1
,,
p∗
∈vvV... and 
1
,,
q
∈vvV...
.  We leave as an exercise to the reader the 
proof of the following theorem. 
 
Theorem 33.1.  
()
p
q
TV
is a vector space with respect to the operations of addition and scalar 
multiplication defined by (33.11) and (33.12).  The null element of 
()
p
q
TV,of course, is the 
zero tensor 
:0 
 
 
()
1
1
,, , ,,0
p
q
=0vv  vv......                                             (33.13)                                             
 
for all 
1
,, 
p∗
∈vv
V...
and 
1
,,
q
∈vvV...
. 
 
Next, we determine the dimension of the vector space 
()
p
q
TV
 by introducing the 
concept of a 
product basis. 
 
Theorem 33.2.  Let 
{}
{}
and
i
i
eebe dual bases for and.
∗
VV  Then the set of tensor products 
 
 
{}
1
11
1
,, ,,, ,1, ,
i
j
q
iipq
p
iij jN⊗⊗ ⊗ ⊗⊗=eee e"".........
                     (33.14)                     
 
forms a basis for 
()
p
q
TV, called the product basis.  In particular, 
 
 
()
()
dim
pq
p
q
N
+
=TV                                                 (33.15)                                                 
 
Proof.  We shall prove that the set of tensor products (33.14) is a linearly independent generating 
set for 
()
p
q
TV
.  To prove that the set (33.14) is linearly independent, let 
 
 
,...,
1
1
,...,
11
iii
j
pq
jjii
qp
A⊗⊗⊗⊗⊗=eee e0""                                (33.16)                                
 
where the right-hand side is the zero tensor given by (33.13).  Then from (33.10) we have 
 
 
(
)
1
1
111
...
11
1
11
1
......
11
11
1111
0,,,,,
,,, ,
iijjk
k
pqqp
jjiill
qpq
j
kj
k
ii
pq
p
iil  l
jj
pq
q
iikjk k
kj
ppqp
jjii  llll
qp qq
A
A
AA
δδδδ
=⊗⋅⋅⋅⊗  ⊗  ⊗⋅⋅⋅⊗
=⋅⋅⋅
=⋅⋅⋅⋅⋅⋅=
eee eeeee
ee   e e   e ee e
...
...
...
......
......
"               (33.17)               
 

222  Chap. 7 • TENSOR ALGEBRA 
which shows that the set (33.14) is linearly independent.  Next, we show that every tensor 
()
p
q
∈ATVcan be expressed as a linear combination of the set (33.14).  We define 
()
pq
N
+
scalars 
{}
1
1
...
...11
,  ...   ,1,   ,
p
q
ii
jjpq
AiijjN=......by 
 
 
()
1
1
11
,,,,,
pp
jj
qq
iii
i
jj
A=Aee  ee
...
...
......                                      (33.18)                                      
 
Now we reverse the steps of equation (33.17) and obtain 
 
 
()
()
1
11
111
1
1
,, ,,
,, ,,
pqp
jj
qpq
p
q
iijk
jk
iill
k
k
ll
A⊗⊗⊗⊗⊗
=
eee eeeee
Aee  ee
...
...
""......
......
                     (33.19)                     
 
for all 
11
,,,,,  1,,
pq
kkllN=..........  Since Ais multilinear and since 
{}
i
e
and 
{}
i
eare dual bases 
for     and,
∗
VVthe condition (33.19) implies that 
 
 
()
()
1
1
11
1
1
1
1
,, ,,
,, ,,
pq
jjp
q
iij
j
p
iiq
p
q
A⊗⊗⊗⊗⊗   ...
=
eee evvvv
Avv vv
...
...
""   ...
......
 
 
for all 
1
,,
q
∈vvV... and 
1
,,
p∗
∈vvV... and thus 
 
 
1
1
11
pq
jj
qp
iij
j
ii
A=⊗⊗⊗⊗⊗Aeeee
...
...
""                                  (33.20)                                  
 
Now from Theorem 9.10 we conclude that the set (33.14) is a basis for 
()
p
q
TV. 
 
Having determined the dimension of 
()
p
q
TV, we can now prove that 
()
1
1
TVis 
isomorphic to 
()
;LVV, a result mentioned in Example 1.  As before we define the operation 
 
 
() ()
1
1
:;→LVVT   V

 
 
by the condition (33.4).  Since from (16.8) and (33.15), the vector space 
()
;LVV and 
()
1
1
TV 
are of the same dimension, namely 
2
,Nit suffices to show that the operation 

 is one-to-one.  
Indeed, let 
ˆ
.=A0
  Then from (33.13) and (33.4) we have 
 
 
,0
∗
=vAv 

Sec. 33            •            Multilinear Functions, Tensors 223 
 
for all 
∗∗
∈vVand all 
.∈vV
  Now since the scalar product is definite, this condition implies 
 
 
=Av0 
 
for all ,∈v
Vand thus .=A0  Consequently the operation 

 is an isomorphism. 
 
As remarked in Example 1, we shall suppress the notation 

 by regarding the 
isomorphism it represents as a canonical one.  Therefore, we can replace the formula (33.4) by 
the formula (33.5). 
 
Now returning to the space 
()
p
q
TVin general, we see that a corollary of Theorem 33.2 is 
the simple fact that the set of all tensor products of the form (33.10) is a generating set for 
()
p
q
TV.  We define a tensor that can be represented as a tensor product to be a simple tensor.  It 
should be noted, however, that such a representation for a simple tensor is not unique.  Indeed, 
from (33.8), we have 
 
 
()
1
2
2
∗∗
⎛⎞
⊗=  ⊗
⎜⎟
⎝⎠
vvvv 
 
for any 
∈vVand 
∗∗
∈vVsince 
 
 
()
()
()
11
2,,2,
22
,,,
∗∗∗∗
∗∗∗∗
⎛⎞
⊗=
⎜⎟
⎝⎠
==⊗
vvuuuvvu
uv vu   v v uu
 
 
for all and   .
∗
uu  In general, the tensor product, as defined by (33.10), can be regarded as a 
mapping 
 
 
()
timestimes
:
p
q
pq
∗∗
⊗×××××→VVV  VTV""
	
	

                                  (33.21)                                  
 
given by 
 
 
()
11
11
:,,,,,
qq
pp
⊗=⊗⊗⊗×⊗vvvvvvvv""  "  "                        (33.22)                        
 
for all 
1
,,
p
∈vvV... and 
1
,,
q∗
∈vvV....  It is easy to verify that the mapping ⊗is multilinear in 
the usual sense, i.e., 
 

224  Chap. 7 • TENSOR ALGEBRA 
 
()()
()
11
1
,,,,,,,,
,,,,
qq
q
αβα
β
⊗+ =⊗
+⊗
vvuvvvv
vuv
"...  "...
"...
                         (33.23)                         
 
where 
αβ+vu, v and u all take the same position in the argument of ⊗ but that position is 
arbitrary.  From (33.23), we see that 
 
 
()
()
1
12
1
12
q
p
q
p
α
α
⊗⊗⊗⊗⊗⊗
=⊗   ⊗⊗⊗⊗⊗
vvvvv
vvvv    v
""
""
 
 
We can extend the operation of tensor product from vectors and covectors to tensors in general.  
If 
()
p
q
∈ATV and 
()
r
s
∈BTV, then we define their tensor product ⊗ABto be a tensor of 
order 
()
,prq  s++by 
 
 
()
()()
1
1
11
11
,,   ,,
,, ,,,,   ,  ,,
pr
qs
pppr
qqqs
+
+
++
++
⊗
=
ABv    v  v  v
Avv vv  Bvv    vv
......
......  ... ...
                   (33.24)                   
 
for all 
1
,,
pr+∗
∈vvV... and 
1
,,
qs+
∈vvV...
.  Applying this definition to arbitrary tensors 
andAB yields a mapping 
 
 
()()()
:
prpr
qs    qs
+
+
⊗×→TV  TVTV
                                       (33.25)                                       
 
 
Clearly this operation can be further extended to more than two tensor spaces, say 
 
 
()()()
1
1
11
:
kk
kk
ppp
p
qqqq
++
++
⊗××→TVT VTV
"
"
"                                (33.26)                                
 
in such a way that 
 
 
()
11
,,
kk
⊗=⊗⊗AAAA..."                                          (33.27)                                          
 
where 
()
,      1,    ,
i
i
p
iq
ik∈=ATV...
.  It is easy to verify that this tensor product operation is also 
multilinear in the sense generalizing (33.23) to tensors.  In component form 
 
 
()
1
11
11
1
......
......
pr
pppr
qqqs
qs
ii
iii    i
jjj   j
jj
+
++
++
+
⊗=ABAB
...
...
                               (33.28)                               
 

Sec. 33            •            Multilinear Functions, Tensors 225 
which can be generalized obviously for (33.27) also.  From (33.28), or from (33.24), we see that 
the tensor product is not commutative, but it is associative and distributive. 
 
Relative to a product basis the component form of a tensor 
()
p
q
∈ATV is given by 
(33.20) where the components of Acan be obtained by (33.18).  If we transfer the basis 
{}{}
ˆ
to
ii
ee
as shown by (31.21) and (31.22), then the components of Aas well as the product 
basis relative to 
{}
i
emust be transformed also.  The following theorem gives the transformation 
laws. 
 
Theorem 33.3.  Under the transformation of bases (31.21) and (31.22) the product basis (33.14) 
for 
()
p
q
TVtransforms according to the rule 
 
 
1
1
111
11 1
ˆˆˆ ˆ
ˆˆ
q
p
pqq
pqp
j
j
ii
kjl
kjl
iillkk
TTTT
⊗⊗⊗⊗⊗
=⊗⊗⊗⊗⊗
eee e
eeee
""
"" "  "
                         (33.29)                         
 
and the components of any tensor 
()
p
q
∈ATVtransform according to the rule. 
 
 
11
11
1111
...
ˆ
ˆˆ
ppqp
qpqq
iiil   k k
il
jjkk jjll
ATTTTA=
...
......
""
                                   (33.30)                                   
 
The proof of these rules involves no more than the multilinearity of the tensor product 
⊗ 
and the tensor 
.A  Many classical treatises on tensors use the transformation rule such as (33.30) 
to define a tensor.  The next theorem connects this alternate definition with the one we used. 
 
Theorem 33.4.  Given any two sets of 
()
pq
N
+
scalars 
{}
1
1
p
q
ii
jj
A
...
...
and 
{}
1
1
ˆ
p
q
ii
jj
A
...
...
related by the 
transformation rule (33.30), there exists a tensor 
()
p
q
∈ATV
whose components relative to the 
product bases of 
{}{}
ˆ
to
ii
eeare 
{}
1
1
p
q
ii
jj
A
...
...
and 
{}
1
1
ˆ
p
q
ii
jj
A
...
...
 provided that the bases are related by 
(31.21) and (31.22). 
 
This theorem is obvious, since we can define the tensor 
Aby (33.20); then the 
transformation rule (33.30) shows that the components of 
A relative to the product basis of 
{}
i
eare 
{}
1
1
p
q
ii
jj
A
...
...
.  Thus a tensor 
A
 corresponds to an equivalence set of components, with 
the transformation rule serving as the equivalence relation. 
 
As an illustration of the preceding theorem, let us examine whether or not there exists a 
tensor whose components relative to the product basis of any basis are the values of the 
generalized Kronecker delta introduced in Section 20.  The answer turns out to be yes, since we 
have the identify 
 

226  Chap. 7 • TENSOR ALGEBRA 
 
11 1    1
11  1   1
ˆˆ
rrrr
rrrr
iiii  ll   k k
jjkk jj ll
TTTTδδ≡
......
......
""                                          (33.31)                                          
 
which follows from the fact that 
i
j
T
⎡⎤
⎣⎦
 and 
ˆ
i
j
T
⎡⎤
⎣⎦
 are the inverse of each other.  We leave the 
proof of this identity as an exercise for the reader.  From Theorem 33.4 and the identity (33.31), 
we see that there exist a tensor 
,K
of order 
()
,rrsuch that 
 
 
11
11
1
!
rr
rr
iijj
rjjii
r
δ=⊗⊗⊗⊗⊗Keeee
...
...
""                                  (33.32)                                  
 
relative to any basis 
{}
.
i
e  This tensor plays an important role in the next chapter. 
 
By the same line of reasoning, we may ask whether or not there exists a tensor whose 
components relative to the product basis of any basis are the values of the 
symbolsε−also 
introduced in Section 20.  The answer turns out to be no, since we have the identities 
 
 
1
ˆ
det
lN
NlNlN
jj
i
iij    iij j
TT   Tεε
⎡⎤
=
⎣⎦
......
"                                          (33.33)                                          
 
and 
 
 
1
ˆˆ
det
lN
NlN
lN
ii
l
jj
ii
jjj
TT   Tεε
⎡⎤
=
⎣⎦
...
...
"                                          (33.34)                                          
 
which also follows from the fact that 
i
j
T
⎡⎤
⎣⎦
 and 
ˆ
i
j
T
⎡⎤
⎣⎦
 are the inverses of each other [cf. (21.8)].  
Since these identities do not agree with the transformation rule (33.30), we can conclude that 
there exists no tensor whose components relative to the product basis of any basis are always the 
values of thesymbols
ε−.  In other words, if the values of the symbolsε− are the components of 
a tensor relative to the product basis of one particular basis 
{}
,
i
ethen the components of the 
same tensor relative to the product basis of another basis generally are not the values of the 
symbolsε−, unless the transformation matrices 
i
j
T
⎡⎤
⎣⎦
 and 
ˆ
i
j
T
⎡⎤
⎣⎦
 have unit determinant. 
 
In the classical treatises on tensors, the transformation rules (33.33) and (33.34), or more 
generally 
 
 
11
11
1111
ˆˆ
det
ppqp
qpqq
w
iiil  k k
il
i
jjjkk jjll
ATTTTTAε
⎡⎤
=
⎣⎦
......
......
""                            (33.35)                            
 
are used to define relative tensors.  The exponent 
wand the coefficient ε on the right-hand side 
of (33.35) are called the weight and the parity of the relative tensor, respectively.  A relative 
tensor is called polar if its parity has the value +1 in all transformations, while a relative tensor is 

Sec. 33            •            Multilinear Functions, Tensors 227 
called axial if ε is equal to the sign of the determinant of the transformation matrix 
i
j
T
⎡⎤
⎣⎦
.  In 
particular, (33.33) shows that 
{}
lN
ii
ε
...
are the components of an axial covariant tensor of order N 
and weight    1−, while (33.34) shows that 
{}
lN
ii
ε
...
 are the components of an axial contravariant 
tensor of order N and weight    1+.  We shall see some more examples of relative tensors in the 
next chapter. 
 
 
Exercises 
 
33.1     Prove Theorem 33.1. 
33.2     Prove equation (33.31). 
33.3     Under the canonical isomorphism of 
()
;LVV
with 
()
1
1
,TV
show that an endomorphism 
:→A
VVand its dual :
∗
∗∗
→AVVcorrespond to the same tensor. 
33.4     Define an isomorphism from 
()()
()
;;;LLVV   LVVto 
()
2
2
TVindependent of any 
basis. 
33.5     If     
()
2
,∈ATVshow that the determinant of the component matrix 
ij
A
⎡⎤
⎣⎦
of Adefines a 
polar scalar of weight two, i.e., the determinant obeys the transformation rule 
 
 
()
2
ˆ
detdetdet
k
ijlij
ATA
⎡⎤
⎡⎤
⎡⎤
=
⎣⎦
⎣⎦
⎣⎦
 
 
33.6     Define isomorphisms from 
()
;
∗
LVV to 
()
2
TV, 
()
;
∗
LV  Vto 
()
2
TV, and from 
()
;
∗∗
LV  V to 
()
1
1
TV independent of any basis. 
 
33.7
 Given a relation of the form 
 
 
()
11
,1,
pp
iiklmii
ABkmC=
......
 
 
where 
{}
1p
iiklm
A
...
 and 
{}
1p
ii
C
...
 are components of tensors with respect to any basis, show 
that the set 
()
{}
,,Bklm, likewise, can be regarded as components of a tensor, belonging 
to 
()
3
TVin this case.  This result is known as the quotient theorem in classical treatises 
on tensors. 
33.8     If     
()
pq+
∈ATV
, define a tensor 
()
pqp   q+
∈ATVT
 by 
 
 
()()
111 1
,,,,,,, ,,,
pqqppq
≡Auu vvAvv uuT......  ......                         (33.36)                         
 

228  Chap. 7 • TENSOR ALGEBRA 
for all 
11
,,,,,.
qp
∈uuvvV......  Show that the operation 
 
 
()()
:
pqp   qp   q++
→TV    TVT                                             (33.37)                                             
 
is an automorphism of 
()
pq+
TV.  We call 
pq
T
 the generalized transpose operation.  What is the 
relation between the components of A and 
pq
AT? 

Sec. 34            •            Contractions                              229 
 
Section 34.  Contractions 
 
In this section we shall consider the operation of contracting a tensor of order 
()
,pq
to 
obtain a tensor of order 
()
1,1  ,pq−−where    ,pqare greater than or equal to one.  To define this 
important operation, we prove first a useful property of the tensor space 
()
p
q
TV.  In the 
preceding section we defined a tensor of order 
()
,pq
to be a multilinear function 
 
timestimes
:**
pq
×××××→AVVVVR""
	
	

 
 
Clearly, this concept can be generalized to a multilinear transformation from the 
()
,pq
-fold 
Cartesian product of and
∗
VVto an arbitrary vector space     ,Unamely 
 
 
timestimes
:**
pq
××× ×× →ZVVV   VU""
	
	

                                        (34.1)                                        
 
The condition that Z be a multilinear transformation is similar to that for a multilinear function, 
namely, Z is linear in each one of its variables while its other variables are held constant, e.g., 
the tensor product ⊗ given by (33.21) is a multilinear transformation. The next theorem shows 
that, in some sense, any multilinear transformation Zof the form (34.1) can be factored through 
the tensor product ⊗ given by (33.21).  This fact is known as the universal factorization property 
of the tensor product. 
 
Theorem 34.1.  If Zis an arbitrary multilinear transformation of the form (34.1), then there 
exists a unique linear transformation 
 
 
()
:
p
q
→TVUC
                                                       (34.2)                                                       
 
such that 
 
 
()()
11
11
,,,,
qq
pp
=⊗⊗⊗⊗⊗vvvvvvvvZC......   "  "                       (34.3)                       
 
for all 
1
1
,and,, .
q
p
∗
∈∈vvv vVV......
 
 
Proof.  Since the simple tensors form a generating set of 
()
p
q
TV, if the linear transformation C 
satisfying (34.3) exists, then it must be unique.  To prove the existence of 
C, we choose a basis 
{}
i
efor 
V
and define the product basis (33.14) for 
()
p
q
TVas before.  Then we define 
 

230  Chap. 7 • TENSOR ALGEBRA 
 
()()
11
11
,,,,,
qq
pp
jj
jj
iiii
⊗⊗⊗⊗⊗ ≡eee eZeeeeC"" ......                       (34.4)                       
 
for all 
11
,,,,,   1,,
pq
iij jN=...... ...
 and extend 
C
to all tensors in 
()
p
q
TV by linearity.  Now it is 
clear that the linear transformation 
Cdefined in this way satisfies the condition (34.3), since both 
C and Zare multilinear in the vectors 
1
,,
p
vv...and the covectors 
1
,,,
q
vv...and they agree on 
the dual bases 
{}
i
e and 
{}
i
e for V and 
∗
V, as shown in (34.4).  Hence they agree on all 
()
1
1
,, ,,,
q
p
vvvv....... 
 
If we use the symbol ⊗ to denote the multilinear transformation (33.21), then the 
condition (34.3) can be rewritten as 
 
 
=⊗ZCD                                                            (34.5)                                                            
 
where the operation D on the right-hand side of (34.5) denotes the composition as defined in 
Section 3.  Equation (34.5) expresses the meaning of the universal factorization property.  In the 
modern treatises on tensors, this property is often used to define the tensor product and the tensor 
spaces.  Our approach to the concept of a tensor is a compromise between this abstract modern 
concept and the classical concept based on transformation rules; the preceding theorem and 
Theorems 33.3 and 33.4 connect our concept with the other two. 
 
Having proved the universal factorization property of the tensor product, we can now 
define the operation of contraction.  Recall that if 
∈vV and 
∗∗
∈vV, then the scalar product 
,
∗
vvis a scalar.  Here we have used the canonical isomorphism given by (32.6).  Of course, the 
operation 
 
 
 ,   :
∗
×→VVR
 
 
is a bilinear function.  By Theorem 34.1 
, can be factored through the tensor product  
 
 
()
1
1
:
∗
⊗×→VVT V 
 
i.e., there exists a linear map 
 
 
()
1
1
:→TVRC
                                                      (34.6)                                                      
 
such that 
 
 
 , =⊗CD 

Sec. 34            •            Contractions                              231 
 
or, equivalently, 
 
 
()
,
∗∗
=⊗vvv   vC                                                    (34.7)                                                    
 
for all 
∈vV and 
∗∗
∈v V
.  This linear function Cis the simplest kind of contraction operation.  
It transforms the tensor space 
()
1
1
TV
to the tensor space 
()()
110
110
−
−
==TV TV R
. 
 
In general if 
()
p
q
∈ATVis an arbitrary simple tensor, say 
 
 
1
1
q
p
=⊗⊗⊗⊗⊗Avv  vv""                                           (34.8)                                           
 
then for each pair of integers 
()
,ij
, where 1, 1ip    jq≤≤≤≤, we seek a unique linear 
transformation 
 
 
()()
1
1
:
ipp
jqq
−
−
→TVT    VC                                               (34.9)                                               
 
such that 
 
 
111
111
,
ijjjq
jiii p
−+
−+
=⊗⊗⊗⊗⊗⊗⊗⊗⊗⊗⊗Avvvv  vv vv  vvC""""     (34.10)     
 
for all simple tensors 
A.  A more compact notation for the tensor product on the right-hand side 
is 
 
 
1
1
jq
ip
⊗⊗  ⊗ ⊗⊗⊗    ⊗vvv vvv

"" " ""
                               (34.11)                               
 
Since the representation of a simple tensor by a tensor product is not unique, the existence of 
such a linear transformation 
i
j
Cis by no means obvious.  However, we can prove that 
i
j
Cdoes 
exist and is uniquely determined by the condition (34.10).  Indeed, using the universal 
factorization property, we can prove the following theorem. 
 
Theorem 34.2.  A unique contraction operation 
i
j
C
satisfying the condition (34.10) exists. 
 
Proof.  We define a multilinear transformation of the form (34.1) with 
()
1
1
p
q
−
−
=UTV by 
 
 
()
1
1
1
1
,, ,,
,
q
p
jjq
iip
=⊗⊗⊗⊗⊗
Zvv vv
vvvvv   vvv
......

""""
                          (34.12)                          

232  Chap. 7 • TENSOR ALGEBRA 
 
for all 
1
,,
p
∈vvV...
 and 
1
,
q∗
∈vvV....  Then by Theorem 34.1 there exists a unique linear 
transformation 
i
j
C
of the form (34.9) such that (34.10) holds, and thus the proof is complete. 
 
Next we express the contraction operation in component form. 
 
Theorem 34.3.  If 
()
p
q
∈ATV is an arbitrary tensor of order 
()
,pq, then in component form 
relative to any dual bases 
{}
i
e
 and 
{}
i
ewe have 
 
 
()
1
111
111
1
ip
iip
jj   q
jq
kkk
kktk   k
i
jlltll
ll l
A
−+
−+
=AC

......
......

......
......
                                 (34.13)                                 
 
Proof.  Since the contraction operation is linear applying it to the component form (33.20), we 
get 
 
 
1
1
11
,
pjjq
qii   p
kklll
l
i
jllkkkk
A=⊗⊗⊗⊗⊗AeeeeeeeeC
...
...

""    ""                (34.14)                
 
which means nothing but the formula (34.13) because we have 
,
jj
ii
ll
kk
δ=ee. 
 
In particular, for the special case 
Cgiven by (34.6) we have 
 
 
()
t
t
A=AC                                                         (34.15)                                                         
 
for all 
()
1
1
∈ATV.  If we now make use of the canonical isomorphism  
 
 
()()
1
1
;≅TVLVV
 
 
we see that 
Ccoincides with the trace operation defined by (19.8). 
 
Using the contraction operation, we can define a scalar product for tensors.  If 
()
p
q
∈ATV and 
()
q
p
∈BTV, the tensor product 
⊗AB
is a tensor in 
()
pq
pq
+
+
TVand is defined 
by (33.24).  We apply the contraction 
 
 
11
11
11
11
times
times
qq
p
q
++
=CCC
CC
DD"D
D"D
	

	

                                         (34.16)                                         
 
to            ,
⊗ABthen the result is a scalar ,,ABnamely 
 

Sec. 34            •            Contractions                              233 
 
()
,≡⊗ABA   BC                                                  (34.17)                                                  
 
called the scalar product of and   .AB  It is a simple matter to see that 
, 
 is a bilinear and 
definite function in the sense similar to those properties of the scalar product of 
and
∗
vvexplained in Section 31.  We can use the bilinear function 
 
 
()()
 ,   :
pq
qp
×→TV  TVR                                          (34.18)                                          
 
to identify the space 
()
p
q
TVwith the dual space 
()
q
p
∗
TVof the space 
()
q
p
TV or equivalently, 
we can define the dual space 
()
q
p
∗
TV
 abstractly as usual and then introduce a canonical 
isomorphism from 
()
p
q
TVto 
()
q
p
∗
TV through (34.17).  Thus we write 
 
 
()   ()
pq
qp
∗
≅TV   TV                                                  (34.19)                                                  
 
Of course we shall also identify the second dual space 
()
q
p
∗∗
TV
 with 
()
q
p
TV as explained in 
general in Section 32.  Hence, we have 
 
 
()   ()
pq
qp
∗∗∗
≅TVTV                                                 (34.20)                                                 
 
which follows also by interchanging     and
pqin (34.19).  From (34.13) and (34.16), the scalar 
product 
,AB is given by the component form 
 
 
11
11
,
pp
qp
iij j
jjii
AB=AB
......
......
                                            (34.21)                                            
 
relative to any dual bases 
{}
i
e
 and 
{}
i
efor V and 
∗
V. 
 
 
Exercises 
 
34.1
 Show that 
 
 
()
11
11
,,,,,,
pp
qq
⊗⊗⊗⊗⊗ =AvvvvAvvvv"" "" 
 
            for            all            
()
p
q
∈ATV, 
1
,,
q
∈vvV" and 
1
,,
p∗
∈vvV". 
34.2     Give another proof of the quotient theorem in Exercise 33.7 by showing that the 
operation 

234  Chap. 7 • TENSOR ALGEBRA 
 
 
()()()
()
:;
rpps
sqqr
−
−
→TVLT VTVJ 
 
defined by 
 
 
()()
111  1
111  1ss
stimes
r times
++
≡⊗ABA  BJCCCCD"D D  D"D
	

	

 
 
for all 
()
r
s
∈ATV and 
()
p
q
∈BTV is an isomorphism.  Since there are many such 
isomorphisms by choosing contractions of pairs of indices differently, we do not make 
any one of them a canonical isomorphism in general unless stated explicitly. 
34.3     Use the universal factorization property and prove the existence and the uniqueness of the 
generalized transpose operation 
pq
Tdefined by (33.36) in Exercise 33.8.  Hint:  Require 
pq
Tto be an automorphism of 
()
pq+
TVsuch that 
 
 
()
11  11pq  q p
pq
⊗⊗ ⊗⊗⊗ =⊗⊗⊗⊗⊗vvuuuuv vT""""                (34.22)                
 
 for all simple tensors 
()
11pq
pq+
⊗⊗ ⊗⊗⊗∈vvuuTV"". 
34.4 
{}
1
1
q
p
j
j
ii
⊗⊗⊗⊗⊗eee e""is the product basis for 
()
p
q
TVinduced by the dual bases 
{}
i
e and 
{}
i
e;  construct its dual basis in 
()
q
p
TVwith respect to the scalar product 
defined by (34.17).

Sec. 35            •            Tensors on Inner Product Spaces 235 
 
 
Section 35.  Tensors on Inner Product Spaces 
 
The main result of this section is that if 
Vis equipped with a particular inner product, 
then we can identify the tensor spaces of the same total order by means of various canonical 
isomorphisms, a special case of these being the isomorphism 
Gfrom      to,
∗
VVwhich we have 
introduced in Section 31 [cf. (31.5)]. 
 
Recall that the isomorphism :
∗
→GVV is defined by the condition [cf. (31.6)] 
 
 
()
,,   ,,==⋅∈Gu  vGv uu  vu  vV
                                      (35.1)                                      
 
In general, if Ais a simple, pure, contravariant tensor of order 
p, say 
 
 
1p
=⊗⊗Avv"
 
 
then we define the pure covariant representation of A to be the tensor 
 
 
()
1
p
pp
=⊗⊗∈AGvGvTVG"
                                          (35.2)                                          
 
By linearity, 
p
Gcan be extended to all of 
()
p
TV.  Equation (35.2) means that 
 
 
()
()
()
11111
,,,,
p
ppppp
==⋅⋅AuuGv uGv uv uv  uG..."  "                   (35.3)                   
 
for all 
1
,,
p
∈uuV...
.  Equation(35.3) generalizes the condition (35.1) from 
v
 to 
1p
⊗⊗vv"
.  
Of course, because the tensor product is not one-to-one, we have to show that the covariant 
tensor 
p
AG does not depend on the representation of A.  This fact follows directly from the 
universal factorization of the tensor product.  Indeed, if we define a p-linear map 
 
 
()
:
p
ptimes
××→ZVVTV"
	  

 
 
by 
 
 
()
11
,,
pp
≡⊗⊗ZvvGvGv..."                                           (35.4)                                           
 
then 
Zcan be factored through the tensor product ⊗, i.e., there exists a unique linear 
transformation 
p
Gfrom 
()
p
TV to 
()
p
TV, 

236  Chap. 7 • TENSOR ALGEBRA 
 
 
()()
:
pp
p
→TVTVG                                                  (35.5)                                                  
 
such that 
 
 
p
=⊗ZGD 
 
or, equivalently, 
 
 
()()
11
,,
pp
pp
≡⊗⊗=ZvvvvAGG..."
                                    (35.6)                                    
 
Comparing (35.6) with (35.4) we see that 
p
Gobeys the condition (35.2), and thus 
p
AG is well 
defined by (35.3). 
 
It is easy to verify that 
p
G
is an isomorphism.  Indeed 
()
1
p
−
G is the unique linear 
transformation from 
()
p
TV to 
()
p
TV such that 
 
 
()()
1
1111
ppp
−
−−
⊗⊗ =   ⊗⊗vvGvGvG""                                 (35.7)                                 
 
for all 
1
,,
p∗
∈vvV....  Thus 
p
Gmakes 
()
p
TV and 
()
p
TV isomorphic, just as Gmakes 
V
 
and 
∗
V isomorphic.  In fact, 
1
=GG. 
 
Clearly, we can extend the preceding argument to mixed tensor spaces on 
Valso.  If Ais 
a mixed simple tensor in 
()
p
q
TV, say 
 
 
1
1
q
p
=⊗⊗ ⊗⊗⊗Avv  uu""
                                           (35.8)                                           
 
then we define the pure covariant representation of 
Ato be the tensor 
 
 
1
1
pq
qp
=⊗⊗⊗ ⊗AGvGv   uuG""                                       (35.9)                                       
 
and 
p
q
Gis an isomorphism from 
()
p
q
TV to 
()
pq+
TV, 
 
 
()()
:
pp
qqpq
+
→TVT  VG
                                              (35.10)                                              
 
Indeed, its inverse is characterized by 
 

Sec. 35            •            Tensors on Inner Product Spaces 237 
 
()()
1
111111
ppqpq
q
−
−−
⊗⊗ ⊗⊗⊗ =   ⊗⊗   ⊗⊗⊗vv   uuGvGv   uuG""  " "        (35.11)        
 
for all 
11
,, ,,,
pq∗
∈vvuuV.......  Clearly, by suitable compositions of the operations 
p
q
G and 
their inverses, we can define isomorphisms between tensor spaces of the same total order.  For 
example, if 
11
andpq are another pair of integers such that 
 
 
11
pqpq+=+ 
 
then 
()
p
q
TV
is isomorphic to 
()
1
1
p
q
TV
by the isomorphism 
 
 
()
()   ()
11
11
1
:
pp
pp
qqqq
−
→TVT VGGD                                       (35.12)                                       
 
In particular, if 
11
0,qppq==+, then for any 
()
p
q
∈ATV
the tensor 
()
()
1
pqppq
q
−
++
∈ATVGG 
is called the pure contravariant representation of 
A
.  For example, if 
A
 is given by (35.8), then 
its pure contravariant representation is given by 
 
 
()
1
111
1
pqpq
qp
−
+−−
=⊗⊗⊗  ⊗⊗Avv  GuGuGG"" 
 
We shall now express the isomorphism 
p
q
Gin component form.  If 
()
p
q
∈ATV has the 
component representation 
 
 
1
1
1
1
pq
p
iii
j
jjii
q
A=⊗⊗⊗⊗⊗Aeeee
...
...
""                                 (35.13)                                 
 
then from (35.9) we obtain 
 
 
1
1
1
1
pq
p
iij
j
p
qjji   i
q
A=⊗⊗⊗⊗⊗AGeGeeeG
...
...
""                           (35.14)                           
 
This result can be written as  
 
 
1
1
1
1
pq
p
iij
j
p
qjjii
q
A=⊗⊗⊗⊗⊗AeeeeG
...
...
""                              (35.15)                              
 
where 
{}
i
eis a basis in 
∗
Vreciprocal to 
{}
i
e, i.e., 
 
 
jj
ii
δ⋅=ee                                                          (35.16)                                                          

238  Chap. 7 • TENSOR ALGEBRA 
 
since it follows from (19.1), (12.6), and (35.1) that we have 
 
 
j
iiji
e==eGe    e                                                      (35.17)                                                      
 
where 
 
 
jiji
e=⋅ee                                                          (35.18)                                                          
 
Substituting (35.17) into (35.15), we obtain 
 
 
11
11
......
pq
pq
ij
ij
p
qiijj
A=⊗⊗⊗⊗⊗AeeeeG""                               (35.19)                               
 
where 
 
 
1
11111
...
.........
p
pqppq
kk
iijjikikjj
AeeA="                                         (35.20)                                         
 
This equation illustrates the component form of the isomorphism 
p
q
G
.  It has the effect of 
lowering the first p superscripts on the components 
{}
1
1
...
...
p
q
ii
jj
A.  Thus 
()
p
q
∈ATVand 
()
p
qpq
+
∈ATVGhave the same components if the bases in (35.13) and (35.15) are used.  On the 
other hand, if the usual product basis for 
()
p
q
TV and 
()
pq+
TVare used, as in (35.13) and 
(35.19), the components of 
A and 
p
q
AG
are related by (35.20). 
 
Since 
p
q
G is an isomorphism, its inverse exists and is a linear transformation from 
()
pq+
TV and 
()
p
q
TV.  If 
()
pq+
∈ATV has the component representation 
 
 
11
11
......
pq
pq
ij
ij
iij j
A=⊗⊗⊗⊗⊗Aeeee""
                                 (35.21)                                 
 
then from (35.11) 
 
 
()
11
11
1
11
......
pq
pq
ii
ij
p
qiijj
A
−
−−
=⊗⊗⊗⊗⊗AGeGeeeG""                      (35.22)                      
 
By the same argument as before, this formula can be rewritten as  
 
 
()
11
11
1
......
pq
pq
ij
ij
p
qiijj
A
−
=⊗⊗⊗⊗⊗AeeeeG""                           (35.23)                           
 

Sec. 35            •            Tensors on Inner Product Spaces 239 
where 
{}
i
eis a basis of Vreciprocal to 
{}
i
e
, or equivalently as 
 
 
()
1
1
11
1
...
...
pq
qp
iij
j
p
qjjii
A
−
=⊗⊗⊗⊗⊗AeeeeG""                           (35.24)                           
 
where 
 
 
1
11
111
...
.........
ppp
qpq
iiik
ik
jjkkjj
AeeA="                                         (35.25)                                         
 
Here of course 
ij
e
⎡⎤
⎣⎦
is the inverse matrix of 
ij
e
⎡⎤
⎣⎦
 and is given by 
 
 
ijij
e=⋅ee                                                          (35.26)                                                          
 
since 
 
 
1iiij
j
e
−
==eGe   e                                                     (35.27)                                                     
 
Equation (35.25) illustrates the component form of the isomorphism 
()
1
p
q
−
G.  It has the effect of 
raising the first 
p subscripts on the components 
{}
11
......
pq
iij j
A. 
 
Combining (35.20) and (35.25), we see that the component form of the isomorphism 
()
1
1
1
p
p
qq
−
GGD in (35.12) is given by raising the first 
1
pp−subscripts of 
()
p
q
∈ATV if 
1
pp>, or 
by lowering the last 
1
pp− superscripts of 
A
 if 
1
pp>.   Thus if 
A
 has the component form 
(35.13), then 
()
1
1
1
p
p
qq
−
AGG has the component form 
 
 
()
1
1111
111
1
1
1
...
...
pq
qp
iij
pj
p
qqjjii
A
−
=⊗⊗⊗⊗⊗GGAeeee""                      (35.28)                      
where 
 
 
1
111
1
11
......
11
1
11
1
...
...
...1
if
k
p
pp p
pp
kkj j
pp    q
q
ii
i
iii k
jj
AA  eepp
−
+
−
=>"                        (35.29)                        
 
and 
 
 
11
111
1111
11111
.........
......1
if
pppp
qppqpppp
iiii   k
jjjj kjk   j
AAeepp
−
−+−  −
=>"                      (35.30)                      
 

240  Chap. 7 • TENSOR ALGEBRA 
For example, if 
()
1
2
∈ATVhas the representation 
 
 
ijk
jk    i
A=⊗⊗Aeee
 
 
then the covariant representation of 
A
 is 
 
 
1
2
ijkiljkljk
jk    ijk   illjk
AAeA=⊗⊗= ⊗⊗=⊗⊗AeeeeeeeeeG 
 
the contravariant representation of 
Ais 
 
 
()
1
31
2
ijkijlkmilm
jk    ijkilmilm
AAeeA
−
=⊗⊗=    ⊗⊗≡⊗⊗Aeeeeee  eeeGG 
 
and the representation of 
A
in 
()
2
1
TVis 
 
 
()
1
21
12
ijkijlkilk
jk    ijkilk    il
AAeA
−
=⊗⊗=  ⊗⊗≡⊗⊗Aeeeeee eeeGG 
 
etc.  These formulas follow from (35.20), (35.24), and (35.29). 
 
 Of course, we can apply the operations of lowering and raising to indices at any position, 
e.g., we can define a “component” 
31
2
      ...   ...
 ...      ...
a
b
ii
i
jj
A
 for 
()
r
∈ATV
by 
 
 
333
111
212
       ...   ...
  ...      ......
aaa
br
iiijij
iij
jjjjj
AAeee≡""                                       (35.31)                                       
 
However, for simplicity, we have not yet assigned any symbol to such representations whose 
“components” have an irregular arrangement of superscripts and subscripts.  In our notation for 
the component representation of a tensor 
()
p
q
∈ATV the contravariant superscripts always 
come first, so that the components of 
Aare written as 
1
1
...
...
p
q
ii
jj
Aas shown in (35.13), not as 
1
1
...
...
p
q
ii
jj
Aor as any other rearrangement of positions of the superscripts 
1p
ii...and the 
subscripts
1q
jj..., such as the one defined by (35.31).  In order to indicate precisely the position 
of the contravariant indices and the covariant indices in the irregular component form, we may 
use, for example, the notation 
 
 
()   ( )    ()( )()
12 3ab
∗∗
⊗⊗⊗⊗⊗⊗⊗VV  VVV"" "                                     (35.32)                                     
 
for the tensor space whose elements have components of the form on the left-hand side of 
(35.31), where the order of 
Vand 
∗
Vin (35.32) are the same as those of the contravariant and 

Sec. 35            •            Tensors on Inner Product Spaces 241 
the covariant indices, respectively, in the component form.  In particular, the simple notation 
()
p
q
TVnow corresponds to 
 
 
()( )  (   )(   )
11pppq
∗∗
++
⊗⊗⊗ ⊗⊗VVVV""
                                           (35.33)                                           
 
Since the irregular tensor spaces, such as the one in (35.32), are not convenient to use, we shall 
avoid them as much as possible, and we shall not bother to generalize the notation 
p
q
G to 
isomorphisms from the irregular tensor spaces to their corresponding pure covariant 
representations. 
 
 So far, we have generalized the isomorphism 
Gfor vectors to the isomorphisms 
p
q
Gfor 
tensors of type 
()
,pqin general.  Equation (35.1) for
G
, however, contains more information 
than just the fact that 
Gis an isomorphism.  If we read that equation reversely, we see that Gcan 
be used to compute the inner product on
V.  Indeed, we can rewrite that equation as  
 
 
1
⋅=≡vu   Gv,uv,uG                                              (35.34)                                              
 
This idea can be generalized easily to tensors.  For example, if 
Aand Bare tensors in 
()
r
TV, 
then we can define an inner product 
⋅AB by 
 
 
r
⋅≡ABA,BG                                                     (35.35)                                                     
 
We leave the proof to the reader that (35.35) actually defines an inner product
()
r
TV.  By use 
of (34.21) and (35.20), it is possible to write (35.35) in the component form 
 
 
11
11
......
rr
rr
iji j
jjii
Ae  eB⋅=AB"                                            (35.36)                                            
 
or, equivalently, in the forms 
 
 
1
1
11
1
11
211
...
...
...
...
...
...
,etc.
r
r
rrr
rr
r
r
ii
ii
iiij
jii
ii   j
iiij
AB
AeB
AB  e
−
⎧
⎪
⎪
⋅=
⎨
⎪
⎪
⎩
AB                                      (35.37)                                      
 
Equation (35.37) suggests definitions of inner products for other tensor spaces besides 
()
r
TV.  
If 
Aand Bare in
()
p
q
TV, we define ⋅ABby 
 

242  Chap. 7 • TENSOR ALGEBRA 
 
()()()
111
,
pqp   pqpp  pqp
qqqq
−−−
+++
⋅≡⋅=ABABABGGGGGGG                  (35.38)                  
 
Again, we leave it as an exercise to the reader to establish that (35.38) does define an inner 
product on
()
p
q
TV; moreover, the inner product can be written in component form by (35.37)
(35.37) also. 
 
 In section 32, we pointed out that if we agree to use a particular inner product, then the 
isomorphism 
Gcan be regarded as canonical.  The formulas of this section clearly indicate the 
desirability of this procedure if for no other reason than notational simplicity.  Thus, from this 
point on, we shall identify the dual space
∗
Vwith
V
, i.e., 
 
 
∗
≅VV
 
 
by suppressing the symbol
G.  Then in view of (35.2) and (35.7), we shall suppress the symbol 
p
q
G also.  Thus, we shall identify all tensor spaces of the same total order, i.e., we write 
 
 
()()()
-1
1
rr
r
≅≅≅TVT   VTV"
                                        (35.39)                                        
 
By this procedure we can replace scalar products throughout our formulas by inner products 
according to the formulas (31.9) and (35.38). 
 
 The identification (35.39) means that, as long as the total order of a tensor is given, it is 
no longer necessary to specify separately the contravariant order and the covariant order.  These 
separate orders will arise only when we select a particular component representation of the 
tensor.  For example, if 
Ais of total orderr, then we can express Aby the following different 
component forms: 
 
 
1
1
11
11
1
1
...
...
...
r
r
rr
rr
r
r
ii
ii
iij
jii
jj
jj
A
A
A
−
−
=⊗⊗
=⊗⊗⊗
=⊗⊗
Ae  e
eee
ee
"
"
#
"
                                         (35.40)                                         
 
where the placement of the indices indicates that the first form is the pure contravariant 
representation, the last form is the pure covariant representation, while the intermediate forms 
are various mixed tensor representations, all having the same total order 
r.  Of course, the 
various representations in (35.40) are related by the formulas (35.20), (35.25), (35.29), and 
(35.30).  In fact, if (35.31) is used, we can even represent 
Aby an irregular component form 
such as 
 
 
132
213
    ...       
       ...     ...
ab
ba
ii  ij
j
jjiii
A=⊗⊗⊗⊗⊗⊗⊗Aeeeee"""
                         (35.41)                         

Sec. 35            •            Tensors on Inner Product Spaces 243 
 
provided that the total order is unchanged. 
 
 The contraction operator defined in Section 34 can now be rewritten in a form more 
convenient for tensors defined on an inner product space.  If 
Ais a simple tensor of (total) 
order  ,2
rr≥, with the representation 
 
 
1r
=⊗⊗Avv" 
 
then 
ij
AC, where 1ijr≤< ≤, is a simple tensor of (total) order 2r−defined by (34.10), 
 
 
()
1ijijijr
≡⋅  ⊗⊗Avvv    vvvC

"""
                                    (35.42)                                    
 
By linearity, 
ij
Ccan be extended to all tensors of orderr.  If 
()
r
∈ATV
has the representation 
 
 
1
1
...
r
r
kk
kk
A=⊗⊗Ae  e" 
 
then by (35.42) and (35.26) 
 
 
()
1
1
1
1
1
1
...
     
... ......
r
r
ijj
ir
r
jj
r
jri
kk
ijkkij
kkk
k
kk
kk
kk
kk
kkkk
j
i
A
eA
A
=⊗⊗
=⊗ ⊗
=⊗⊗
Aee
eeee
eeee
CC
...
"

"""

"""
                                 (35.43)                                 
 
It follows from the definition (35.42) that the complete contraction operator 
C [cf. (35.16)] 
 
 
()
2
:
r
→TVRC 
 
can be written 
 
 
()
1213
11r
+
=CCCCDD"D                                               (35.44)                                               
 
Also, if 
Aand Bare in
()
r
TV, it is easily establish that [cf. (34.17)] 
 
 
()
⋅=  ⊗ABA  BC                                                    (35.45)                                                    
 

244  Chap. 7 • TENSOR ALGEBRA 
 In closing this chapter, it is convenient for later use to record certain formulas here.  The 
identity automorphism 
Iof Vcorresponds to a tensor of order 2.  Its pure covariant 
representation is simply the inner product 
 
 
()
⋅Iu,v =uv                                                       (35.46)                                                       
 
The tensor 
Ican be represented in any of the following forms: 
 
 
ijijij ji
ijijj    iij
eeδδ=⊗=⊗=⊗= ⊗Iee  ee  ee  ee
                             (35.47)                             
 
There is but one contraction for 
I
, namely 
 
 
12
tr
i
i
Nδ===IIC                                                  (35.48)                                                  
 
In view of (35.46), the identity tensor is also called the 
metric tensor of the inner product space. 
 
 
Exercises 
 
35.1
 Show that (35.45) defines an inner product on 
()
r
TV 
35.2
 Show that the formula (35.38) can be rewritten as  
 
 
()
1
,
qp
ppqq
−
⋅=ABABGTG                                            (35.49)                                            
 
where
pq
T
is the generalized transpose operator defined in Exercises 33.8 and 34.3.  In 
particular, if 
Aand B are second order tensors, then (35.49) reduces to the more familiar 
formula: 
 
 
()
tr
T
⋅=ABAB
                                                     (35.50)                                                     
 
35.3
 Show that the linear transformation 
 
 
()()
:
pp
p
→TVTTG 
 
 defined by (35.2) can also be characterized by 
 
 
()
()()
11
,,,,
p
pp
≡A    uuA  GuGuG......
                                    (35.51)                                    
 

Sec. 35            •            Tensors on Inner Product Spaces 245 
            for            all            
()
p
V∈ATand all 
1
,,
p
∈uuV...
. 
35.4
 Show that if Ais a second-order tensor, then 
 
 
()
23
=⊗AIAC
 
 
 
 What is the component form of this identity?



 247 
_______________________________________________________________________________ 
Chapter 8 
 
 
EXTERIOR ALGEBRA 
 
 
The purpose of this chapter is to formulate enough machinery to define the determinant of 
an endomorphism in a component free fashion, to introduce the concept of an orientation for a 
vector space, and to establish a certain isomorphism which generalizes the classical operation of 
vector product to a N-dimensional vector space.  For simplicity, we shall assume throughout this 
chapter that the vector space, and thus its associated tensor spaces, are equipped with an inner 
product.  Further, for definiteness, we shall use only the pure covariant representation; 
transformations into other representations shall be explained in Section 42. 
 
Section 36    Skew-Symmetric Tensors and Symmetric Tensors 
 
If            (    )
r
∈ATV and 
σ
 is a given permutation of 
{}
1,...,r, then we can define a new tensor 
()
rσ
∈ATVT by the formula 
 
 
1(1)()
(    ,..,)(,..,)
rrσσσ
=AvvAvvT                                            (36.1)                                            
 
for all 
1
,..,
r
∈vvV.  For example, the generalized transpose operation 
pq
T
 defined by (33.36) in 
Exercise 33.8 is a special case of 
σ
T with σ given by 
 
 
11
11
qqqp
ppqp
σ
⋅⋅⋅+ ⋅⋅⋅ +
⎛⎞
=
⎜⎟
+ ⋅⋅⋅  +⋅⋅⋅
⎝⎠
 
 
Naturally, we call 
σ
AT the σ-transpose of A for any σ in general.  We have obtained the 
components of the transpose 
pq
T in Exercise 33.8.  For an arbitrary permutation σ the components 
of 
σ
AT are related to those of A by 
 
 
1(1)()
......
()
rr
iiii
A
σο
σ
=AT                                                      (36.2)                                                      
 
Using the same argument as that of Exercise 34.3, we can characterize 
σ
T by the condition that 
 
 
11
1(1)()
()
rrσσ
σ
−−
⊗⋅⋅⋅⊗   =⊗⋅⋅⋅⊗vvvvT                                       (36.3)                                       
 
for all simple tensors 
1
()
r
r
⊗⋅⋅⋅⊗  ∈vvTV 
 

248                                                      Chap.                                                      8                                                      •                                                      EXTERIOR ALGEBRA 
If 
σ
=AAT for all permutations σ A is said to be (comp1etely) symmetric. On the other 
hand if 
σσ
ε=AAT for all permutations σ, where 
σ
ε denotes the parity of σ defined in Section 
20, then A is said to be 
(complete1y) skew-symmetric.  For example, the identity tensor I given by 
(35.30) is a symmetric second-order tensor, while the tensor 
⊗−⊗uvvu, for any vectors 
,∈uvV, is clearly a skew-symmetric second-order tensor.  We shall denote by 
ˆ
()
r
TV the set of 
all skew-symmetric tensors in (    )
r
TV.  We leave it to the reader to establish the fact that 
ˆ
()
r
TV is 
a subspace of (    )
r
TV.  Elements of 
ˆ
()
r
TV are often called r-vectors or r-forms. 
 
Theorem 36.1.  An element 
ˆ
()
r
∈ATV assumes the value 0 if any two of its variables coincide. 
 
Proof:  We wish to establish that 
 
 
1
(    ,...,   ,...,   ,...,)
r
=Avvvv0                                                 (36.4)                                                 
 
This result is a special case of the formula 
 
 
11
(    ,...,,...,    ,...,)(    ,...,    ,...,,...,)
strtsr
=−=AvvvvAvvvv0                          (36.5)                          
 
which follows by the fact that 1
σ
ε=− for the permutation which switches the pair of indices (,)st) 
while leaving the remaining indices unchanged.   If we take 
st
==vv v in (36.5), then (36.4) 
follows. 
 
Corollary.  An element 
ˆ
()
r
∈ATV assumes the value zero if it is evaluated on a linearly 
dependent set of vectors. 
 
This corollary generalizes the result of the preceding theorem but is itself also a direct 
consequence of that theorem, for if 
{}
1
,...,
r
vv
 is a linearly dependent set, then at least one of the 
vectors can be expressed as is a linear combination of the remaining ones.  From the 
r-linearity of 
A
, 
1
(    ,...,)
r
Avv can then be written as the linear combination of quantities which are all equal to 
zero because which are the values of 
A at arguments having at least two equal variables. 
 
Corollary.  If .
r is greater than N , the dimension of V , then 
{}
ˆ
()
r
=0TV. 
 
This corollary follows from the last corollary and the fact that every set of more than 
N 
vectors in a 
N- dimensional space is linearly dependent. 
 
 
Exercises 
 
36.1     Show that the set of symmetric tensors of order 
r forms a subspace of (    )
r
TV. 
36.2     Show     that     

Sec. 36 • Skew-Symmetric and Symmetric Tensors 249 
 
 
στστ
=TTT 
 
for all permutations 
σ and τ.  Also, show that 
σ
T is the identity automorphism of (    )
r
TVif 
and only if 
σ is the identity permutation. 

250                                                      Chap.                                                      8                                                      •                                                      EXTERIOR ALGEBRA 
 
Section 37    The Skew-Symmetric Operator 
 
In this section we shall construct a projection from (    )
r
TV into (    )
r
TV.  This projection is 
called the 
skew-symmetric operator.  If            (    )
r
∈ATV, we define the skew-symmetric projection 
r
AK 
of 
A
 by 
 
 
1
!
r
r
σσ
σ
ε=
∑
AAKT                                                      (37.1)                                                      
 
where the summation is taken over all permutations 
σ of 
{}
1,...,r.  The endomorphism 
 
                                                                :()()
rrr
→TVTVK                                                     (37.2)                                                     
 
defined in this way is called the 
skew-symmetric operator. 
 
Before showing that 
r
K has the desired properties, we give one example first.  For simplicity, let 
us choose 
2r=
.  Then there are only two permutations of 
{}
1, 2, namely 
 
 
1212
,
1221
σσ
⎛⎞  ⎛⎞
==
⎜⎟  ⎜⎟
⎝⎠  ⎝⎠
                                                (37.3)                                                
 
and their parities are 
 
                                                                   1and1
σσ
εε==−                                                   (37.4)                                                   
 
Substituting (37.3) and (37.4) into (37.1), we get 
 
 
()
2121221
1
()(,)(,)(,)
2
=+AvvAvv    AvvK                                     (37.5)                                     
 
for all 
2
()∈ATV and 
12
,∈vvV.  In particular, if A is skew-symmetric, namely 
 
 
2112
(,)(, )=−Av  vAv v                                                    (37.6)                                                    
 
then (37.5) reduces to 
 
 
21212
()(,)(,)=AvvAvvK 
 
or, equivalently, 
 

Sec. 37               •               The Skew-Symmetric Operator 251 
 
22
ˆ
,()
=∈AAATVK                                                 (37.7)                                                 
 
Since from (37.5), 
22
ˆ
()∈ATVK
for any 
2
()∈ATV????, by (37.7) we then have 
 
 
222
()=AAKKK 
 
or, equivalently 
 
 
2
22
=KK
                                                               (37.8)                                                               
 
which means that 
2
K is a projection.  Hence, for second order tensors, 
2
K has the desired 
properties.  We shall now prove the same for tensors in general. 
 
Theorem 37.1.  Let :(    )(    )
rrr
→TVTVK be defined by (37.1).  Then the range of 
r
K is 
ˆ
()
r
TV, 
namely 
 
 
ˆ
()   ()
rr
R=TVK                                                          (37.9)                                                          
 
Moreover, the restriction of 
r
K on 
ˆ
()
r
TV is the identity automorphism of 
ˆ
()
r
TV, i.e., 
 
 
ˆ
,()
rr
=∈AAATVK                                               (37.10)                                               
 
Proof.  We prove the equation (37.10) first.  If 
ˆ
()
r
∈ATV, then by definition we have 
 
 
σσ
ε=AAT                                                           (37.11)                                                           
 
 
for all permutations 
σ.  Substituting (37.11) into (37.1), we get 
 
 
2
11
(!)
!!
r
r
rr
σ
σ
ε===
∑
AAAAK                                          (37.12)                                          
 
Here we have used the familiar fact that there are a total of 
!r
 permutations for r numbers 
{}
1,...,r
. 
 
Having proved (37.10), we can conclude immediately that 
 
 
ˆ
()()
rr
R⊃TVK                                                       (37.13)                                                       
 
since 
ˆ
()
r
TV is a subspace of (    )
r
TV.  Hence, to complete the proof it suffices to show that 

252                                                      Chap.                                                      8                                                      •                                                      EXTERIOR ALGEBRA 
 
 
ˆ
()    ()
rr
R⊂TVK 
 
This condition means that 
 
                                                              ()
rrττ
ε=AATKK                                                      (37.14)                                                      
 
for all (    )
r
∈ATV.  From (37.1), ()
rτ
ATK is given by 
 
 
()
1
()
!
r
r
τστσ
σ
ε=
∑
AATKTT 
 
From the resu1t of Exercise 36.2, we can rewrite the preceding equation as 
 
 
1
()
!
r
r
τστσ
σ
ε=
∑
AATKT                                                 (37.15)                                                 
 
 
Since the set of all permutations form a group, and since 
 
 
τστσ
εεε= 
 
the right-band side of (37.15) is equal to 
 
 
1
!
r
ττστσ
σ
εε
∑
AT 
 
or, equivalently, 
 
 
1
!
r
ττστσ
σ
εε
∑
AT 
 
which is simply another way of writing 
rτ
εKA, so (37.14) is proved. 
 
By exactly the same argument leading to (37.14) we can prove also that 
 
                                                                ()
rrττ
ε=AAKTK                                                      (37.16)                                                      
 
Hence 
r
K and 
τ
T commute for all permutations τ.  Also, (37.9) and (37.10) now imply that 
2
rr
=KK, for all 
r
.  Hence we have shown that 
r
K is a projection from (    )
r
TV to 
ˆ
()
r
TV.  From a 
result for projections in general [cf. equation (17.13)], (    )
r
TV can be decomposed into the direct 
sum 

Sec. 37               •               The Skew-Symmetric Operator 253 
 
 
ˆ
()()( )
rrr
K=⊕TVTVK                                                (37.17)                                                
 
where     ()
r
KK is the: kernel of 
r
K and is characterized by the following theorem. 
 
Theorem 37.2.  The kernel     ()
r
KK is generated by the set of simple tensors 
1r
⊗⋅⋅⋅⊗vv having at 
least one pair of equal vectors among the vectors 
1
,...,
r
vv. 
 
Proof.  :  Since     (    )
r
TV is generated by simple tensors, it suffices to show that the difference 
 
 
()
11rr
r
⊗⋅⋅⋅⊗  −⊗⋅⋅⋅⊗vvvvK                                          (37.18)                                          
 
can be expressed as a linear combination of simple tensors having the prescribed property.  From 
(36.3) and (37.1), the difference (37.18) can be written as 
 
 
()
1(1)()
1
!
rr
r
σσ
σ
σ
ε⊗⋅⋅⋅⊗  −⊗⋅⋅⋅⊗
∑
vvv    v                                   (37.19)                                   
 
We claim that each sum of the form 
 
 
1(1)()rrσσ
σ
ε⊗⋅⋅⋅⊗  −⊗⋅⋅⋅⊗vvv    v                                         (37.20)                                         
 
can be expressed as a sum of simple tensors each having at least two equal vectors among the 
r 
vectors forming the tensor product.  This fact is more or less obvious since in Section 20 we have 
mentioned that every permutation 
σ
 can be decomposed into a product of permutations each 
switching only one pair of indices, say 
 
 
121kk
σσσ   σσ
−
=⋅⋅⋅                                                     (37.21)                                                     
 
where the number 
k is even or odd corresponding to σ being an even or odd permutation, 
respectively.  Using the decomposition (37.21), we can rewrite (37.20) in the form 
 
 
()
()
()
11
112121
1111
(1)(  )
1
(1)(  )(1)(  )
(1)(  )
(1)(  )
kk
r
r
rr
r
r
σσ
σσσσσσ
σσσσ
σσ
σ
ε
−−
⋅⋅⋅⋅⋅⋅
+   ⊗⋅⋅⋅⊗  +    ⊗⋅⋅⋅⊗
−⊗⋅⋅⋅⊗+⊗⋅⋅⋅⊗
+⋅⋅⋅
−⋅⋅⋅
−⊗⋅⋅⋅⊗+    ⊗⋅⋅⋅⊗
vvvv
vvv  v
vvvv
                            (37.22)                            
 
where all the intermediate terms 
11
(1)(  )
jj
rσσσσ⋅⋅⋅⋅⋅⋅
⊗⋅⋅⋅⊗vv, 1,...,1jk=− cancel in the sum.  Since 
each of 
1
,...,
k
σσ switches only one pair of indices, a typical term in the sum (37.22) has the form 

254                                                      Chap.                                                      8                                                      •                                                      EXTERIOR ALGEBRA 
 
 
astbatsb
⊗⋅⋅⋅  ⋅⋅⋅  ⋅⋅⋅⊗  +  ⊗⋅⋅⋅  ⋅⋅⋅  ⋅⋅⋅⊗vvvvvvvv 
 
which can be combined into three simple tensors: 
 
                                                                      ()  ()
ab
⊗⋅⋅⋅  ⋅⋅⋅   ⋅⋅⋅⊗vuuv 
 
where 
,
st
=uvv, and 
st
+vv
.  Consequently, (37.22) is a sum of simple tensors having the 
property prescribed by the theorem.  Of course, from (37.16) all those simple tensors belong to the 
kernel     ()
r
KK.  The proof is complete. 
 
In view of the decomposition (37.17), the subspace 
ˆ
()
r
TV is isomorphic to the factor space 
() ( )
rr
KTVK.  In fact, some authors use this structure to define the space 
ˆ
()
r
TVabstractly 
without making 
ˆ
()
r
TV a subspace of (    )
r
TV.  The preceding theorem shows that this abstract 
definition of 
ˆ
()
r
TV is equivalent to ours. 
 
The next theorem gives a useful property of the skew-symmetric operator 
r
K. 
 
Theorem 37.3.  If (    )
p
∈ATV and (    )
q
∈BTV, then 
 
 
()
()
()
()
pqpqq
pqp
pqpq
++
+
+
⊗=    ⊗
=⊗
=⊗
ABAB
AB
AB
KKK
KK
KKK
                                        (37.23)                                        
 
Proof.  Let  
τ be an arbitrary permutation of 
{}
1,...,q.  We define 
 
 
122
12(1)()
pppq
pppq
σ
ττ
⋅⋅⋅+  ⋅⋅⋅   +
⎛⎞
=
⎜⎟
⋅⋅⋅+   ⋅⋅⋅  +
⎝⎠
 
 
 
Then 
στ
εε= and 
 
 
()
τσ
⊗= ⊗AB  ABTT 
 
 
Hence from (37.14) we have 
 

Sec. 37               •               The Skew-Symmetric Operator 255 
 
()()
()
()
()
pqpqpq
pq
τσ σ
τ
ε
ε
+++
+
⊗=⊗=    ⊗
=⊗
ABABAB
AB
KTKTK
K
 
 
 
or, equivalently, 
 
 
()()
pqpqττ
ε
++
⊗= ⊗ABABKTK                                         (37.24)                                         
 
Summing (37.24) over all 
τ, we obtain (37.23)
1
,  A similar argument implies (37.23). 
 
In closing this section, we state without proof an expression for the skew-symmetric 
operator in terms of the components of its tensor argument.  The formula is 
 
 
11
11
...
......
1
!
rr
rr
iijj
rjjii
A
r
δ=⊗⋅⋅⋅⊗AeeK                                          (37.25)                                          
 
We leave the proof of this formula as an exercise to the reader.  A classical notation for the 
components of 
r
AK is 
1
[... ]
r
jj
A
, so from (37.25) 
 
 
1
111
...
[    ...    ]......
1
!
r
rrr
ii
jjjj ii
AA
r
δ=                                                   (37.26)                                                   
 
Naturally, we call 
r
AK the skew-symmetric part of A.  The formula (37.25) and the quotient 
theorem mentioned in Exercise 33.7 imply that the component formula (33.32) defines a tensor 
r
K 
of order 
2r, a fact proved in Section 33 by means of the transformation law. 
 
 
Exercises 
 
37.1
 Let            (    )
r
∈ATV and define an endomorphism 
r
S of (    )
r
TVby 
 
 
1
!
r
r
σ
σ
≡
∑
AAST 
 
where the summation is taken over all permutations 
σ of 
{}
1,...,r
 as in (37.1).  Naturally 
r
S is called the symmetric operator.  Show that it is a projection from (    )
r
TV into the 
subspace consisting of completely symmetric tensors of order 
r.  What is the kerne1 
()
r
KS?  A classical notation for the components of 
r
AS is 
1
(... )
r
jj
A. 
37.2
 Prove the formula (37.25). 
 

256                                                      Chap.                                                      8                                                      •                                                      EXTERIOR ALGEBRA 
 
Section 38.   The Wedge Product 
 
In Section 33 we defined the concept of the tensor product 
⊗, first for vectors, then 
generalized to tensors.  We pointed out that the tensor product has the important universal 
factorization property.  In this section we shall define a similar operation, called the wedge product 
(or the exterior product), which we shall denote by the symbol 
∧.  We shall define first the wedge 
product of any set of vectors. 
 
If 
()
1
,...,
r
vv is any r-triple of vectors, then their tensor product 
1r
⊗⋅⋅⋅⊗vv is a simple 
tensor in (    )
r
TV [cf. equation (33.10)].  In the preceding section, we have introduced the skew-
symmetric operator 
r
K, which is a projection from (    )
r
TV onto 
ˆ
()
r
TV.  We now define 
 
 
()()
11  1
,...,!
rrr
r
r∧⋅⋅⋅∧  ≡∧≡⊗⋅⋅⋅⊗vvvvv vK                                (38.1)                                
 
for any vectors 
1
,...,
r
vv.  For example, if 
2r=
, from (37.5) for the special case that 
12
=⊗Av  v 
we have 
 
 
12 1 22 1
∧=⊗−⊗vv v vv v                                                 (38.2)                                                 
 
In general, from (37.1) and (36.3) we have 
 
 
()
()
()
11
11
(1)(  )
(1)(  )
rr
r
r
σσ
σ
σσ
σ
σ
σσ
σ
σ
ε
ε
ε
−−
∧⋅⋅⋅∧  =⊗⋅⋅⋅⊗
=⊗⋅⋅⋅⊗
=⊗⋅⋅⋅⊗
∑
∑
∑
vvv v
vv
vv
T
                                     (38.3)                                     
 
where in deriving (38.3), we have used the fact that 
 
 
1
σ
σ
εε
−
=
                                                               (38.4)                                                               
 
and the fact that the summation is taken over all permutations 
σ of 
{}
1,...,r
. 
 

Sec. 38               •               The Wedge Produce  257 
We can regard (38.1)
2
 as the definition of the operation 
 
 
 times
ˆ
:()
p
r
∧×⋅⋅⋅×→VVTV
	

                                                  (38.5)                                                  
 
which is called the operation of wedge product.  From (38.1)
3
 it is clear that this operation, like the 
tensor product, is multilinear; further, 
∧ is a (completely) skew-symmetric operation in the sense 
that 
 
 
(1)(  )1
(,...,)(    ,...,)
rrσσ
σ
ε∧=∧vvvv                                           (38.6)                                           
 
for all 
1
,...,
r
∈vvV and all permutations σ of 
{}
1,...,r.  This fact follows directly from the skew 
symmetry of 
r
K [cf. (37.16)].  Next we show that the wedge product has also a universal 
factorization property which is the condition asserted by the following. 
 
Theorem 38.1.  If W is an arbitrary completely skew-symmetric multilinear transformation 
 
 
 times
:
r
×⋅⋅⋅×  →WVVU
	

                                                     (38.7)                                                     
 
where 
U is an arbitrary vector space, then there exists a unique linear transformation 
 
 
ˆ
:()
r
→TVUD                                                          (38.8)                                                          
 
such that 
 
 
()()
11
,...,
rr
=∧⋅⋅⋅∧WvvvvD                                             (38.9)                                             
 
for all 
1
,...,
r
∈vvV.  In operator form, (38.9) means that 
 
 
=∧WDD                                                           (38.10)                                                           
 
Proof   We can use the universal factorization property of the tensor product (cf. Theorem 34.1) to 
decompose 
W by 
 

258                                                      Chap.                                                      8                                                      •                                                      EXTERIOR ALGEBRA 
 
=⊗WCD                                                           (38.11)                                                           
 
where 
C is a linear transformation from (    )
r
TV to U, 
 
                                                                   :()
r
→TVUC                                                        (38.12)                                                        
 
Now in view of the fact that 
W
 is skew-symmetric, we see that the particular linear transformation 
C has the property 
 
 
()()
(1)(  )1rrσσ
σ
ε⊗⋅⋅⋅⊗=⊗⋅⋅⋅⊗vv  vvCC                                  (38.13)                                  
 
for all simple tensors 
1
()
r
r
⊗⋅⋅⋅⊗  ∈vvTV and all permutations σ of 
{}
1,...,r.  Consequently, if 
we multiply (38.13) by 
σ
ε and sum the result over all σ, then from the linearity of C and the 
definition (38.3) we have 
 
 
()()
()
11
1
!
!,...,
rr
r
r
r
∧⋅⋅⋅∧   =⊗⋅⋅⋅⊗
=
vv   v v
Wvv
CC
                                       (38.14)                                       
 
for all 
1
,...,
r
∈vvV.  Thus the desired linear transformation 
D
 is simply given by 
 
 
ˆ
()
1
!
r
r
=
TV
DC                                                          (38.15)                                                          
 
where the symbol on the right-hand denotes the restriction of C on 
ˆ
()
r
TV as usual. 
 
Uniqueness of D can be proved in exactly the same way as in the proof of Theorem 34.1.  
Here we need the fact that the tensors of the form 
1
ˆ
()
r
r
∧⋅⋅⋅∧  ∈vvTV, which may be called 
simply skew-symmetric tensors for an obvious reason, generate the space 
ˆ
()
r
TV.  This fact is a 
direct consequence of the following results: 
 
(i)
 ()
r
TV is generated by the set of all simple tensors 
1r
⊗⋅⋅⋅⊗vv. 
(ii)
 
r
K is a linear trans formation from (    )
r
TV onto 
ˆ
()
r
TV. 

Sec. 38               •               The Wedge Produce  259 
(iii)
 Equation (38.1) which defines the simple skew-symmetric tensors 
1r
∧⋅⋅⋅∧vv. 
 
Knowing that the simple skew-symmetric tensors 
1r
∧⋅⋅⋅∧vv
 form a generating set of 
ˆ
()
r
TV, we can conclude immediately that the linear transformation 
D
 is unique, since its values 
on the generating set are uniquely determined by 
W
 through the basic condition (38.9). 
 
As remarked before, the universal factorization property can be used to define the tensor 
product abstractly.  The preceding theorem shows that the same applies to the wedge product.  In 
fact, by following this abstract approach, one can define the vector space 
ˆ
()
r
TV entirely 
independent of the vector space (    )
r
TV and the operation ∧ entirely independent of the operation 
⊗. 
 
Having defined the wedge product for vectors, we can generalize the operation easily to 
skew-symmetric tensors.  If 
ˆ
()
p
∈ATV and 
ˆ
()
q
∈BTV, then we define 
 
                                                                            ()
r
r
p
⎛⎞
∧=⊗
⎜⎟
⎝⎠
ABA BK                                                 (38.16)                                                 
 
where, as before, 
 
 
rpq=+                                                             (38.17)                                                             
 
and 
 
 
(1)(1)    !
!!!
r
rrr   pr
p
ppq
⎛⎞
−⋅⋅⋅−+
==
⎜⎟
⎝⎠
                                        (38.18)                                        
 
We can regard (38.16) as the definition of the wedge product from 
ˆˆ  ˆ
()    ()()
pq    r
×→TVTVTV, 
 
 
ˆˆ  ˆ
:()  ()()
pq    r
∧×→TVTVTV                                             (38.19)                                             
 
Clearly, this operation is bilinear and is characterized by the condition that 
 

260                                                      Chap.                                                      8                                                      •                                                      EXTERIOR ALGEBRA 
 
1111
()()
pqpq
∧⋅⋅⋅∧   ∧   ∧⋅⋅⋅∧   =  ∧⋅⋅⋅∧  ∧  ∧⋅⋅⋅∧vvuuvvuu                      (38.20)                      
 
for all simple skew-symmetric tensors 
1
ˆ
()
p
p
∧⋅⋅⋅∧  ∈vvTV and 
1
ˆ
()
q
q
∧⋅⋅⋅∧  ∈uuTV.  We leave 
the proof of this simple fact as an exercise to the reader.  In component form, the wedge product 
∧AB is given by 
 
 
11
111
...
.........
1
!!
rr
rppr
iijj
jj ii i   i
AB
pq
δ
+
∧  =⊗⋅⋅⋅⊗ABee                                  (38.21)                                  
 
relative to the product basis of any basis 
{}
j
e for 
V
. 
 
In view of (38.20), we can generalize the wedge product further to an arbitrary member of 
skew-symmetric tensors.  For example, if 
ˆ
()
a
∈ATV, 
ˆ
()
b
∈BTV and 
ˆ
()
c
∈CTV, then we have 
 
                                       ()()
∧∧=∧∧=∧∧AB CA BC  ABC                                    (38.22)                                    
 
Moreover, 
∧∧ABC is also given by 
 
 
()
()
()!
!!!
abc
abc
abc
++
++
∧∧=⊗⊗ABCA B CK
                                (38.23)                                
 
Further, the wedge product is multilinear in its arguments and is characterized by the associative 
law such as 
 
 
()()()
11 1
11 1
ab   c
ab   c
∧⋅⋅⋅∧   ∧   ∧⋅⋅⋅∧   ∧   ∧⋅⋅⋅∧
=  ∧⋅⋅⋅∧  ∧  ∧⋅⋅⋅∧  ∧  ∧⋅⋅⋅∧
vvuuww
vvuuww
                      (38.24)                      
 
for all simple tensors involved. 
 
From (38.24), the skew symmetry of the wedge product for vectors [cf. (38.6)] can be 
generalized to the condition such as 
 
                                                                          (1)
pq
∧=−   ∧BAAB                                                   (38.25)                                                   
 

Sec. 38               •               The Wedge Produce  261 
for all 
ˆ
()
p
∈ATV and 
ˆ
()
q
∈BTV.  In particular, if A is of odd order, then 
 
 
∧=AA0                                                           (38.26)                                                           
 
since from (38.25) if the order 
p of A is odd, then 
 
 
2
(1)
p
∧=−  ∧=−∧AAAA   AA                                          (38.27)                                          
 
A special case of (38.26) is the elementary result that 
 
 
∧=vv0                                                            (38.28)                                                            
 
 
which is also obvious from (38.2). 
 
 
Exercises 
 
38.1     Verify (38.6), (38.20), (38.22)
1
, and (38.23). 
38.2     Show that (38.23) can be rewritten as 
 
 
1
1
11...
...
r
r
ii
rr
ii
δ∧⋅⋅⋅∧  =⊗⋅⋅⋅⊗vvv  v                                           (38.29)                                           
 
where the repeated indices are summed from 1  to r. 
38.3     Show     that     
 
 
11
1
1
1
1
(    ,...,)det
r
r
r
rr
r
⎡⎤
⋅⋅⋅⋅  ⋅
⎢⎥
⋅⋅
⎢⎥
⎢⎥
∧⋅⋅⋅∧=
⋅⋅
⎢⎥
⋅⋅
⎢⎥
⎢⎥
⋅⋅⋅⋅  ⋅
⎣⎦
vuvu
vvuu
vuvu
                        (38.30)                        
 
for all vectors involved. 

262                                                      Chap.                                                      8                                                      •                                                      EXTERIOR ALGEBRA 
38.4
 Show that 
 
 
11
121
...
...
(,...,)
rr
r
iiii
jj jj
δ∧⋅⋅⋅∧=eeee                                            (38.31)                                            
 
 
for any reciprocal bases 
{}
i
e
 and 
{}
i
e. 

Sec. 39               •               Product Bases and Strict Components 263 
 
Section 39.   Product Bases and Strict Components 
 
In the preceding section we have remarked that the space 
ˆ
()
r
TV is generated by the set of 
all simple skew-symmetric tensors of the form 
1r
∧⋅⋅⋅∧vv.  This generating set is linearly 
dependent, of course.  In this section we shall determine a linear1y independent generating set and 
thus a basis for 
ˆ
()
r
TV consisting entirely of simple skew-symmetric tensors .  Naturally, we call 
such a basis a basis a product basis for 
ˆ
()
r
TV. 
 
Let 
{}
i
e be a basis for V as usual.  Then the simple tensors 
{}
1r
ii
⊗⋅⋅⋅⊗ee form a product 
basis for (    )
r
TV.  Since 
ˆ
()
r
TV is a subspace of (    )
r
TV, every element 
ˆ
()
r
∈ATV has the 
representation 
 
 
1
1
...
r
r
ii
ii
A=⊗⋅⋅⋅⊗Ae   e
                                                     (39.1)                                                     
 
where the repeated indices are summed from 1  to N, the dimension of (    )
r
TV.  Now, since 
ˆ
()
r
∈ATV, it is invariant under the skew-symmetric operator 
r
K, namely 
 
 
()
1
1
...
r
r
ii
riir
A=    =⊗⋅⋅⋅⊗AAeeKK                                           (39.2)                                           
 
Then from (38.1) we can rewrite the representation (39.1) as 
 
 
1
1
...
1
!
r
r
ii
ii
A
r
=∧⋅⋅⋅∧Aee
                                                   (39.3)                                                   
 
Thus we have shown that the set of simple skew-symmetric tensors 
{}
1r
ii
∧⋅⋅⋅∧ee already forms a 
generating set for 
ˆ
()
r
TV. 
 
The generating set 
{}
1r
ii
∧⋅⋅⋅∧ee is still not linearly independent, however.  This fact is 
easily seen, since the wedge product is skew-symmetric, as shown by (38.6).  Indeed, if 
1
i and 
2
i 
are equal, then 
1r
ii
∧⋅⋅⋅∧ee must vanish.  In general if σ is any permutation of 
{}
1,...,r, then 
(1)(   )r
ii
σσ
∧⋅⋅⋅∧ee is linearly related to 
1r
ii
∧⋅⋅⋅∧ee by 
 
 
(1)(   )
1
r
r
ii
ii
σσ
σ
ε∧⋅⋅⋅∧    =    ∧⋅⋅⋅∧eeee                                             (39.4)                                             
 
Hence if we eliminate the redundant elements of the generating set 
{}
1r
ii
∧⋅⋅⋅∧ee by restricting the 
range of the indices 
1
(  ,...,   )
r
ii in such a way that 
 

264                                                      Chap.                                                      8                                                      •                                                      EXTERIOR ALGEBRA 
 
1r
ii<⋅⋅⋅<                                                              (39.5)                                                              
 
 
the resulting subset 
{}
1
1
,
r
ii
r
ii∧⋅⋅⋅∧    <⋅⋅⋅<ee remains a generating set of 
ˆ
()
r
TV.  The next theorem 
shows that this subset is linearly independent and thus a basis for 
ˆ
()
r
TV, called the product basis. 
 
Theorem 39.1.  The set 
{}
1
1
,
r
ii
r
ii∧⋅⋅⋅∧    <⋅⋅⋅<ee is  linearly independent. 
 
Proof.  Suppose that the set 
{}
1
1
,
r
ii
r
ii∧⋅⋅⋅∧    <⋅⋅⋅<ee obeys the homogeneous linear equation 
 
 
1
1
1
...
r
r
r
ii
ii
ii
C
<⋅⋅⋅<
∧⋅⋅⋅∧
∑
ee
                                                     (39.6)                                                     
 
where 
{}
1
...1
,
r
iir
Cii<⋅⋅⋅< are scalars, and where the summation is taken over all indices 
1
,...,
r
ii 
from 1  to 
N
 subject to the condition (39.5).  Then we must show that 
1
...
r
ii
C vanishes completely.  
From (38.31), if we evaluate the tensor (39.6) at the argument 
()
1
,...,
r
ii
ee, where 
{}
j
e denotes the 
reciprocal basis of 
{}
i
e as usual, we get 
 
 
1
11
1
...
......
0
r
rr
r
ii
ii  j j
ii
Cδ
<⋅⋅⋅<
=
∑
                                                       (39.7)                                                       
 
for all 
1
,...,
r
jj ranging from 1  to N.  In particular, if we choose 
1r
jj<⋅⋅⋅<, then the summation 
reduces to only one term, namely 
1
...
r
jj
C
 and the equation yields 
 
 
1
...
0
r
jj
C= 
 
which is the desired result. 
 
It is easy to see that there are only 
N
r
⎛⎞
⎜⎟
⎝⎠
 number of elements in the product basis 
 
 
{}
1
1
,
r
ii
r
ii∧⋅⋅⋅∧    <⋅⋅⋅<ee                                                  (39.8)                                                  
 
Thus we have 
 
 
!
ˆ
dim(    )
!()!
r
N
N
r
rN r
⎛⎞
==
⎜⎟
−
⎝⎠
TV                                             (39.9)                                             
 
In particular, we recover the coro11ary of Theorem 36.1, that is 

Sec. 39               •               Product Bases and Strict Components 265 
 
 
ˆ
dim(    )0
r
=TV 
 
if 
rN>. 
 
Returning now to the representation (39.3) for an arbitrary skew-symmetric tensor 
ˆ
()
r
∈ATV, we can rewrite that representation as 
 
 
1
1
1
...
r
r
r
ii
ii
ii
A
<⋅⋅⋅<
=∧⋅⋅⋅∧
∑
Aee
                                               (39.10)                                               
 
The reason that we can replace the full summation in (39.3) by the restricted summation is because 
for each increasing 
r-tuple 
1
(  ,...,   )
r
ii there are precisely !r permutations of 
{}
1
,...,
r
ii.  Further, their 
corresponding 
!r terms in the full summation (39.3) are all equal to one another, since both 
1
...
r
ii
A 
and 
1r
ii
∧⋅⋅⋅∧ee
 are completely skew-symmetric in the indices 
1
(  ,...,   )
r
ii, so for any permutation σ 
of 
{}
1
,...,
r
ii
 we have 
 
 
(1)(   )
11
(1)(   )11
2
.........
r
rr
rrr
ii
ii ii
iiiiii
AAA
σσ
σσ
σ
ε∧⋅⋅⋅∧    =∧⋅⋅⋅∧  =∧⋅⋅⋅∧e   eee ee 
 
In view of (39.10), we see that the scalars 
 
 
{}
1
...1
,
r
iir
Aii<⋅⋅⋅< 
 
are the components of A relative to the product basis (39.8).  For definiteness, we call these 
scalars the 
strict components of A. 
 
As an illustration of this concept, let us compute the strict components of the simple skew-
symmetric tensor 
1
ˆ
()
r
r
∧⋅⋅⋅∧  ∈vvTV.  As usual we represent the vector 
i
v
 in component form 
relative to 
{}
j
e by 
 
 
iij
j
v=ve 
 
for all 1,...,
ir=.  Using the skew-symmetry of the wedge product, we have 
 
 
111
111
1
...
111
...
rrr
rrr
r
jjjjii
rrr
jjjjii
ii
vvvvδ
<⋅⋅⋅<
∧⋅⋅⋅∧  =  ⋅⋅⋅∧⋅⋅⋅∧   =⋅⋅⋅∧⋅⋅⋅∧
∑
vve ee e
               (39.11)               
 
which means that the strict components of 
1r
∧⋅⋅⋅∧vv are 
 
 
{}
1
11
...
1
...1
,
r
rr
jj
r
jjiir
vviiδ⋅⋅⋅<⋅⋅⋅< 

266                                                      Chap.                                                      8                                                      •                                                      EXTERIOR ALGEBRA 
 
Next, we consider the transformation rule for the strict components in general. 
 
Theorem 39.2.  Under a change of basis from 
{}
j
e to 
{}
ˆ
j
e, the strict components of a tensor 
ˆ
()
r
∈ATV obey the transformation rule 
 
 
1
111
1
...
.........
ˆ
r
rrr
r
ii
jjjj ii
ii
ATA
<⋅⋅⋅<
=
∑
                                                  (39.12)                                                  
 
where 
1
1
...
...
r
r
ii
jj
T
 is an rr× minor of the transformation matrix 
i
j
T
⎡⎤
⎣⎦
 as defined by (21.21).  Of course, 
i
j
T is given by 
 
 
ii
jj
T=⋅ee 
 
as usual [cf. (31.21)], where 
{}
ˆ
j
e is the reciprocal basis of 
{}
ˆ
j
e. 
 
Proof.  Since the strict components are nothing but the ordinary tensor components restricted to the 
subset of indices in increasing order [cf. (39.5)], we can compute their values in the usual way by 
 
 
11
...
ˆ
ˆˆ
(,...,)
rr
jjjj
A=Aee                                                   (39.13)                                                   
 
Substituting the strict component representation (39.10) into (39.13) and making use of the formula 
(38.30) we obtain 
 
 
()
1
111
1
11
1
1
1
1
......
...
ˆ
ˆˆ
,...,
ˆˆ
det
ˆˆ
r
rrr
r
r
r
r
rr
r
ii
jjiijj
ii
ii
jj
ii
ii
ii
jj
AA
A
<⋅⋅⋅<
<⋅⋅⋅<
=∧⋅⋅⋅∧
⎡⎤
⋅  ⋅⋅⋅  ⋅
⎢⎥
⋅⋅
⎢⎥
⎢⎥
⋅⋅
=
⎢⎥
⋅⋅
⎢⎥
⎢⎥
⋅  ⋅⋅⋅  ⋅
⎣⎦
∑
∑
eeee
eeee
eeee
 
 
which is the desired result. 
 
From (21.25), we can write the transformation rule (39.12) in the form 
 
 
()
1
111
1
...
.........
ˆ
ˆ
detcof
r
rrr
r
jj
a
jjbiiii
ii
AT   TA
<⋅⋅⋅<
⎡⎤
=
⎣⎦
∑
                                      (39.14)                                      
 
As al1 illustration of (39.12), take 
rN=.  Then (39.9) implies 

Sec. 39               •               Product Bases and Strict Components 267 
 
 
ˆ
dim(    )    1
N
=TV                                                        (39.15)                                                        
 
And (39.12) reduces to 
 
 
12...12...
ˆ
det
i
NjN
ATA
⎡⎤
=
⎣⎦
                                                  (39.16)                                                  
 
Comparing (39.16) with (33.15), we see that the strict component of an 
N-vector transforms 
according to the rule of an axial scalar of weight 1 [cf. (33.35)].  
N-vectors are also called densities 
or density tensors. 
 
Next take 
1rN=−.  Then (39.9) implies that 
 
 
1
ˆ
dim(    )
N
N
−
=TV                                                      (39.17)                                                      
 
In this case the transformation rule (39.12) can be rewritten as 
 
 
ˆ
ˆ
det
kikl
jl
ATTA
⎡⎤
=
⎣⎦
                                                     (39.18)                                                     
 
where the quantities 
ˆ
k
A and 
l
A are defined by 
 
 
12...   ..
ˆ
(1)
kNk
kN
AA
−
≡−

                                                    (39.19)                                                    
 
and similarly 
 
 
12...  ..
(1)
lNl
lN
AA
−
≡−

                                                    (39.20)                                                    
 
Here the symbol  ̆ over 
k
 or 
l
 means 
k
 or 
l
 are deleted from the list of indices as before.  To 
prove (39.18), we make use of the alternative form (39.14), obtaining 
 
 
()
12...   ...
12...   ...12...  ...12...  ...
ˆ
ˆ
detcof
akN
b
kNlNl N
l
ATTA
⎡⎤
=
⎣⎦
∑


 
 
Multiplying this equation by  (  1)
Nk−
− and using the definitions (39.19) and (39.20), we get 
 
 
()
12...   ...
12...  ...
ˆ
ˆ
det(  1)cof
kakl kNl
b
lN
l
ATTA
+
⎡⎤
=−
⎣⎦
∑


 
 
But now from (21.23), it is easy to see that the cofactor of the  (1)(1)
NN−− minor 
12...   ...
12...  ...
ˆ
kN
lN
T


 in the 
matrix 
ˆ
a
b
T
⎡⎤
⎣⎦
 is simply  (  1)
kl+
− times the element 
ˆ
k
l
T.  Thus (39.18) is proved. 
 

268                                                      Chap.                                                      8                                                      •                                                      EXTERIOR ALGEBRA 
From (39.19) or (39.20) we see that the quantities 
ˆ
k
A and 
l
A, like the strict components 
12...   ...
ˆ
kN
A

 and 
12...  ...lN
A

 characterize the tensor A completely.  In view of the transformation rule 
(39.18), we see that the quantity 
l
A transforms according to the rule of the component of an axial 
vector of weight 1 [cf. (33.35)].  (1)
N− vectors are often called (axial) vector densities. 
 
The operation given by (39.19) and (39.20) can be generalized to 
ˆ
()
Nr−
TV in general.  If 
1
...
Nr
ii
A
−
 is a strict component of 
ˆ
()
Nr−
∈ATV, then we define a quantity 
1
...
r
jj
A by 
 
 
11
1
1
1
11
1
......
...
...
......
...
1
()!
Nrr
r
Nr
Nr
Nrr
Nr
ii  j j
jj
ii
ii
ii  j j
ii
AA
A
Nr
ε
ε
−
−
−
−
−
<⋅⋅⋅<
=
=
−
∑
                                          (39.21)                                          
 
When       1r=, (39.21) reduces to (39.20).  Recall that 
1
...
N
ii
ε
 transforms according to the rule of an 
axial contravariant tensor of order N and weight 1.  Moreover, since 
1
...
r
jj
A is skew-symmetric in 
1
(   ,...,    )
r
jj, we can write the transformation rule as 
 
 
111
1
1
.........
...
ˆ
ˆ
det
rrr
r
r
jjjj ii
a
bii
ii
ATTA
<⋅⋅⋅<
⎡⎤
=
⎣⎦
∑
                                          (39.22)                                          
 
where 
1
1
...
...
ˆ
r
r
jj
ii
T is an rr× minor of the transformation matrix 
a
b
T
⎡⎤
⎣⎦
.  When 1r=, (39.22) reduces to 
(39.18). 
 
As an illustration of (39.20), we take 
3N=;  then (39.20) yields 
 
 
123
231312
,,AAAAAA==−=
                                     (39.23)                                     
 
As we shall see, this operation is closely related to the classical operation of cross product (or 
vector product) which assigns an axial vector to a skew- symmetric two-vector on a three-
dimensional space. 
 
Before closing this section, we note here a convenient condition for determining whether or 
not a given set of vectors is linearly dependent by examining their wedge product. 
 
Theorem 39.3.  A set of r vectors 
{}
1
,...,
r
vv is linearly dependent if and only if their wedge 
product vanishes, i.e., 
 
 
1r
∧⋅⋅⋅∧  =vv0                                                       (39.24)                                                       
 
Proof.  Necessity is obvious, since if 
{}
1
,...,
r
vv is linearly dependent, then at least one of the 
vectors can be expressed as a linear combination of the other vectors, say 

Sec. 39               •               Product Bases and Strict Components 269 
 
 
11
11
rr
r
αα
−
−
=+⋅⋅⋅+vvv                                                 (39.25)                                                 
 
Then (39.24) follows because 
 
 
1
111
1
r
rrj
j
j
α
−
−
=
∧⋅⋅⋅∧  =∧⋅⋅⋅∧   ∧  =
∑
vv  vvv0 
 
as required by the skew symmetry of the wedge product.  Converse1y, if 
{}
1
,...,
r
vv is linearly 
independent, then it can be extended to a basis 
{}
1
,...,
N
vv (cf. Theorem 9.8).  From Theorem 39.1, 
the simple skew-symmetric tensor 
1N
∧⋅⋅⋅∧vv forms a basis for the one-dimensional space 
ˆ
()
N
TV and thus is nonzero, so that this factor 
1r
∧⋅⋅⋅∧vv
 is also nonzero. 
 
Corollary.  A set of N covectors 
{}
1
,...,
N
vv is a basis for V if and only if 
1N
∧⋅⋅⋅∧   ≠vv0. 
 
 
Exercises 
 
39.1     Show that the product basis of 
ˆ
()
r
TV satisfies the following transformation rule: 
 
 
111
1
1
...
...
ˆˆ
rrr
r
r
iiiij j
jj
jj
T
<⋅⋅⋅<
∧⋅⋅⋅∧  =∧⋅⋅⋅∧
∑
eee e
                                      (39.26)                                      
 
 
re1ative to any change of basis from 
{}
ˆ
i
e to 
{}
j
e with transformation matrix 
i
j
T
⎡⎤
⎣⎦
. 
39.2
 For the skew-symmetric tensor space 
ˆ
()
r
TV it is customary to define the inner produce 
 
 
ˆˆ
:()   ()
rr
∗×→TVTVR                                                  (39.27)                                                  
 
by requiring the product basis of an orthonormal basis be orthonormal with respect to 
∗.  In 
other words, relative to an orthonormal basis 
{}
1
,...,
N
ii the inner produce ∗AB of any 
ˆ
,()
r
∈ABTV is given by 
 
 
11
1
......
rr
r
ii  ii
ii
AB
<⋅⋅⋅<
∗=
∑
AB
                                                  (39.28)                                                  
 
Show that this inner product is related to the ordinary inner product 
⋅ on 
ˆ
()
r
TV, regarded 
as a subspace of (    )
r
TV, by 
 

270                                                      Chap.                                                      8                                                      •                                                      EXTERIOR ALGEBRA 
 
1
!r
∗=⋅ABAB                                                       (39.29)                                                       
 
 
for all 
ˆ
,()
r
∈ABTV. 
39.3
 Relative to the inner product 
∗
, show that 
 
 
()()
111
11
1
det
r
rr
rrr
⎡⎤
⋅⋅⋅⋅  ⋅
⎢⎥
⋅⋅
⎢⎥
⎢⎥
∧⋅⋅⋅∧   ∗   ∧⋅⋅⋅∧   =
⋅⋅
⎢⎥
⋅⋅
⎢⎥
⎢⎥
⋅⋅⋅⋅  ⋅
⎣⎦
vuvu
vvuu
vuvu
                   (39.30)                   
 
 
for all simple skew- symmetric tensors 
1r
∧⋅⋅⋅∧vv and 
1r
∧⋅⋅⋅∧uu in 
ˆ
()
r
TV. 
39.4
 Relative to the product basis of any basis 
{}
i
e, show that the inner product ∗AB has the 
representation 
 
 
111
11
1
1
1
......
det
r
rr
r
r
rrr
ijij
ii  j j
ii
jj
ijij
ee
AB
ee
<⋅⋅⋅<
<⋅⋅⋅<
⎡⎤
⋅⋅⋅
⎢⎥
⋅⋅
⎢⎥
⎢⎥
∗=
⋅⋅
⎢⎥
⋅⋅
⎢⎥
⎢⎥
⋅⋅⋅
⎣⎦
∑
AB
                             (39.31)                             
 
 
where 
ij
e
 is given by 
 
 
ijij
e=⋅ee                                                            (39.32)                                                            
 
as before [cf. (14.8)].  In particular, if 
{}
i
e is orthonormal, then 
ijij
eδ=
 and (39.31) 
reduces to (39.28). 
39.5 Prove the transformation rule (39.22).

Sec. 40               •               Determinant and Orientation 271 
 
Section 40.   Determinant and Orientation 
 
In the preceding section we have shown that the strict component of an N-vector over an N- 
dimensional space 
V transforms according to the rule of an axial scalar of weight 1.  For brevity, 
we call an N-vector simply a density or a density tensor.  From (39.15), the space 
ˆ
()
N
TV of 
densities on 
V is one-dimensional, and for any basis 
{}
i
e a product basis for 
ˆ
()
N
TV
 is 
1N
∧⋅⋅⋅∧ee.  If D is a density, then it has the representation 
 
 
1
12...
N
N
D=∧⋅⋅⋅∧Dee
                                                     (40.1)                                                     
 
where the strict component 
12...N
D is given by 
 
 
()
12...1
,...,
NN
D=Dee
                                                     (40.2)                                                     
 
{}
i
e being the reciprocal basis of 
{}
j
e as usual.  The representation (40.1) shows that every 
density is a simple skew-symmetric tensor, e.g., we can represent 
D by 
 
 
()
1
12...
N
N
D=∧⋅⋅⋅∧Dee                                                   (40.3)                                                   
 
This representation is not unique, of course. 
 
Now if 
A
 is an endomorphism of V, we can define a linear map 
 
 
ˆˆ
:()()
NN
f→TVTV                                                     (40.4)                                                     
 
by the condition 
 
 
()()()
11NN
f∧⋅⋅⋅∧   =∧⋅⋅⋅∧vvAvAv                                       (40.5)                                       
 
for all 
1
,...,
N
∈vvV.  We can prove the existence and uniqueness of the linear map f by the 
universal factorization property of 
ˆ
()
N
TV as shown by Theorem 38.1.  Indeed, we define first the 
skew-symmetric multilinear map 
 
 
ˆ
:()
N
F×⋅⋅⋅×  →VVTV                                                  (40.6)                                                  
 
by 
 
 
()()()
11
,...,
NN
F=∧⋅⋅⋅∧vvAvAv                                          (40.7)                                          
 

272                                                      Chap.                                                      8                                                      •                                                      EXTERIOR ALGEBRA 
where the skew symmetry and the multilinearity of F are obvious.  Then from (38.10) we can 
define a unique linear map 
f of the form (40.4) such that 
 
 Ff
=∧D                                                              (40.8)                                                              
 
which means precisely the condition (40.5). 
 
Since 
ˆ
()
N
TV is one-dimensional, the linear map f must have the representation 
 
 
()
11NN
fα∧⋅⋅⋅∧   =   ∧⋅⋅⋅∧vvvv                                            (40.9)                                            
 
where 
α
 is a scalar uniquely determined by f and hence A.  We claim that 
 
 
detα=A                                                            (40.10)                                                            
 
Thus the determinant of 
A
 can be defined free of any basis by the condition 
 
 
()()
11
(det    )
NN
∧⋅⋅⋅∧=∧⋅⋅⋅∧AvAvA  vv                                   (40.11)                                   
 
 
for all 
1
,...,
N
∈vvV
. 
 
To prove (40.10), or, equivalently, (40.11), we choose reciprocal basis 
{}
i
e and 
{}
i
e for V 
and recall that the determinant of 
A is given by the determinant of the component matrix of A.  
Let 
A be represented by the component forms 
 
 
,
jiij
iijj
AA==AeeAee
                                         (40.12)                                         
 
where (40.12)
2
 is meaningful because 
V
 is an inner product space.  Then, by definition, we have 
 
                                                          detdetdet
ji
ij
AA
⎡⎤⎡⎤
==
⎣⎦⎣⎦
A                                             (40.13)                                             
 
Substituting (40.12)
2
 into (40.5), we get 
 
 
()
1
1
11
N
N
j
j
NN
jj
fAA∧⋅⋅⋅∧   =   ⋅⋅⋅∧⋅⋅⋅∧eee e                                  (40.14)                                  
 
The skew symmetry of the wedge product implies that 
 
 
1
1
,,,
1
NN
jjj
j
N
ε∧⋅⋅⋅∧   =∧⋅⋅⋅∧ee  ee
                                          (40.15)                                          
 
where the 
ε symbol is defined by (20.3).  Combining (40.14) and (40.15), and comparing the 
resu1t with (40.9), we see that 

Sec. 40               •               Determinant and Orientation 273 
 
 
1
1
,,,
111
N
N
jj
NNN
jj
AAαε∧⋅⋅⋅∧  =   ⋅⋅⋅∧⋅⋅⋅∧eeee                                 (40.16)                                 
 
or equivalently 
 
 
1
1
,,,
1
N
N
jj
N
jj
AAαε=⋅⋅⋅
                                                   (40.17)                                                   
 
since 
1N
∧⋅⋅⋅∧ee is a basis for 
ˆ
()
N
TV.  But by definition the right-hand side of (40.17) is equal to 
the determinant of the matrix 
i
j
A
⎡⎤
⎣⎦
 and thus (40.10) is proved. 
 
As an illustration of (40.11), we see that 
 
 
det1=I                                                             (40.18)                                                             
 
 
since we have 
 
 
()   ( )
111
(det  )
NNN
∧⋅⋅⋅∧=  ∧⋅⋅⋅∧   =∧⋅⋅⋅∧IvIvvvI  vv 
 
More generally, we have 
 
                                                                      det()
N
αα=I                                                         (40.19)                                                         
 
Since 
 
 
() ( )()()
111NNNN
ααααα∧⋅⋅⋅∧=∧⋅⋅⋅∧=∧⋅⋅⋅∧IvIvvvvv 
 
 
It is now also obvious that 
 
                                                            det()(det    )(det   )
=ABAB                                                (40.20)                                                
 
since we have 
 
 
()()()()
11
1
(det    )
(det    )(det   )
NN
N
∧⋅⋅⋅∧=∧⋅⋅⋅∧
=∧⋅⋅⋅∧
ABvABvA    BvBv
ABvv
 
 
for all 
1
,...,
N
∈vvV.  Combining (40.19) and (40.20), we recover also the formula 
 
                                                               det()(det    )
N
αα=AA                                                   (40.21)                                                   
 

274                                                      Chap.                                                      8                                                      •                                                      EXTERIOR ALGEBRA 
Another application of (40.11) yields the result that 
A
 is non-singular if and only if detA 
is non-zero.  To see this result, notice first the simple fact that 
A
 is non-singular if and only if 
A
 
transforms any basis 
{}
j
e of V into a basis 
{}
j
Ae of V.  From the corollary of Theorem 39.3, 
{}
j
Ae is a basis of V if and only if 
()()
1N
∧⋅⋅⋅∧≠AeAe0.  Then from (40.11), 
()  ( )
1N
∧⋅⋅⋅∧≠AeAe0 if and only if det0≠A.  Thus det0≠A is necessary and sufficient for A 
to be non-singular. 
 
The determinant, of course), is just one of the invariants of 
A
.  In Chapter 6, Section 26, 
we have introduced the set of fundamental invariants 
{}
1
,...,
N
μμ for A by the equation 
 
 
()
1
11
det
NN
NN
tt  ttμμμ
−
−
+=+  +⋅⋅⋅+  +AI                                   (40.22)                                   
 
Since we have shown that the determinant of any endomorphism can be characterized by the 
condition (40.11), if we apply (40.11) to the endomorphism 
t+AI, the result is 
 
 
()( )()
1111
11
NN   N  NN
NN
tttttμμμ
−
−
+   ∧⋅⋅⋅∧+    =   ++⋅⋅⋅++∧⋅⋅⋅∧AvvAvvvv 
 
Comparing the coefficient of 
Nk
t
−
 on the two sides of this equation, we obtain 
 
 
12
1
11
K
K
ii i
NN
K
ii
μ
<⋅⋅⋅<
∧⋅⋅⋅∧   =∧⋅⋅⋅∧   ∧⋅⋅⋅∧    ∧⋅⋅⋅∧    ∧⋅⋅⋅∧
∑
vv  vAv Av Av v
             (40.23)             
 
where there are precisely 
N
K
⎛⎞
⎜⎟
⎝⎠
 terms in the summation on the right hand side of (40.23), each 
containing K factors of 
A acting on the covectors 
1
,...,
K
ii
vv.  In particular, taking 
KN=
, we 
recover the result 
 
 
()()
11NN
N
μ∧⋅⋅⋅∧   =∧⋅⋅⋅∧vvAvAv 
 
which is the same as (40.11) since 
 
                                                                                det
N
μ=A 
 
Likewise, taking 
1K=, we obtain 
 
 
11
1
1
N
NiN
i
μ
=
∧⋅⋅⋅∧   =∧⋅⋅⋅∧   ∧⋅⋅⋅∧
∑
vvvAvv                                  (40.24)                                  
 
which characterizes the trace of 
A free of any basis, since 
 

Sec. 40               •               Determinant and Orientation 275 
 
1
trμ=A 
 
Of course, we can prove the existence arid the uniqueness of 
K
μ satisfying the condition (40.23) 
by the universal factorization property as before. 
 
Since the space 
ˆ
()
N
TV is one-dimensional, it is divided by 0 into two nonzero segments.  
Two nonzero densities 
1
D and 
2
D are in the same segment if they differ by a positive scalar factor, 
say 
21
λ=DD, where 0λ>.  Conversely, if 
1
D and 
2
D differ by a negative scalar factor, then they 
belong to different segments.  If 
{}
i
e and 
{}
ˆ
i
e are two basis for V, we define their corresponding 
product basis 
1N
∧⋅⋅⋅∧ee and 
1
ˆˆ
N
∧⋅⋅⋅∧ee for 
ˆ
()
N
TV as usual;  then we say that 
{}
i
e and 
{}
ˆ
i
e 
have the same orientation and that the change of basis is a proper transformation  if 
1N
∧⋅⋅⋅∧ee
 
and 
1
ˆˆ
N
∧⋅⋅⋅∧ee belong to the same segment of 
ˆ
()
N
TV.  Conversely, opposite orientation and 
improper transformation are defined by the condition that 
1N
∧⋅⋅⋅∧ee
 and 
1
ˆˆ
N
∧⋅⋅⋅∧ee
 belong to 
different segments of 
ˆ
()
N
TV.  From (39.26) for the case rN=, the product basis 
1N
∧⋅⋅⋅∧ee and 
1
ˆˆ
N
∧⋅⋅⋅∧ee
 are related by 
 
 
11
ˆˆ
det
NiN
j
T
⎡⎤
∧⋅⋅⋅∧  =∧⋅⋅⋅∧
⎣⎦
eeee
                                        (40.25)                                        
 
Consequently, the change of basis is proper or improper if and only if  det
i
j
T
⎡⎤
⎣⎦
 is positive or 
negative, respectively. 
 
It is conventional to designate one segment of 
ˆ
()
N
TV positive and the other one negative.  
Whenever such a designation has been made, we say that 
ˆ
()
N
TV and, thus, 
V
, are oriented.  For 
an oriented space 
V a basis 
{}
i
e is positively oriented or right-handed if its product basis 
1N
∧⋅⋅⋅∧ee
 belongs to the positive segment of 
ˆ
()
N
TV;  otherwise, the basis is negatively oriented 
or left-handed.  It is customary to restrict the choice of basis for an oriented space to positively 
oriented bases only.  Under such a restriction, the parity 
ε [cf, (33.35)] of all relative tensors 
always has the value    1
+, since the transformation is restricted to be proper.  Hence, in this case it is 
.not necessary to distinguish relative tensors into axial ones and polar ones. 
 
As remarked in Exercise 39.2, it is conventional to use the inner product 
∗
 defined by 
(39.29) for skew-symmetric tensors.  For the space of densities 
ˆ
()
N
TV the inner product is given 
by [cf. (39.31)] 
 
 
12...12...
det
ij
NN
eA B
⎡⎤
∗=
⎣⎦
AB
                                              (40.26)                                              
 

276                                                      Chap.                                                      8                                                      •                                                      EXTERIOR ALGEBRA 
where 
12...N
A and 
12...N
B are the strict component of A and B as defined by (40.2).  Clearly, there 
exists an unique unit density with respect to 
∗ in each segment of 
ˆ
()
N
TV.  If 
ˆ
()
N
TV is oriented, 
the unit density in the positive segment is usually denoted by 
E, then the unit density in the 
negative segment is 
−E.  In general, if D is any density in 
ˆ
()
N
TV, then it has the representation 
 
 
d=DE                                                              (40.27)                                                              
 
In accordance with the usual practice, the scalar 
d in (40.27), which is the component of D 
relative to the positive unit density 
E, is also called a density or more specifically a density scalar, 
as opposed to the term density tensor for 
D.  This convention is consistent with the common 
practice of identifying a scalar 
α with an element 1α in R, where R is, of course, an oriented 
one-dimensional space with positive unit element 
1
. 
 
If 
{}
i
e is a basis in an oriented space V, then we define the density e
∗
 of 
{}
i
e to be the 
component of the product basis 
1N
∧⋅⋅⋅∧ee relative to E, namely 
 
 
1N
e
∗
∧⋅⋅⋅∧  =eeE                                                      (40.28)                                                      
 
In other words, e
∗
 is the density of the product basis of 
{}
i
e.  Clearly, 
{}
i
e is positively oriented or 
negatively oriented depending on whether its density e
∗
 is positive or negative, respectively.  We 
say that a basis 
{}
i
e is unimodular if its density has unit absolute value. i.e., 1e
∗
=.  All 
orthonormal bases, right-handed or left-handed, are always unimodular.  A unimodular basis in 
general need not be orthonormal, however. 
 
From (40.28) and (40.26), we have 
 
 
2
det
ij
eeee
∗∗∗
⎡⎤
=∗=
⎣⎦
EE
                                                (40.29)                                                
 
for any basis 
{}
i
e.  Hence the absolute density e
∗
 of 
{}
i
e is given by 
 
 
()
1/ 2
det
ij
ee
∗
⎡⎤
=
⎣⎦
                                                      (40.30)                                                      
 
Substituting (40.30) into (40.28), we have 
 
 
()
1/ 2
1
det
Nij
eε
⎡⎤
∧⋅⋅⋅∧  =
⎣⎦
eeE                                            (40.31)                                            
 
where 
ε is 1+ if 
{}
i
e is positively oriented and it is 1− if 
{}
i
e is negatively oriented. 
 
From (40.25) and (40.27) the density of a basis transforms according to the rule 

Sec. 40               •               Determinant and Orientation 277 
 
 
ˆ
det
i
j
eTe
∗∗
⎡⎤
=
⎣⎦
                                                       (40.32)                                                       
 
under a change of basis from 
{}
j
e to 
{}
ˆ
j
e.  Comparing (40.32) with (33.35), we see that the 
density of a basis is an axial relative scalar of weight    1
−. 
 
An interesting property of a unit density (E or 
−E) is given by the following theorem. 
 
Theorem 40.1.  If U is a unit density in 
ˆ
()
N
TV, then 
 
 
()()
11
det
NNij
⎡⎤⎡⎤
⎡⎤
∗   ∧⋅⋅⋅∧∗   ∧⋅⋅⋅∧=⋅
⎣⎦
⎣⎦⎣⎦
Uvv   Uuuvu                        (40.33)                        
 
for all 
1
,...,
N
vv and 
1
,...,
N
uu in 
V
. 
 
Proof.  Clearly we can represent U as the product basis of an orthonormal basis, say 
{}
k
i, with 
 
 
1N
=∧⋅⋅⋅∧Uii                                                        (40.34)                                                        
 
 
From (40.34) and (39.30), we then have 
 
 
()
1
detdet
Nii
jj
v
⎡⎤⎡⎤
∗   ∧⋅⋅⋅∧   =⋅   =
⎣⎦⎣⎦
Uvviv 
 
and 
 
 
()
1
detdet
Nii
jj
u
⎡⎤⎡⎤
∗∧⋅⋅⋅∧ =  ⋅=
⎣⎦⎣⎦
Uuuiu 
 
where 
i
j
v and 
i
j
u are the jth components of 
i
v and 
i
u relative to 
{}
k
i, respectively.  Using the 
product rule and the transpose rule of the determinant, we then obtain 
 
 
()()
()
11
1
detdet
detdetdet
detdet
NNik
jl
TT
ikik
jljl
N
iii   j
kk
k
vu
vuvu
vu
=
⎡⎤⎡⎤
⎡⎤⎡⎤
∗   ∧⋅⋅⋅∧∗   ∧⋅⋅⋅∧=
⎣⎦⎣⎦
⎣⎦⎣⎦
⎡⎤ ⎡⎤    ⎡⎤⎡⎤
==
⎣⎦ ⎣⎦    ⎣⎦⎣⎦
⎡⎤
⎡⎤
==⋅
⎢⎥
⎣⎦
⎣⎦
∑
Uvv   Uuu
vu
 
 
which is the desired result. 
 
By exactly the same argument, we have also the following theorem. 
 

278                                                      Chap.                                                      8                                                      •                                                      EXTERIOR ALGEBRA 
Theorem 40.2.  If U is a unit density as before, then 
 
 
()()
11
,...,,...,det
NNij
⎡⎤
=⋅
⎣⎦
Uvv  Uuuv u                                    (40.35)                                    
 
for all 
1
,...,
N
vv and 
1
,...,
N
uu in 
V
. 
 
 
Exercises 
 
40.1
 If A is an endomorphism of 
V
 show that the determinant of A can be characterized by 
the following basis-free condition: 
 
 
()()()
11
,...,det,...,
NN
=DAvAvADvv
                                     (40.36)                                     
 
for all densities 
ˆ
()
N
∈DTV and all vectors 
1
,...,
N
vv in V. 
Note.   A complete proof of this result consists of the following two parts: 
(i) There exists a unique scalar 
α, depending on 
A
, such that 
 
 
()()
11
,...,,...,
NN
α=DAvAvDvv 
 
for all 
ˆ
()
N
∈DTV and all vectors 
1
,...,
N
vv in 
V
. 
(ii)       The       scalar       
α is equal to detA. 
40.2
 Use the formula (40.36) and show that the fundamental invariants 
{}
1
,...,
N
μμ of an 
endomorphism A can be characterized by 
 
 
()
()
1
1
11
,...,,...,,...,,...,
K
K
kNiiN
ii
μ
<⋅⋅⋅<
=
∑
DvvDvAvAvv                           (40.37)                           
 
for all 
1
,...,
N
∈vvV and all 
ˆ
()
N
∈DTV.  Here the summation on the right-hand side of 
(40.37) is similar to that of (40.23), i.e., there are 
N
K
⎛⎞
⎜⎟
⎝⎠
 terms in the summation, each 
containing the value of D at the argument with K vectors 
1
,...,
K
ii
vv
, acted on by 
A
.  In 
particular, taking 
KN=, we recover the formula (40.36), and taking 1K=, we obtain 
 
 
()()
11
1
(tr    ),...,,...,,...,
N
NiN
i=
=
∑
AD vvD vAvv                                 (40.38)                                 
 
40.3     The     
Gramian is a function (not multilinear) G of K vectors defined by 
 
 
()
1
,...,det
Nij
G
⎡⎤
=⋅
⎣⎦
vvvv                                              (40.39)                                              

Sec. 40               •               Determinant and Orientation 279 
 
where the matrix 
ij
⎡⎤
⋅
⎣⎦
vv is 
KK×, of course,  Use the result of Theorem 40.2 and show 
that 
 
 
()
1
,...,0
N
G≥vv                                                       (40.40)                                                       
 
and that the equality holds if and only if 
{}
1
,...,
K
vv is a linear dependent set.  Note.  When 
2
K=, the result (40.40) reduces to the Schwarz inequality. 
40.4
 Use the results of this section and prove that 
 
                                                                    detdet
T
=AA 
 
for an endomorphism (   ;    )∈A
LVV. 
40.5
 Show that  adjA, which is used in (26.15), can be defined by 
 
 
1212
(adj   )
NN
∧  ∧⋅⋅⋅∧   =  ∧    ∧⋅⋅⋅∧Av    vvv    AvAv 
 
40.6
 Use (40.23) and the result in Exercise 40.5 and show that 
 
 
1
tr adj
N
μ
−
=A 
 
40.7
 Show that 
 
 
()
()()
1
2
adjadj   adj
det adjdet
adj  adjdet
N
N
−
−
=
=
=
ABBA
AA
AAA
 
 
and 
 
 
()()
2
(1)
det adj  adjdet
N−
=AA 
 
for     ,(   ;    )∈AB
LVV and dimN=V. 

280                                                      Chap.                                                      8                                                      •                                                      EXTERIOR ALGEBRA 
 
 
Section 41.   Duality 
 
As a consequence of (39.9), the dimension of the space 
ˆ
()
r
TV is equal to that of 
ˆ
()
Nr−
TV.  
Hence the spaces 
ˆ
()
r
TV and 
ˆ
()
Nr−
TV are isomorphic.  The purpose of this section is to establish a 
particular isomorphism, 
 
 
ˆˆ
:()()
rrNr−
→TVTVD                                                    (41.1)                                                    
 
called the 
duality operator, for an oriented space V.  As we shall see, this duality operator gives 
rise to a definition to the operation of 
cross product or vector product when 3N= and 
2r=
. 
 
We recall first that 
ˆ
()
r
TV is equipped with the inner product ∗ given by (39.29).  Let E be 
the distinguished positive unit density in 
ˆ
()
N
TV as introduced in the preceding section.  Then for 
any 
ˆ
()
r
∈ATV we define its dual 
r
AD in 
ˆ
()
Nr−
TV by the condition 
 
 
()()
r
∗∧=   ∗EAZAZD                                                   (41.2)                                                   
 
for all 
ˆ
()
Nr−
∈ZTV.  Since the left-hand side of (41.2) is linear in Z, and since ∗ is an inner 
product, 
r
AD is uniquely determined by (41.2).  Further, from (41.2), 
r
AD depends linearly on 
A
, so 
r
D, is a linear transformation from 
ˆ
()
r
TV to 
ˆ
()
Nr−
TV. 
 
Theorem 41.1.  The duality operator 
r
D defined by the condition (41.2) is an isomorphism. 
 
Proof.  Since  
ˆ
()
r
TV and 
ˆ
()
Nr−
TV are isomorphic, it suffices to show that 
r
D is an isometry, i.e., 
 
 
()()
rr
∗=∗AAAADD                                                   (41.3)                                                   
 
for all 
ˆ
()
r
∈ATV.  The polar identity 
 
 
()()
{}
1
2
∗=  ∗+∗− − ∗ −ABAABB  AB  AB
                                  (41.4)                                  
 
then implies that 
r
D preserves the inner product also; i.e., 
 
 
()()
rr
∗=∗ABABDD
                                                    (41.5)                                                    
 

Sec. 41               •               Duality                                          281 
for all A and B in 
ˆ
()
r
TV.  To prove (41.3), we choose an arbitrary right-handed orthogonal basis 
{}
j
i of V.  Then E is simply the product basis of 
{}
j
i 
 
 
1N
=∧⋅⋅⋅∧Eii                                                          (41.6)                                                          
 
Of course, we represent     ,AZ, and 
r
AD in strict component form relative to 
{}
j
i also;  then (41.2) 
can be written as 
 
 
()
11111
1
11
1
...............
...
rNrrNrNr
Nr
rNr
Nr
iijjii  jjrjj
jj
iij j
jj
AZZε
−−−
−
−
−
<⋅⋅⋅<<⋅⋅⋅<
<⋅⋅⋅<
=
∑∑
AD                        (41.7)                        
 
Since Z is arbitrary, (41.7) implies 
 
 
()
111
1
1
.........
...
rNr    r
Nr
r
riijjii
jj
ii
Aε
−
−
<⋅⋅⋅<
=
∑
AD
                                           (41.8)                                           
 
This formula gives the strict components of 
r
AD in terms of those of A relative to a right-handed 
orthonormal basis. 
 
From (41.8) we can compute the value of left-hand side of (41.3) by 
 
 
()()
111 111
1
1
1
11
1
..................
......
rNrrNr    rr
r
r
Nr
rr
r
rriijjkkjjii kk
ii
kk
jj
ii ii
ii
AA
AA
εε
−−
−
<⋅⋅⋅<
<⋅⋅⋅<
<⋅⋅⋅<
<⋅⋅⋅<
∗=
==∗
∑
∑
AA
AA
DD
 
 
which is the desired result. 
 
Having proved that 
r
D is an isomorphism from 
ˆ
()
r
TV to 
ˆ
()
Nr−
TV relative to the inner 
product 
∗, we can now use the condition (41.2) and (41.5) to compute the inverse of 
r
D.  Indeed, 
from the skew symmetry of the wedge product [cf. (38.25)] we have 
 
 
()
(1)
rN r−
∧=−∧AZZA                                                   (41.9)                                                   
 
Hence (41.2) yields 
 
 
()()
()
(1)
rN r
rNr
−
−
∗=−∗AZAZDD
                                         (41.10)                                         
 
for all 
ˆ
()
r
∈ATV and 
ˆ
()
Nr−
∈ZTV.  On the other hand. (41.5) yields 
 

282                                                      Chap.                                                      8                                                      •                                                      EXTERIOR ALGEBRA 
 
()()
()
1
rr
−
∗=∗AZAZDD                                               (41.11)                                               
 
when we chose 
r
=BZD.  Comparing (41.10) with (41.11), we obtain 
 
 
1()
(1)
rN r
rNr
−−
−
=−DD
                                                   (41.12)                                                   
 
which is the desired result. 
 
We notice that the equations (41.8) and (39.21) are very similar to each other, the only 
difference being that (39.21) applies to all bases while (41.8) is restricted to right-handed 
orthonormal bases only.  Since the quantities given by (39.21) transform according to the rule of an 
axial tensor density, while the dual 
r
AD is a tensor in 
ˆ
()
Nr−
TV, the formula (41.8) is no longer 
valid if the basis is not right-handed and orthogonal,  If 
{}
i
e is an arbitrary basis, then E is given 
by (40.31).  In this case if we represent     ,AZ, and 
r
AD again by their strict components relative to 
{}
i
e, then from (39.21) and (40.28) the condition (41.2) can be written as 
 
 
()
1
11
11
111
1
11
1
1
1
......
.........
...
det
Nr
rNr
rNrNr
Nr
rNr
Nr
Nr
NrNrNr
kj
kj
iij j
ii   j jrjj
kk
iikk
jj
jj
kjkj
ee
AZeZ
ee
ε
−
−
−−
−
−
−
−
−−−
∗
<⋅⋅⋅<<<
<⋅⋅⋅<
<⋅⋅⋅<
⎡⎤
⋅⋅⋅
⎢⎥
⋅⋅
⎢⎥
⎢⎥
=
⋅⋅
⎢⎥
⋅⋅
⎢⎥
⎢⎥
⋅⋅⋅
⎣⎦
∑∑
AD
(41.13) 
 
Since Z is arbitrary, (41.13) implies 
 
 
()
1
11
11
1
1
11
1
......
...
...
det
Nr
rNr
r
Nr
rNr
NrNrNr
kj
kj
iij j
iir
kk
iikk
kjkj
ee
Ae
ee
ε
−
−
−
−
−−−
∗
<⋅⋅⋅<<<
⎡⎤
⋅⋅⋅
⎢⎥
⋅⋅
⎢⎥
⎢⎥
=
⋅⋅
⎢⎥
⋅⋅
⎢⎥
⎢⎥
⋅⋅⋅
⎣⎦
∑∑
AD
          (41.14)          
 
We can solve (41.14) for the components of 
r
AD in the following way.  We multiply (41.14) by 
 
 
111
1
det
Nr
NrNrNr
jljl
jljl
ee
ee
−
−−−
⋅⋅⋅
⎡⎤
⎢⎥
⋅⋅
⎢⎥
⎢⎥
⋅⋅
⎢⎥
⋅⋅
⎢⎥
⎢⎥
⋅⋅⋅
⎣⎦
 
 

Sec. 41               •               Duality                                          283 
and sum on 
1
,...,
Nr
jj
−
 in increasing order, then we obtain 
 
 
()
111
11
1
1
1
1
1
......
...
...
det
Nr
rNr
r
Nr
r
Nr
NrNrNr
jljl
iij j
rii
ll
ii
jj
jljl
ee
Ae
ee
ε
−
−
−
−
−−−
∗
<⋅⋅⋅<
<⋅⋅⋅<
⋅⋅⋅
⎡⎤
⎢⎥
⋅⋅
⎢⎥
⎢⎥
=
⋅⋅
⎢⎥
⋅⋅
⎢⎥
⎢⎥
⋅⋅⋅
⎣⎦
∑
AD              (41.15)              
 
which is the desired result.  In deriving (41.15) we have used the identity (21.26) for the matrix 
ij
e
⎡⎤
⎣⎦
 and its inverse 
ij
e
⎡⎤
⎣⎦
 in order to obtain the formula 
 
 
1
11
111
1
1
1
1
1
...
...
detdet
Nr
Nr
Nr
Nr
Nr
NrNrNr
NrNrNr
kj
kj
jljl
kk
ll
jj
kjkj
jljl
ee
ee
ee
ee
δ
−
−
−
−
−
−−−
−−−
<⋅⋅⋅<
⋅⋅⋅
⎡⎤
⎡⎤
⋅⋅⋅
⎢⎥
⎢⎥
⋅⋅
⋅⋅
⎢⎥
⎢⎥
⎢⎥
⎢⎥
=
⋅⋅
⋅⋅
⎢⎥
⎢⎥
⋅⋅
⋅⋅
⎢⎥
⎢⎥
⎢⎥
⎢⎥
⋅⋅⋅
⋅⋅⋅
⎣⎦
⎣⎦
∑
      (41.16)      
 
Equation (41.15) follows, since we have 
 
 
()()
1
1
11
1
...
...
......
Nr
Nr
NrNr
Nr
kk
llrr
kkll
kk
δ
−
−
−−
−
<⋅⋅⋅<
=
∑
AADD
                                    (41.17)                                    
 
Notice that (41.8) is a special case of (41.15) when the basis is right-handed and orthonormal, since 
in this case 
1,
ijij
eeδ
∗
==, and, from (21.5), 
 
 
111
1
1
1
...
...
det
Nr
r
r
NrNrNr
jljl
jj
ll
jljl
δδ
δ
δδ
−
−−−
⋅⋅⋅
⎡⎤
⎢⎥
⋅⋅
⎢⎥
⎢⎥
=
⋅⋅
⎢⎥
⋅⋅
⎢⎥
⎢⎥
⋅⋅⋅
⎣⎦
 
 
Then as in (41.17) we have 
 
 
1111
1
1
1
........
...
...
rNrrNr
r
r
Nr
iij jiill
jj
ll
jj
εδε
−−
−
<⋅⋅⋅<
=
∑
 
 
And thus (41.15) reduces to (41.8). 
 

284                                                      Chap.                                                      8                                                      •                                                      EXTERIOR ALGEBRA 
The duality operator can be used to define the 
cross product or the vector product for an 
oriented three-dimensional space.  If we take 
3N=, then 
2
D is an isomorphism from 
2
ˆ
()
TV to 
1
ˆ
()
TV 
 
 
221
ˆˆ
:()()→=
TVTVVD                                                (41.18)                                                
 
so for any 
u and ∈vV, 
()
2
∧uvD is a vector in V.  We put 
 
 
()
2
×≡∧uvu vD
                                                      (41.19)                                                      
 
called the 
cross product of u with v. 
 
We now prove that this definition is consistent with the classical definition of a cross 
product.  This fact is more or less obvious.  From (41.2), we have 
 
 
()()
for all ∗∧∧ =⋅×∈Euvw uvwwV
                           (41.20)                           
 
where we have replaced the 
∗ inner product on the right-hand side by the ⋅ inner product since 
×uv and w belong to 
V
.  Equation (41.20) shows that 
 
 
()()
0×⋅= × ⋅=uvu  uv v
                                              (41.21)
1
 
 
so that 
×uv is orthogonal to v.  That equation shows also that ×=uv 0 if and only if    ,uv are 
linearly dependent.  Further, if    ,uv are linearly independent, and if 
n
 is the unit normal of u and 
v such that 
{}
,,uvn
 from a right-handed basis, then 
 
 
()
0×⋅>uvn                                                        (41,21)
2
 
 
which means that 
×uv is pointing in the same direction as 
n
.  Finally, from (40.33) we obtain 
 
 
()()()()
()
()
222
2222
22
0
det0
001
1cossin
θθ
∗∧∧   ∗∧∧ =×⋅×
⎡⎤⎡⎤
⎣⎦⎣⎦
⋅⋅
⎡⎤
⎢⎥
=⋅⋅=  −⋅
⎢⎥
⎢⎥
⎣⎦
=−=
Euvw Euvwuv uv
uu  uv
vu  vvu  vuv
uvuv
                          (41.22)                          
 
where 
θ is the angle between u and v.  Hence we have shown that ×uv is in the direction of n 
and has the magnitude 
sinθuv
, so that the definition of ×uv, based on (41.20), is consistent 
with the classical definition of the cross product. 

Sec. 41               •               Duality                                          285 
 
From (41.19), the usual properties of the cross product are obvious; e.g., 
×uv is bilinear 
and skew-symmetric in 
u and v.  From (41.8), relative to a right-handed orthogonal basis 
{}
123
,,ii i
 the components of ×uv are given by 
 
 
()
()
ijkijj   iijk    ij
k
ij
uvu vuvεε
<
×=− =
∑
uv                                      (41.23)                                      
 
where the summations on   ,
ij in the last term are unrestricted.  Thus we have 
 
 
()
()
()
2332
1
3113
2
1221
3
uv    uv
uv    uv
uvu v
×= −
×= −
×= −
uv
uv
uv
                                                   (41.24)                                                   
 
On the other hand, if we use an arbitrary basis 
{}
i
e, then from (41.15) we have 
 
 
()
()
ijkijk
ijj   ikjklij
l
ij
uvu v  eee euvεε
∗∗
<
×=   −=
∑
uv
                               (41.25)                               
 
where 
e
∗
 is given by (40.31), namely 
 
 
()
1/ 2
det
ij
eeε
∗
⎡⎤
=
⎣⎦
 
 
 
Exercises 
 
41.1     If     
ˆ
()
N
∈DTV
, what is the value of 
N
DD? 
41.2     If     
{}
i
e
 is a basis of V, determine the strict components of the dual 
()
1r
ii
r
∧⋅⋅⋅∧eeD
. 
Hint.    The strict components of 
1r
ii
∧⋅⋅⋅∧ee
 are 
{}
1
1
...
...1
,
r
r
ii
jjr
jjδ<⋅⋅⋅< since as in (41.17) we 
have 
 
 
111
1
1
...
...
rrr
r
r
ii  jjii
jj
jj
δ
<⋅⋅⋅<
∧⋅⋅⋅∧   =  ∧⋅⋅⋅∧
∑
eeee
                                      (41.26)                                      
 
41.3     If     
A is an endomorphism of a three-dimensional oriented inner product space V, show 
that 
 
 
()()()
det⋅× =    ⋅×Au    Av    AwA  u    v    w                                       (41.27)                                       
 
and if 
A is invertible, show that 

286                                                      Chap.                                                      8                                                      •                                                      EXTERIOR ALGEBRA 
 
 
()
()
1
det
T
−
×=×Av    AwA    Av    w                                          (41.28)                                          

Sec. 42               •               Contravariant Representation 287 
 
Section 42.   Transformation to the Contravariant Representation 
 
So far we have used the covariant representations of skew-symmetric tensors only.  We 
could, of course, develop the results of exterior algebra using the contravariant representations or 
even the mixcd representations of skew-symmetric tensors, since we have a fixed rule of 
transformation among the various representations based on the inner product, as explained in 
Section 35.  In this section, we shall demonstrate the transformation from the covariant 
representation to the contravariant representation. 
 
Recall that in general if 
A is a tensor of order r, then relative to any reciprocal bases 
{}
i
e
 
and 
{}
i
e the contravariant components 
1
...
r
ii
A of A are related to the covariant components 
1
...
r
ii
A of 
A by [cf. (35.21)] 
 
 
112
1
...
...
rrr
r
iiijij
jj
AeeA=⋅⋅⋅
                                                    (42.1)                                                    
 
This transforn1ation rule is valid for all 
rth order tensors, including skew-symmetric ones. 
However, as we have explained in Section 39, for skew-symmetric tensors it is convenient to use 
the strict components.  Their transformation rule no longer has the simple form (42.1), since the 
summations on the repeated indices 
1r
jj⋅⋅⋅ on the right-hand side of (42.1) are unrestricted.  We 
shall now derive the transformation rule between the contravariant and the covariant strict 
components of a skew-symmetric tensor. 
 
Recall that the strict components of a skew-symmetric tensor are simply the ordinary 
components restricted to an increasing set of indices, as shown in (39.10).  In order to obtain an 
equivalent form of (42.1) using the strict components of 
A only, we must replace the right-hand 
side of that equation by a restricted summation.  For this purpose we use the identity [cf. (37.26)] 
 
 
1
1111
...
...[    ...    ]......
1
!
r
rrrr
kk
jjjjjj  kk
AAA
r
δ==
                                              (42.2)                                              
 
where, by assumption, 
A is skew-symmetric.  Substituting (42.2) into (42.1), we obtain 
 
 
1111
11
.....
......
1
!
rrrr
rr
iik k  ijij
jjkk
AeeA
r
δ=⋅⋅⋅                                              (42.3)                                              
 

288                                                      Chap.                                                      8                                                      •                                                      EXTERIOR ALGEBRA 
Now, since the coefficient of 
1
...
r
kk
A is skew-symmetric, we can restrict the summations on 
1r
kk⋅⋅⋅ 
to the increasing order by removing the factor 
1
!
r
, that is 
 
 
1111
11
1
.....
......
rrrr
rr
r
iik k  ijij
jjkk
kk
AeeAδ
<⋅⋅⋅<
=⋅⋅⋅
∑
                                           (42.4)                                           
 
This is the desired transformation rule from the covariant strict components to the contravariant 
ones.  Clearly, the inverse of (42.4) is 
 
 
11
1111
1
......
.....
rr
rrrr
r
jjkk
iik k  ijij
kk
AeeAδ
<⋅⋅⋅<
=⋅⋅⋅
∑
                                            (42.5)                                            
 
which is the transformation rule from the contravariant strict components to the covariant ones. 
 
From (21.21) we see that (42.4) is equivalent to 
 
 
111
1
1
111
1
1
1
....   ,   ...
...
...
det
rrr
r
r
r
r
r
rrr
iiiik k
kk
kk
ikik
kk
kk
ikik
AeA
ee
A
ee
<⋅⋅⋅<
<⋅⋅⋅<
=
⎡⎤
⋅⋅⋅
⎢⎥
⋅⋅
⎢⎥
⎢⎥
=
⋅⋅
⎢⎥
⋅⋅
⎢⎥
⎢⎥
⋅⋅⋅
⎣⎦
∑
∑
                                   (42.6)                                   
 
while (42.5) is equivalent to 
 
 
1
111
1
111
1
1
1
...
....   ,   ...
...
det
r
rrr
r
r
r
r
rrr
kk
iiiik k
kk
ikik
kk
kk
ikik
AeA
ee
A
ee
<⋅⋅⋅<
<⋅⋅⋅<
=
⋅⋅⋅
⎡⎤
⎢⎥
⋅⋅
⎢⎥
⋅⋅
⎢⎥
=
⎢⎥
⋅⋅
⎢⎥
⎢⎥
⋅⋅⋅
⎣⎦
∑
∑
                                   (42.7)                                   
 
In particular, when 
rN=, we have 

Sec. 42               •               Contravariant Representation 289 
 
 
1...1...
1...1...
det,det
NijN
NNij
AeAAeA
⎡⎤
⎡⎤
==
⎣⎦
⎣⎦
                             (42.8)                             
 
From the transformation rules (42.6) and (42.7), or directly from the skew-symmetry of the 
wedge product, we see that the product basis of any reciprocal bases 
{}
i
e and 
{}
i
e obeys the 
following transformation rules: 
 
 
111
1
1
1
1
det
r
r
r
r
rrr
ikik
ii
kk
kk
ikik
ee
ee
<⋅⋅⋅<
⎡⎤
⋅⋅⋅
⎢⎥
⋅⋅
⎢⎥
⎢⎥
∧⋅⋅⋅∧  =∧⋅⋅⋅∧
⋅⋅
⎢⎥
⋅⋅
⎢⎥
⎢⎥
⋅⋅⋅
⎣⎦
∑
eee e
                        (42.9)                        
 
and 
 
 
111
1
1
1
1
det
r
r
r
r
rrr
ikik
kk
ii
kk
ikik
ee
ee
<⋅⋅⋅<
⋅⋅⋅
⎡⎤
⎢⎥
⋅⋅
⎢⎥
⋅⋅
⎢⎥
∧⋅⋅⋅∧  =∧⋅⋅⋅∧
⎢⎥
⋅⋅
⎢⎥
⎢⎥
⋅⋅⋅
⎣⎦
∑
eee e
                      (42.10)                      
 
In deriving (42.9) and (42.10) we have used the fact that the strict covariant components of 
1r
ii
∧⋅⋅⋅∧ee are 
 
 
{}
1
1
...
...1
,
r
r
ii
jjr
jjδ<⋅⋅⋅< 
 
and, likewise, the strict contravariant components of 
1r
ii
∧⋅⋅⋅∧ee are 
 
 
{}
1
1
...
...1
,
r
r
jj
iir
jjδ<⋅⋅⋅< 
 
as shown by (41.26). If we apply (42.9) and (42.10) to the product bases 
1N
∧⋅⋅⋅∧ee
 and 
1N
∧⋅⋅⋅∧ee, we get 
 

290                                                      Chap.                                                      8                                                      •                                                      EXTERIOR ALGEBRA 
 
1
1
1
1
det
det
Nij
N
N
Nij
e
e
⎡⎤
∧⋅⋅⋅∧  =∧⋅⋅⋅∧
⎣⎦
⎡⎤
∧⋅⋅⋅∧  =∧⋅⋅⋅∧
⎣⎦
eeee
eeee
                                         (42.11)                                         
 
The product bases 
 
 
{}
{}
1
1
11
,and,
r
r
ii
riir
iiii∧⋅⋅⋅∧    <⋅⋅⋅<∧⋅⋅⋅∧    <⋅⋅⋅<eeee 
 
are reciprocal bases with respect to the inner product 
∗, since from (39.30) we have 
 
 
()
()
11
1
11
11
1
...
...
det
r
rr
rr
i
r
r
r
ii
jj
iiii
jjjj
i
jj
δδ
δ
δδ
⎡⎤
⋅⋅⋅
⎢⎥
⋅⋅
⎢⎥
⎢⎥
⋅⋅
∧⋅⋅⋅∧   ∗   ∧⋅⋅⋅∧   ==
⎢⎥
⋅⋅
⎢⎥
⎢⎥
⋅⋅⋅
⎣⎦
eee e
                 (42.12)                 
 
In particular, when 
rN=
 we have 
 
 
()
()
1
1
1
N
N
∧⋅⋅⋅∧   ∗  ∧⋅⋅⋅∧   =eeee                                          (42.13)                                          
 
From (42.12), we can compute the 
∗
 inner product of any two rth order skew-symmetric tensors 
 
 
11
11
11
......
......
rr
rr
rr
kkkk
kkkk
kkkk
ABAB
<⋅⋅⋅<<⋅⋅⋅<
∗==
∑∑
AB
                                 (42.14)                                 
 
These formulas are equivalent to the formula (39.31), which is based on the covariant strict 
components of 
A and B. 
 
For an oriented space we have defined the density 
e
∗
 of a basis 
{}
i
e to be the components 
of 
1N
∧⋅⋅⋅∧ee relative to the positive unit density E, namely 
 
 
1N
e
∗
∧⋅⋅⋅∧  =eeE
                                                      (42.15)                                                      
 
as shown by (40.28).  Clearly we can define a similar component for the basis 
{}
i
e
, namely 

Sec. 42               •               Contravariant Representation 291 
 
 
1N
e∧⋅⋅⋅∧  =eeE                                                      (42.16)                                                      
 
Further, from (42.13) the components 
e and e
∗
 are related by 
 
 
1ee
∗
=
                                                              (42.17)                                                              
 
In view of this relation, we call 
e the volume of 
{}
i
e
.  Then 
{}
i
e
 is positively oriented or right-
handed 
if its volume e is positive;  otherwise, 
{}
i
e is negatively oriented or left-handed.  As 
before, a unimodular basis 
{}
i
e is defined by the condition that the absolute volume e is equal to 
unity. 
 
We can compute the absolute volume by 
 
 
2
det
ij
ee
⎡⎤
=
⎣⎦
                                                          (42.18)                                                          
 
or equivalently 
 
 
()
1/ 2
det
ij
ee
⎡⎤
=
⎣⎦
                                                       (42.19)                                                       
 
The proof is the same as that of (40.29) and (40.30).  Substituting (42.19) into (42.17), we have 
also 
 
 
()
1/ 2
1
det
Nij
eε
⎡⎤
∧⋅⋅⋅∧  =
⎣⎦
eeE
                                            (42.20)                                            
 
where 
ε
 is + if 
{}
i
e is positively oriented and it is − if 
{}
i
e is negatively oriented. 
 
Using the contravariant components, we can simplify the formulas of the preceding section 
somewhat;  e.g., from (42.6) we can rewrite (41.15) as 
 
 
()
1
11
1
1
...
......
...
Nr
rNr
r
r
jj
iij j
rii
ii
Aeε
−
−
∗
<⋅⋅⋅<
=
∑
AD                                       (42.21)                                       
 

292                                                      Chap.                                                      8                                                      •                                                      EXTERIOR ALGEBRA 
which is equivalent to 
 
 
()
1
11
1
1
...
......
...
r
rNr
Nr
r
ii
riijj
jj
ii
Aeε
−
−
<⋅⋅⋅<
=
∑
AD
                                        (42.22)                                        
 
Similarly, (41.25) can be rewritten as 
 
 
()
k
ijk
ij
euvε
∗
×=uv                                                     (42.23)                                                     
 
which is equivalent to 
 
 
()
ij
ijk
k
euvε×=uv                                                     (42.24)                                                     
 
 
Exercises 
 
42.1     Prove the formula (42.22). 
42.2     Use the result of Exercise 41.2 or the transformation rules (42.21)and (42.22) and show that 
 
 
()
11
1
1
1
......
rNr
r
Nr
r
iij j
ii
rjj
jj
eε
−
−
∗
<⋅⋅⋅<
∧⋅⋅⋅∧   =∧⋅⋅⋅∧
∑
eee eD                            (42.25)                            
 
which is equivalent to 
 
 
()
1
111
1
......
Nr
rrNr
r
j
j
riiiijj
jj
eε
−
−
∗
<⋅⋅⋅<
∧⋅⋅⋅∧   =∧⋅⋅⋅∧
∑
eee eD                            (42.26)                            
 
42.3
 Show that has the representations 
 
 
1
1
11
...
...
1
NN
NN
iij
j
iijj
e
e
εε=⊗⋅⋅⋅⊗   =⊗⋅⋅⋅⊗Eee  ee
                               (42.27)                               
 
 
where 

Sec. 42               •               Contravariant Representation 293 
 
 
1
11
1
...
...
1
NNN
N
iiij
ij
jj
eee
e
εε=⋅⋅⋅                                               (42.28)                                               
 
and 
 
 
()
1/ 2
det
ij
eeε
⎡⎤
=
⎣⎦
                                                      (42.29)                                                      

 
 
 

 x 
_______________________________________________________________________________ 
INDEX 
 
 
The page numbers in this index refer to the versions of Volumes I and II of the online versions.  In addition to the 
information below, the search routine in Adobe Acrobat will be useful to the reader.   Pages 1-294 will be found in the 
online editions of Volume 1, pages 295-520 in Volume 2 
 
Abelian group, 24, 33, 36, 37, 41 
Absolute density, 276 
Absolute volume, 291 
Addition 
for linear transformations, 93 
for rings and fields, 33 
for subspaces, 55 
for tensors, 228 
for vectors, 41 
Additive group, 41 
Adjoint linear transformation, 105-117 
determinant of, 138 
matrix of, 138  
Adjoint matrix, 8, 154, 156,279 
Affine space, 297 
Algebra 
associative,100 
tensor, 203-245 
Algebraic equations, 9, 142-145 
Algebraic multiplicity, 152, 159, 160, 161, 
164, 189, 191, 192 
Algebraically closed field, 152, 188 
Angle between vectors, 65 
Anholonomic basis, 332 
Anholonomic components, 332-338 
Associated homogeneous system of 
equations, 147 
Associative algebra, 100 
Associative binary operation, 23, 24 
Associative law for scalar multiplication, 41 
Asymptotic coordinate system, 460 
Asymptotic direction, 460 
Atlas, 308 
Augmented matrix, 143 
Automorphism 
of groups, 29 
of linear transformations, 100, 109, 
136, 168,  
Axial tensor, 227 
Axial vector densities, 268 
 
Basis, 47 
Anholonomic, 332 
change of, 52, 76, 77, 118, 210, 225, 
266, 269, 275, 277, 287 
composite product, 336, 338 
cyclic, 164, 193, 196 
dual, 204, 205, 215 
helical, 337 
natural, 316 
orthogonal, 70 
orthonormal,70-75 
product, 221, 263 
reciprocal, 76 

INDEX                                                                                                                        xi                                                            
standard, 50, 116 
Beltrami field, 404  
Bessel’s inequality, 73 
Bijective function, 19 
Bilinear mapping, 204 
Binary operation, 23 
Binormal, 391 
 
Cancellation axiom, 35 
Canonical isomorphism, 213-217 
Canonical projection, 92, 96, 195, 197 
Cartan parallelism, 466, 469 
Cartan symbols, 472 
Cartesian product, 16, 43, 229 
Cayley-Hamilton theorem, 152, 176, 191 
Characteristic polynomial, 149, 151-157 
Characteristic roots, 148 
Characteristic subspace, 148, 161, 172, 189, 
191 
Characteristic vector, 148 
Christoffel symbols, 339, 343-345, 416 
Closed binary operation, 23 
Closed set, 299 
Closure, 302 
Codazzi, equations of, 451 
Cofactor, 8, 132-142, 267 
Column matrix, 3, 139 
Column rank, 138 
Commutation rules, 395 
Commutative binary operation, 23 
Commutative group, 24 
Commutative ring, 33 
Complement of sets, 14 
Complete orthonormal set, 70 
Completely skew-symmetric tensor, 248 
Completely symmetric tensor, 248  
Complex inner product space, 63 
Complex numbers, 3, 13, 34, 43, 50, 51 
Complex vector space, 42, 63 
Complex-lamellar fields, 382, 393, 400 
Components 
anholonomic, 332-338 
composite, 336 
composite physical, 337 
holonomic, 332 
of a linear transformation 118-121 
of a matrix, 3 
physical, 334 
of a tensor, 221-222 
of a vector, 51, 80 
Composition of functions, 19 
Conformable matrices, 4,5 
Conjugate subsets of 
(; )LVV, 200 
Continuous groups, 463 
Contractions, 229-234, 243-244 
Contravariant components of a vector, 80, 
205 
Contravariant tensor, 218, 227, 235, 268 
Coordinate chart, 254 
Coordinate curves, 254 
Coordinate functions, 306 
Coordinate map, 306 
Coordinate neighborhood, 306 
Coordinate representations, 341 
Coordinate surfaces, 308 
Coordinate system, 308 

xii                                                                                                                                        INDEX                                                                    
            asymptotic,            460            
            bispherical,            321            
            canonical,            430            
            Cartesian,            309            
            curvilinear,            312            
            cylindrical,            312            
            elliptical            cylindrical,            322            
 geodesic, 431  
            isochoric,            495            
 normal, 432  
            orthogonal,            334            
            paraboloidal,            320            
            positive,            315            
 prolate spheroidal, 321 
            rectangular            Cartesian,            309            
            Riemannian,            432            
            spherical,            320            
            surface,            408            
            toroidal,            323            
Coordinate transformation, 306 
Correspondence, one to one, 19, 97 
Cosine, 66, 69 
Covariant components of a vector, 80, 205 
Covariant derivative, 339, 346-347 
 along a curve, 353, 419 
            spatial,            339            
            surface,            416-417            
            total,            439            
Covariant tensor, 218, 227, 235 
Covector, 203, 269 
Cramer’s rule, 143 
Cross product, 268, 280-284 
Cyclic basis, 164, 193, 196 
Curl, 349-350 
Curvature, 309 
            Gaussian,            458            
            geodesic,            438            
 mean, 456  
 normal, 438, 456, 458 
            principal,            456            
 
Decomposition, polar, 168, 173 
Definite linear transformations, 167 
Density 
scalar, 276 
tensor, 267,271 
Dependence, linear, 46 
Derivation, 331 
Derivative 
Covariant, 339, 346-347, 353, 411, 
439 
 exterior, 374, 379 
Lie, 359, 361, 365, 412 
Determinant 
of linear transformations, 137, 271-
279 
of matrices, 7, 130-173 
Developable surface, 448 
Diagonal elements of a matrix, 4 
Diagonalization of a Hermitian matrix, 158-
175 
Differomorphism, 303 
Differential, 387 
            exact,            388            
            integrable,            388            
Differential forms, 373 

INDEX                                                                                                                        xiii                                                            
            closed,            383            
            exact,            383            
Dilitation, 145, 489 
Dilatation group, 489 
Dimension definition, 49 
of direct sum, 58 
of dual space, 203 
of Euclidean space, 297 
of factor space, 62 
of hypersurface, 407 
of orthogonal group, 467 
of space of skew-symmetric tensors, 
264 
of special linear group, 467 
of tensor spaces, 221 
of vector spaces, 46-54 
Direct complement, 57 
Direct sum 
of endomorphisms, 145 
of vector spaces, 57 
Disjoint sets, 14 
Distance function, 67 
Distribution, 368 
Distributive axioms, 33 
Distributive laws for scalar and vector 
addition, 41 
Divergence, 348 
Domain of a function, 18 
Dual basis, 204, 234 
Dual isomorphism, 213 
Dual linear transformation, 208 
Dual space, 203-212 
Duality operator, 280 
Dummy indices, 129 
 
Eigenspace, 148 
Eigenva1ues, 148 
of Hermitian endomorphism, 158 
multiplicity, 148, 152, 161 
Eigenvector, 148 
Element 
identity,24 
inverse, 24 
of a matrix, 3 
of a set, 13 
unit,24  
Empty set, 13 
Endomorphism 
of groups, 29 
of vector spaces, 99 
Equivalence relation, 16, 60 
Equivalence sets, 16, 50 
Euclidean manifold, 297 
Euclidean point space, 297 
Euler-Lagange equations, 425-427, 454 
Euler’s representation for a solenoidal field, 
400 
Even permutation, 126 
Exchange theorem, 59 
Exponential of an endomorphism, 169 
Exponential maps 
 on a group, 478 
 on a hypersurface, 428 
Exterior algebra, 247-293 
Exterior derivative, 374, 379, 413 
Exterior product, 256 

xiv                                                                                                                                      INDEX                                                                   
ε-symbol definition, 127 
transformation rule, 226 
 
Factor class, 61 
Factor  space, 60-62, 195-197, 254 
Factorization of characteristic polynomial, 
152 
Field, 35 
Fields 
Beltrami, 404 
complex-lamellar, 382, 393, 400 
scalar, 304 
screw, 404 
solenoidal, 399 
tensor, 304 
Trkalian, 405 
vector, 304 
Finite dimensional, 47 
Finite sequence, 20 
Flow, 359 
Free indices, 129 
Function 
constant, 324 
continuous, 302 
coordinate, 306 
definitions, 18-21 
differentiable, 302 
Fundamental forms 
            first,            434            
second, 434 
Fundamental invariants, 151, 156, 169, 274, 
278 
 
Gauss 
equations of, 433 
formulas of, 436, 441 
Gaussian curvature, 458 
General linear group, 101, 113 
Generalized Kronecker delta, 125 
Generalized Stokes’ theorem, 507-513 
Generalized transpose operation, 228, 234, 
244, 247 
Generating set of vectors, 52 
Geodesic curvature, 438 
Geodesics, equations of, 425, 476 
Geometric multiplicity, 148 
Gradient, 303, 340 
Gramian, 278 
Gram-Schmidt orthogonalization process, 71, 
74 
Greatest common divisor, 179, 184 
Group 
axioms of, 23-27 
continuous, 463 
general linear, 101, 464 
homomorphisms,29-32 
orthogonal, 467 
properties of, 26-28 
special linear, 457 
subgroup, 29 
unitary, 114 
 
Hermitian endomorphism, 110, 139, 138-170 
Homogeneous operation, 85 
Homogeneous system of equations, 142 
Homomorphism 

INDEX                                                                                                                        xv                                                            
of group,29 
of vector space, 85 
 
Ideal, 176 
improper, 176 
principal, 176 
trivial, 176 
Identity element, 25 
Identity function, 20 
Identity linear transformation, 98 
Identity matrix, 6 
Image, 18 
Improper ideal, 178 
Independence, linear, 46 
Inequalities 
Schwarz, 64 
triangle, 65 
Infinite dimension, 47 
Infinite sequence, 20 
Injective function, 19 
Inner product space, 63-69, 122, 235-245 
Integer, 13 
Integral domain, 34 
Intersection 
of sets, 14 
of subspaces, 56 
Invariant subspace, 145 
Invariants of a linear transformation, 151 
Inverse 
of a function, 19 
of a linear transformation, 98-100 
of a matrix, 6 
right and left, 104 
Invertible linear transformation, 99 
Involution, 164 
Irreduciable polynomials, 188 
Isometry, 288 
Isomorphic vector spaces, 99 
Isomorphism 
of groups, 29 
of vector spaces, 97 
 
Jacobi’s identity, 331 
Jordan normal form, 199 
 
Kelvin’s condition, 382-383 
Kernel 
of homomorphisms, 30 
of linear transformations, 86 
Kronecker delta, 70, 76 
generalized, 126 
 
Lamellar field, 383, 399 
Laplace expansion formula, 133 
Laplacian, 348 
Latent root, 148 
Latent vector, 148 
Law of Cosines, 69 
Least common multiple, 180 
Left inverse, 104 
Left-handed vector space, 275 
Left-invariant field, 469, 475 
Length, 64 
Levi-Civita parallelism, 423, 443, 448 
Lie algebra, 471, 474, 480 
Lie bracket, 331, 359 

xvi                                                                                                                                      INDEX                                                                   
Lie derivative, 331, 361, 365, 412 
Limit point, 332 
Line integral, 505 
Linear dependence of vectors, 46 
Linear equations, 9, 142-143 
Linear functions, 203-212 
Linear independence of vectors, 46, 47 
Linear transformations, 85-123 
Logarithm of an endomorphism, 170 
Lower triangular matrix, 6 
 
Maps 
            Continuous,            302            
            Differentiable,            302            
Matrix 
adjoint, 8, 154, 156, 279 
block form, 146 
column rank, 138 
diagonal elements, 4, 158 
identity, 6 
inverse of, 6 
nonsingular, 6 
of a linear transformation, 136 
product, 5 
row rank, 138 
skew-symmetric, 7 
square, 4 
trace of, 4 
transpose of, 7 
triangular, 6 
zero, 4 
Maximal Abelian subalgebra, 487 
Maximal Abelian subgroup, 486 
Maximal linear independent set, 47 
Member of a set, 13 
Metric space, 65 
Metric tensor, 244 
Meunier’s equation, 438 
Minimal generating set, 56 
Minimal polynomial, 176-181 
Minimal surface, 454-460 
Minor of a matrix, 8, 133, 266 
Mixed tensor, 218, 276 
Module over a ring, 45 
Monge’s representation for a vector field, 401 
Multilinear functions, 218-228 
Multiplicity 
algebraic, 152, 159, 160, 161, 164, 
189, 191, 192 
geometric, 148 
Negative definite, 167 
Negative element, 34, 42 
Negative semidefinite, 167 
Negatively oriented vector space, 275, 291 
Neighborhood, 300 
Nilcyclic linear transformation, 156, 193 
Nilpotent linear transformation, 192 
Nonsingular linear transformation, 99, 108 
Nonsingular matrix, 6, 9, 29, 82 
Norm function, 66 
Normal 
 of a curve, 390-391 
 of a hypersurface, 407 
Normal linear transformation, 116 
Normalized vector, 70 
Normed space, 66 

INDEX                                                                                                                        xvii                                                            
N-tuple, 16, 42, 55 
Null set, 13 
Null space, 87, 145, 182 
Nullity, 87 
 
Odd permutation, 126 
One-parameter group, 476 
One to one correspondence, 19 
Onto function, 19 
Onto linear transformation, 90, 97 
Open set, 300 
Order 
            of            a            matrix,            3            
preserving function, 20 
of a tensor, 218 
Ordered 
N-tuple, 16,42, 55 
Oriented vector space, 275 
Orthogonal complement, 70, 72, 111, 115, 
209, 216 
Orthogonal linear transformation, 112, 201 
Orthogonal set, 70 
Orthogonal subspaces, 72, 115 
Orthogonal vectors, 66, 71, 74 
Orthogonalization process, 71, 74 
Orthonormal basis, 71 
Osculating plane, 397 
 
Parallel transport, 420 
Parallelism 
 of Cartan, 466, 469 
 generated by a flow, 361 
 of Levi-Civita, 423 
 of Riemann, 423 
Parallelogram law, 69 
Parity 
of a permutation, 126, 248 
of a relative tensor, 226, 275 
Partial ordering, 17 
Permutation, 126 
Perpendicular projection, 115, 165, 173 
Poincaré lemma, 384 
Point difference, 297 
Polar decomposition theorem, 168, 175 
Polar identity, 69, 112, 116, 280 
Polar tensor, 226 
Polynomials 
characteristic, 149, 151-157 
of an endomorphism, 153 
greatest common divisor, 179, 184 
irreducible, 188 
least common multiplier, 180, 185 
minimal, 180 
Position vector, 309 
Positive definite, 167 
Positive semidefinite, 167 
Positively oriented vector space, 275 
Preimage, 18 
Principal curvature, 456 
Principal direction, 456 
Principal ideal, 176 
Principal normal, 391 
Product 
basis, 221, 266 
of linear transformations, 95 
of matrices, 5 
scalar, 41 
tensor, 220, 224 

xviii                                                                                                                                  INDEX                                                                 
wedge, 256-262  
Projection, 93, 101-104, 114-116 
Proper divisor, 185 
Proper subgroup, 27 
Proper subset, 13 
Proper subspace, 55 
Proper transformation, 275 
Proper value, 148 
Proper vector, 148 
Pure contravariant representation, 237 
Pure covariant representation, 237 
Pythagorean theorem, 73 
 
Quotient class, 61 
Quotient theorem, 227 
 
Radius of curvature, 391 
Range of a function, 18 
Rank 
of a linear transformation, 88 
of a matrix, 138 
Rational number, 28, 34 
Real inner product space, 64 
Real number, 13 
Real valued function, 18 
Real vector space, 42 
Reciprocal basis, 76 
Reduced linear transformation, 147 
Regular linear transformation, 87, 113 
Relation, 16 
equivalence, 16 
reflective, 16 
Relative tensor, 226 
Relatively prime, 180, 185, 189 
Restriction, 91 
Riemann-Christoffel tensor, 443, 445, 448-
449 
Riemannian coordinate system, 432 
Riemannian parallelism, 423 
Right inverse, 104 
Right-handed vector space, 275 
Right-invariant field, 515 
Ring, 33 
commutative, 333 
with unity, 33 
Roots of characteristic polynomial, 148 
Row matrix, 3 
Row rank, 138 
r-form, 248 
r-vector, 248 
 
Scalar addition, 41 
Scalar multiplication 
for a linear transformation, 93 
for a matrix, 4 
for a tensor, 220 
for a vector space, 41 
Scalar product 
for tensors, 232 
for vectors, 204 
Schwarz inequality, 64, 279 
Screw field, 404 
Second dual space, 213-217 
Sequence 
finite, 20 
infinite, 20 

INDEX                                                                                                                        xix                                                            
Serret-Frenet formulas, 392 
Sets 
bounded, 300 
closed, 300 
compact, 300) 
complement of, 14 
disjoint, 14 
empty or null, 13 
intersection of, 14 
open, 300 
retractable, 383 
simply connected, 383 
singleton, 13 
star-shaped, 383 
subset, 13 
union, 14 
Similar endomorphisms, 200 
Simple skew-symmetric tensor, 258 
Simple tensor, 223 
Singleton, 13 
Skew-Hermitian endomorphism, 110 
Skew-symmetric endomorphism, 110 
Skew-symmetric matrix, 7 
Skew-symmetric operator, 250 
Skew-symmetric tensor, 247 
Solenoidal field, 399-401 
Spanning set of vectors, 52 
Spectral decompositions, 145-201 
Spectral theorem 
for arbitrary endomorphisms, 192 
for Hermitian endomorphisms, 165 
Spectrum, 148 
Square matrix, 4 
Standard basis, 50, 51 
Standard representation of Lie algebra, 470 
Stokes’ representation for a vector field, 402 
Stokes theorem, 508 
Strict components, 263 
Structural constants, 474 
Subgroup, 27 
proper,27 
Subsequence, 20 
Subset, 13 
Subspace, 55 
characteristic, 161 
direct sum of, 54 
invariant, 145 
sum of, 56 
Summation convection, 129 
Surface 
 area, 454, 493 
 Christoffel symbols, 416 
 coordinate systems, 408 
covariant derivative, 416-417 
 exterior derivative, 413 
            geodesics,            425            
Lie derivative, 412 
metric, 409 
Surjective function, 19 
Sylvester’s theorem, 173 
Symmetric endomorphism, 110 
Symmetric matrix, 7 
Symmetric operator, 255 
Symmetric relation, 16 
Symmetric tensor, 247 
σ-transpose, 247 

xx                                                                                                                                        INDEX                                                                    
 
Tangent plane, 406 
Tangent vector, 339 
Tangential projection, 409, 449-450 
Tensor algebra, 203-245 
Tensor product 
of tensors, 224 
universal factorization property of, 
229 
of vectors, 270-271 
Tensors, 218 
axial, 227 
contraction of, 229-234, 243-244 
contravariant, 218 
covariant, 218 
on inner product spaces, 235-245 
mixed, 218 
polar, 226 
relative, 226 
simple, 223 
skew-symmetric, 248 
symmetric, 248  
Terms of a sequence, 20 
Torsion, 392 
Total covariant derivative, 433, 439 
Trace 
of a linear transformation, 119, 274, 
278 
of a matrix, 4 
Transformation rules 
for basis vectors, 82-83, 210 
for Christoffel symbols, 343-345 
for components of linear 
transformations, 118-122, 136-138 
for components of tensors, 225, 226, 
239, 266, 287, 327 
for components of vectors, 83-84, 
210-211, 325 
for product basis, 225, 269 
for strict components, 266 
Translation space, 297 
Transpose 
of a linear transformation, 105 
of a matrix, 7 
Triangle inequality, 65 
Triangular matrices, 6, 10 
Trivial ideal, 176 
Trivial subspace, 55 
Trkalian field, 405 
Two-point tensor, 361 
 
Unimodular basis, 276 
Union of sets, 14 
Unit vector, 70 
Unitary group, 114 
Unitary linear transformation, 111, 200 
Unimodular basis, 276 
Universal factorization property, 229 
Upper triangular matrix, 6 
 
Value of a function, 18 
Vector line, 390 
Vector product, 268, 280 
Vector space, 41 
basis for, 50 
dimension of, 50 
direct sum of, 57 

INDEX                                                                                                                        xxi                                                            
dual, 203-217 
factor space of, 60-62 
with inner product, 63-84 
isomorphic, 99 
normed, 66 
Vectors, 41 
angle between, 66, 74 
component of, 51 
difference,42 
length of, 64 
normalized, 70 
sum of, 41 
unit, 76 
Volume, 329, 497 
 
Wedge product, 256-262 
Weight, 226 
Weingarten’s formula, 434 
 
Zero element, 24 
Zero linear transformation, 93 
Zero matrix,4 
Zero 
N-tuple, 42 
Zero vector, 42 
